{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a06fe9",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f640b16",
   "metadata": {},
   "source": [
    "# MIRI Imaging TSO Pipeline Notebook\n",
    "\n",
    "**Authors**: Ian Wong; MIRI branch<br>\n",
    "**Last Updated**: July 16, 2025<br>\n",
    "**Pipeline Version**: 1.19.0 (Build 12.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e3bfd",
   "metadata": {},
   "source": [
    "**Purpose**:<br>\n",
    "This notebook provides a framework for processing generic Mid-Infrared Instrument \n",
    "(MIRI) Imaging time series observations (TSO) data through all\n",
    "three James Webb Space Telescope (JWST) pipeline stages.  The data are assumed\n",
    "to be located in the observation directory located in the path set up below.\n",
    "It should not be necessary to edit any cells other than in the\n",
    "[Configuration](#1.-Configuration) section unless modifying the standard\n",
    "pipeline processing options.\n",
    "\n",
    "A significant portion of the data processing workflow for Imaging TSOs is identical to the methods used to process (non-TSO) Imaging observations, and much of this notebook mirrors the corresponding steps shown in the general MIRI Imaging notebook.\n",
    "\n",
    "**Data**:<br>\n",
    "This example is set up to use observations of a secondary eclipse event of LHS-1478b, which were obtained as part of Cycle 1 General Observers Proposal ID (PID) 3730 Observation 13 (PI: H. Diamond-Lowe). A continuous series of 964 integrations of the target was collected with the F1500W filter. No dithering was carried out, as is standard practice for Imaging TSOs. The SUB256 subarray readout was used. The example uncalibrated data will be downloaded automatically unless disabled (i.e., to run with user-supplied local files instead).<br>\n",
    "\n",
    "Most Imaging TSO programs do not have dedicated background observations, with background flux subtraction handled using pixels near the target in the science observations. This version of the notebook does not accommodate cases where dedicated background subtraction is needed. Consult the general MIRI Imaging notebook for a detailed implementation of dedicated background exposures.\n",
    "\n",
    "**JWST pipeline version and CRDS context**:<br>\n",
    "This notebook was written for the above-specified pipeline version and associated build context for this version of the JWST Calibration Pipeline. Information about this and other contexts can be found in the JWST Calibration Reference Data System (CRDS [server](https://jwst-crds.stsci.edu/)). If you use different pipeline versions, please refer to the table [here](https://jwst-crds.stsci.edu/display_build_contexts/) to determine what context to use. To learn more about the differences for the pipeline, read the relevant [documentation](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/jwst-operations-pipeline-build-information#references).\n",
    "\n",
    "Please note that pipeline software development is a continuous process, so results in some cases may be slightly different if a subsequent version is used. **For optimal results, users are strongly encouraged to reprocess their data using the most recent pipeline version and [associated CRDS context](https://jwst-crds.stsci.edu/display_build_contexts/), taking advantage of bug fixes and algorithm improvements.** Any [known issues](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/nirspec-known-issues/nirspec-ifu-known-issues#gsc.tab=0) for this build are noted in the notebook.<br>\n",
    "\n",
    "**Updates**:<br>\n",
    "This notebook is regularly updated as improvements are made to the\n",
    "pipeline. Find the most up to date version of this notebook at:\n",
    "https://github.com/spacetelescope/jwst-pipeline-notebooks/\n",
    "\n",
    "**Recent Changes**:<br>\n",
    "Feb 2 2025: Notebook created.<br>\n",
    "May 5, 2025: Updated to jwst 1.18.0 (no significant changes)<br>\n",
    "July 16, 2025: Updated to jwst 1.19.0 (no significant changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945a04a",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec67c45",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Configuration](#1.-Configuration)\n",
    "2. [Package Imports](#2.-Package-Imports)\n",
    "3. [Demo Mode Setup (ignore if not using demo data)](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "4. [Directory Setup](#4.-Directory-Setup)\n",
    "3. [Detector1 Pipeline](#5.-Detector1-Pipeline)\n",
    "4. [Image2 Pipeline](#6.-Image2-Pipeline)\n",
    "5. [Tso3 Pipeline](#7.-Tso3-Pipeline)\n",
    "6. [Visualize the photometric light curve](#8.-Visualize-the-photometric-light-curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6b230",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f1533b",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "### Install dependencies\n",
    "To make sure that the pipeline version is compatabile with this notebook and the required dependencies and packages are installed,\n",
    "it is recommended that users create a new dedicated conda environment and install the provided\n",
    "`requirements.txt` file before starting this notebook: <br>\n",
    "```\n",
    "conda create -n lrs_demo python=3.11\n",
    "conda activate lrs_demo\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Set run parameters\n",
    "Set basic parameters to use with the notebook. These will affect\n",
    "what observation is used, where the uncalibrated data are located (if already on disk), which\n",
    "pipeline modules to run on the data, and whether background subtraction is carried out. The list of parameters are:\n",
    "\n",
    "* demo_mode\n",
    "* directory with data\n",
    "* pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70330265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import necessary for configuration\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f95ad9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note that <code>demo_mode</code> must be set appropriately below.\n",
    "</div>\n",
    "\n",
    "Set <code>demo_mode = True </code> to run in demonstration mode. In this mode, this\n",
    "notebook will download the example data from the\n",
    "Barbara A. Mikulski Archive for Space Telescopes ([MAST](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html)) and process them through the pipeline.\n",
    "All input and output data will be stored in the local directory unless modified\n",
    "in [Section 3](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data)) below. \n",
    "\n",
    "Set <code>demo_mode = False</code> to process user-specified data that have already\n",
    "been downloaded and provide the location of the data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3af08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for demo_mode and processing steps.\n",
    "\n",
    "# -----------------------------Demo Mode---------------------------------\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode using online example data!')\n",
    "\n",
    "# --------------------------User Mode Directories------------------------\n",
    "# If demo_mode = False, look for user data in these paths\n",
    "if not demo_mode:\n",
    "    # Set directory paths for processing specific data; these will need\n",
    "    # to be changed to your local directory setup (below are given as\n",
    "    # examples)\n",
    "    basedir = os.path.join(os.getcwd(), '')\n",
    "\n",
    "    # Point to where science observation data are stored.\n",
    "    # Assumes uncalibrated data in sci_dir/uncal/, with the results stored in stage1,\n",
    "    # stage2, stage3 directories\n",
    "    sci_dir = os.path.join(basedir, 'imaging_demo_data/PID03730Obs013/')\n",
    "\n",
    "# --------------------------Set Processing Steps--------------------------\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# Science processing\n",
    "do_det1 = True  # calwebb_detector1\n",
    "do_image2 = True  # calwebb_image2\n",
    "do_tso3 = True  # calwebb_tso3\n",
    "do_viz = True  # Visualize calwebb_tso3 results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7e952",
   "metadata": {},
   "source": [
    "### Set CRDS context and server\n",
    "Before importing <code>CRDS</code> and <code>JWST</code> modules, we need\n",
    "to configure our environment. This includes defining a CRDS cache\n",
    "directory in which to keep the reference files that will be used by the\n",
    "calibration pipeline.<br>\n",
    "\n",
    "If the root directory for the local CRDS cache directory has not been set\n",
    "already, it will be set to create one in the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae976dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Set CRDS context and paths----------------------\n",
    "# Each version of the calibration pipeline is associated with a specific CRDS\n",
    "# context file. The pipeline will select the appropriate context file behind\n",
    "# the scenes while running. However, if you wish to override the default context\n",
    "# file and run the pipeline with a different context, you can set that using\n",
    "# the CRDS_CONTEXT environment variable. Here we show how this is done,\n",
    "# although we leave the line commented out in order to use the default context.\n",
    "# If you wish to specify a different context, uncomment the line below.\n",
    "#%env CRDS_CONTEXT jwst_1293.pmap\n",
    "\n",
    "# Check whether the local CRDS cache directory has been set.\n",
    "# If not, set it to the user home directory\n",
    "if (os.getenv('CRDS_PATH') is None):\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds')\n",
    "\n",
    "# Check whether the CRDS server URL has been set.  If not, set it.\n",
    "if (os.getenv('CRDS_SERVER_URL') is None):\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Echo CRDS path in use\n",
    "print(f\"CRDS local filepath: {os.environ['CRDS_PATH']}\")\n",
    "print(f\"CRDS file server: {os.environ['CRDS_SERVER_URL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db2e8f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae9894",
   "metadata": {},
   "source": [
    "## 2. Package Imports\n",
    "\n",
    "Automatically import necessary Python packages for use in the data processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19883028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire available screen width for this notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "# ----------------------General Imports------------------------------------\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------Astropy Imports-----------------------------------\n",
    "# Astropy utilities for opening FITS and ASCII files and downloading demo files\n",
    "from astropy.io import ascii\n",
    "\n",
    "# -----------------------Astrquery Imports-----------------------------------\n",
    "# Utilities to download data \n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# -----------------------Plotting Imports----------------------------------\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# ----------------------JWST calibration pipeline--------------------------\n",
    "import jwst\n",
    "import crds\n",
    "\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Image2Pipeline\n",
    "from jwst.pipeline import Tso3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels\n",
    "#from jwst.datamodels import ImageModel\n",
    "from jwst.associations import asn_from_list as afl  # Tools for creating association files\n",
    "#from jwst.associations.lib.rules_level2_base import DMSLevel2bBase  # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base  # Definition of a Lvl3 association file\n",
    "\n",
    "# Echo pipeline version and CRDS context in use\n",
    "print(f\"JWST Calibration Pipeline Version: {jwst.__version__}\")\n",
    "print(f\"Using CRDS Context: {crds.get_context_name('jwst')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f3a6e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935cafe",
   "metadata": {},
   "source": [
    "## 3. Demo Mode Setup (ignore if not using demo data)\n",
    "\n",
    "If running in demonstration mode, set up the program information to\n",
    "retrieve the uncalibrated data automatically from MAST using\n",
    "[astroquery](https://astroquery.readthedocs.io/en/latest/mast/mast.html).\n",
    "MAST allows for flexibility of searching by the proposal ID and the\n",
    "observation ID instead of just filenames.<br>\n",
    "\n",
    "More information about the JWST file naming conventions can be found at:\n",
    "https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e966e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the program information and paths for demo program\n",
    "if demo_mode:\n",
    "    program = '03730'\n",
    "    sci_obs = \"013\"\n",
    "    basedir = os.path.join('.', 'imaging_demo_data')\n",
    "    sci_dir = os.path.join(basedir, 'PID' + program + 'Obs' + sci_obs)\n",
    "    uncal_dir = os.path.join(sci_dir, 'uncal')\n",
    "\n",
    "    # Ensure filepaths for input data exists\n",
    "    os.makedirs(uncal_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e57b3",
   "metadata": {},
   "source": [
    "Identify list of uncalibrated files associated with the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9427aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of observation IDs for the specified demo program\n",
    "if demo_mode:\n",
    "    obs_id_table = Observations.query_criteria(instrument_name=[\"MIRI/IMAGE\"], provenance_name=[\"CALJWST\"], obs_id=['jw' + program + sci_obs + '*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e75b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list of observations into a list of uncalibrated data files\n",
    "if demo_mode:\n",
    "    # Define types of files to select\n",
    "    file_dict = {'uncal': {'product_type': 'SCIENCE',\n",
    "                           'productSubGroupDescription': 'UNCAL',\n",
    "                           'calib_level': [1]}}\n",
    "\n",
    "    # Science files\n",
    "    files_to_download = []\n",
    "    # Loop over visits identifying uncalibrated files that are associated with them\n",
    "    for exposure in (obs_id_table):\n",
    "        products = Observations.get_product_list(exposure)\n",
    "        for filetype, query_dict in file_dict.items():\n",
    "            filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                             productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                             calib_level=query_dict['calib_level'])\n",
    "            files_to_download.extend(filtered_products['dataURI'])\n",
    "\n",
    "    print(\"Number of files selected for downloading: \", len(files_to_download))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa69c8",
   "metadata": {},
   "source": [
    "Due to data file size constraints, long TSOs have their exposures broken up into multiple segment files. In this case, there should be a total of 5 segments, which together comprise the full duration of the time series observation. \n",
    "\n",
    "Now, download all the uncal files and place them into the appropriate\n",
    "directories.<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Warning: If this notebook is halted during this step the downloaded file\n",
    "may be incomplete, and cause crashes later on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af46999",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "if demo_mode:\n",
    "    for filename in files_to_download:\n",
    "        obs_manifest = Observations.download_file(filename, local_path=os.path.join(uncal_dir, Path(filename).name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7346a",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045603da",
   "metadata": {},
   "source": [
    "## 4. Directory Setup\n",
    "\n",
    "Set up detailed paths to input/output stages here. When running this notebook outside of demo mode, the uncalibrated pipeline input files must be placed into the appropriate directories before proceeding to the JWST pipeline processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output subdirectories to keep the data products organized\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "det1_dir = os.path.join(sci_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "image2_dir = os.path.join(sci_dir, 'stage2')  # calwebb_spec2 pipeline outputs will go here\n",
    "tso3_dir = os.path.join(sci_dir, 'stage3')  # calwebb_tso3 pipeline outputs will go here\n",
    "\n",
    "# Create desired output directories, if needed\n",
    "os.makedirs(det1_dir, exist_ok=True)\n",
    "os.makedirs(image2_dir, exist_ok=True)\n",
    "os.makedirs(tso3_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfedd6f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a953f1",
   "metadata": {},
   "source": [
    "## 5. Detector1 Pipeline\n",
    "Run the datasets through the\n",
    "[Detector1](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_detector1)\n",
    "stage of the pipeline to apply detector level calibrations and create a\n",
    "countrate data product where slopes are fitted to the integration ramps.\n",
    "The `*_rate.fits` products are 2D countrate images, averaged over all\n",
    "integrations within each segment. Meanwhile, 3D countrate data stacks containing an image for each integration (`*_rateints.fits` files) are also\n",
    "created, which will be used in the subsequent stage to preserve the temporal information in the exposure series.<br>\n",
    "\n",
    "When processing MIRI Imaging TSOs, the Detector1 pipeline skips a few pipeline steps by default that would otherwise be run for non-TSO data. These are `rscd` and `firstframe`, which both essentially instruct the pipeline to ignore initial frames within each ramp. The motivation behind the different settings here is that most TSOs have short ramps, and ignoring too many frames can leave too little of the ramp to adequately produce a ramp slope fit. See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_detector1 for a detailed overview of the various pipeline steps that comprise Detector1.\n",
    "\n",
    "As of CRDS context `jwst_1201.pmap` and later, the\n",
    "`jump` step\n",
    "in the Detector1 pipeline allows for the removal of residuals associated\n",
    "with [showers](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/shower-and-snowball-artifacts)\n",
    "for MIRI Imaging observations, but only for data taken with filters shortward (inclusive) of F1500W.\n",
    "Setting the `find_showers` parameter to `True` in the `jump` step activates this functionality. The default parameters for this correction are specified in the `pars-jumpstep` parameter reference files. Users may wish to alter parameters to optimize removal of\n",
    "shower residuals. Available parameters are discussed in the\n",
    "[Detection and Flagging of Showers and Snowballs in JWST Technical Report (Regan 2023)](https://www.stsci.edu/files/live/sites/www/files/home/jwst/documentation/technical-documents/_documents/JWST-STScI-008545.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Detector1 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "det1dict = {}\n",
    "det1dict['group_scale'], det1dict['dq_init'], det1dict['emicorr'] = {}, {}, {}\n",
    "det1dict['saturation'], det1dict['firstframe'], det1dict['lastframe'] = {}, {}, {}\n",
    "det1dict['reset'], det1dict['linearity'], det1dict['rscd'] = {}, {}, {}\n",
    "det1dict['dark_current'], det1dict['refpix'], det1dict['jump'] = {}, {}, {}\n",
    "det1dict['ramp_fit'], det1dict['gain_scale'], det1dict['clean_flicker_noise'] = {}, {}, {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped\n",
    "#det1dict['refpix']['skip'] = True\n",
    "#det1dict['jump']['find_showers'] = True  # Turn on detection of cosmic ray showers\n",
    "#det1dict['clean_flicker_noise']['skip'] = True  # Skipped by default\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#det1dict['dq_init']['override_mask'] = 'myfile.fits'  # Bad pixel mask\n",
    "#det1dict['saturation']['override_saturation'] = 'myfile.fits'  # Saturation\n",
    "#det1dict['reset']['override_reset'] = 'myfile.fits'  # Reset\n",
    "#det1dict['linearity']['override_linearity'] = 'myfile.fits'  # Linearity\n",
    "#det1dict['rscd']['override_rscd'] = 'myfile.fits'  # RSCD\n",
    "#det1dict['dark_current']['override_dark'] = 'myfile.fits'  # Dark current subtraction\n",
    "#det1dict['jump']['override_gain'] = 'myfile.fits'  # Gain used by jump step\n",
    "#det1dict['ramp_fit']['override_gain'] = 'myfile.fits'  # Gain used by ramp fitting step\n",
    "#det1dict['jump']['override_readnoise'] = 'myfile.fits'  # Read noise used by jump step\n",
    "#det1dict['ramp_fit']['override_readnoise'] = 'myfile.fits'  # Read noise used by ramp fitting step\n",
    "\n",
    "# Turn on multi-core processing (off by default).  Choose what fraction of cores to use (quarter, half, or all)\n",
    "det1dict['jump']['maximum_cores'] = 'half'\n",
    "\n",
    "# Alter parameters to optimize removal of shower residuals (example)\n",
    "#det1dict['jump']['after_jump_flag_dn1'] = X  # A floating point value in units of DN\n",
    "#det1dict['jump']['after_jump_flag_time1'] = x.x  # A floating point value in units of seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f458e",
   "metadata": {},
   "source": [
    "Grab all of the uncalibrated files, which comprise the full time series observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e65ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_files = sorted(glob.glob(os.path.join(uncal_dir, '*_uncal.fits')))\n",
    "print(uncal_files)\n",
    "print('Found ' + str(len(uncal_files)) + ' input uncal files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d3f7c",
   "metadata": {},
   "source": [
    "Run the Detector1 pipeline on the selected uncalibrated data using the call method. For long TSOs with full array readouts, this process may take more than 10 minutes per file, particularly if <code>find_showers = True</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9664619",
   "metadata": {
    "scrolled": true,
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run the pipeline on the selected uncal files one by one with the custom parameter dictionary \n",
    "if do_det1:\n",
    "    for file in uncal_files:\n",
    "        Detector1Pipeline.call(file, steps=det1dict, save_results=True, output_dir=det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime for Detector1: {time1 - time0:0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34385329",
   "metadata": {},
   "source": [
    "### Exploring the data\n",
    "\n",
    "Identify the `*_rateints.fits` files and verify which pipeline steps were run and\n",
    "which calibration reference files were applied.<br>\n",
    "\n",
    "The header contains information about which calibration steps were\n",
    "completed and skipped and which reference files were used to process the\n",
    "data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_det1:\n",
    "    # Find rate files\n",
    "    rate_files = sorted(glob.glob(os.path.join(det1_dir, '*_rateints.fits')))\n",
    "\n",
    "    # Read in file as datamodel\n",
    "    rate_f = datamodels.open(rate_files[0])\n",
    "\n",
    "    # Check which steps were run\n",
    "    rate_f.meta.cal_step.instance\n",
    "\n",
    "    # Check which reference files were used to calibrate the dataset:\n",
    "    rate_f.meta.ref_file.instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618af2a5",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7cc2e",
   "metadata": {},
   "source": [
    "## 6. Image2 Pipeline \n",
    "\n",
    "In the [Image2](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_image2.html) stage of the pipeline,\n",
    "flat-fielded and flux-calibrated data products (`*_calints.fits` files) are created from the `*_rateints.fits` files produced by Detector1.\n",
    "\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_image2 for a detailed overview of the various pipeline steps that comprise Image2.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "To override certain steps and reference files, use the examples below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_image2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Image2 pipeline should be configured.\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "image2dict = {}\n",
    "image2dict['assign_wcs'], image2dict['flat_field'] = {}, {}\n",
    "image2dict['photom'] = {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#image2dict['photom']['skip'] = True \n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#image2dict['assign_wcs']['override_distortion'] = 'myfile.asdf'  # Spatial distortion (ASDF file)\n",
    "#image2dict['assign_wcs']['override_filteroffset'] = 'myfile.asdf'  # Imager filter offsets (ASDF file)\n",
    "#image2dict['assign_wcs']['override_specwcs'] = 'myfile.asdf'  # Spectral distortion (ASDF file)\n",
    "#image2dict['assign_wcs']['override_wavelengthrange'] = 'myfile.asdf'  # Wavelength channel mapping (ASDF file)\n",
    "#image2dict['flat_field']['override_flat'] = 'myfile.fits'  # Pixel flatfield\n",
    "#image2dict['photom']['override_photom'] = 'myfile.fits'  # Photometric calibration array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768b56e",
   "metadata": {},
   "source": [
    "Grab the `*rateints.fits` files, ensuring the use of absolute paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rate files from the Detector1 output folder\n",
    "rate_files = sorted(glob.glob(os.path.join(det1_dir, '*rateints.fits')))\n",
    "\n",
    "# Use the absolute file paths\n",
    "for ii in range(len(rate_files)):\n",
    "    rate_files[ii] = os.path.abspath(rate_files[ii])\n",
    "rate_files = np.array(rate_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6280e7d",
   "metadata": {},
   "source": [
    "Run the files through the Image2 pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3d113",
   "metadata": {
    "scrolled": true,
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run the pipeline on the selected rate files one by one with the custom parameter dictionary\n",
    "if do_image2:\n",
    "    for ii, file in enumerate(rate_files):\n",
    "        Image2Pipeline.call(file, steps=image2dict, save_results=True, output_dir=image2_dir)\n",
    "\n",
    "else:\n",
    "    print('Skipping Image2 processing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0fe607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.0f} seconds\")\n",
    "print(f\"Runtime for Image2: {time1 - time_image2:0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd2ce3",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc24ef",
   "metadata": {},
   "source": [
    "## 7. Tso3 Pipeline\n",
    "\n",
    "The Stage 3 pipeline for TSOs, [Tso3](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_tso3.html), is significantly simpler than the analogous processing stage for non-TSO data sets. First, an \n",
    "[association file](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/overview.html)\n",
    "needs to be created that contains all of the `*_calints.fits` files produced from Stage 2.<br>\n",
    "\n",
    "By default, the Tso3 pipeline performs the following steps on MIRI Imaging TSO data sets:<br>\n",
    "* `outlier_detection` flags any remaining cosmic rays, bad pixels, or other artifacts not already flagged during the Detector1 stage of the pipeline. For TSOs, a moving median filtering process is carried out that searches for outlier pixel values along the time axis. This method is designed to guard against the spurious flagging of true astrophysical variability in the target. The default rolling window width is 25 integrations, but that parameter can be adjusted in the user-specified dictionary below.<br>\n",
    "* `tso_photometry` does aperture photometry using a circular aperture centered on the target. The sky background is computed as the mean within a circular annulus. The output is a table (ASCII ecsv format) containing the time at the midpoint of each integration and the photometry values. The default extraction aperture and background annulus sizes are stored in the `tsophot` [reference file](https://jwst-crds.stsci.edu/browse/jwst_miri_tsophot_0001.asdf). <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "IMPORTANT NOTE: the position of the photometric aperture is determined by the world coordinate solution (WCS) in the header, which can be offset from the true position of the target due to pointing and/or target coordinate inaccuracies. It is recommended that users carry out their own photometric extraction using a computed centroid position for the target aperture.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "To override certain steps and reference files, use the examples below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tso3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Tso3 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "tso3dict = {}\n",
    "tso3dict['outlier_detection'], tso3dict['tso_photometry'] = {}, {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#tso3dict['outlier_detection']['skip'] = True\n",
    "\n",
    "# Overrides for specific parameters in the step (examples)\n",
    "#tso3dict['outlier_detection']['rolling_window_width'] = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efcdce",
   "metadata": {},
   "source": [
    "Collect all of the Stage 2 `*calints.fits` files, ensuring the use of absolute paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all the calints.fits files\n",
    "cal_files = sorted(glob.glob(os.path.join(image2_dir, '*calints.fits')))\n",
    "for ii in range(0, len(cal_files)):\n",
    "    cal_files[ii] = os.path.abspath(cal_files[ii])\n",
    "calfiles = np.array(cal_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addb7ae",
   "metadata": {},
   "source": [
    "### Create Association File\n",
    "\n",
    "An association file lists the exposures to be calibrated together in Stage 3\n",
    "of the JWST pipeline. The code below creates an\n",
    "association file from the `*calints.fits` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16fb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Level 3 Association\n",
    "if do_tso3:\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(cal_files, rule=DMS_Level3_Base, product_name='Stage3')\n",
    "\n",
    "    # Write the association to a json file\n",
    "    asnfile = os.path.join(tso3_dir, 'stage3_asn.json')\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448c7e6",
   "metadata": {},
   "source": [
    "### Run the Tso3 pipeline\n",
    "\n",
    "In addition to the photometry file `Stage3_phot.ecsv`, the Tso3 pipeline produces outlier-masked calibrated image stacks `*crfints.fits` for each segment of integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914964ac",
   "metadata": {
    "scrolled": true,
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "if do_tso3:\n",
    "    Tso3Pipeline.call(asnfile, output_dir=tso3_dir, steps=tso3dict, save_results=True)\n",
    "else:\n",
    "    print('Skipping Tso3 processing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce569423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.0f} seconds\")\n",
    "print(f\"Runtime for Image3: {time1 - time_tso3:0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cb848",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbedda",
   "metadata": {},
   "source": [
    "## 8. Visualize the photometric light curve\n",
    "\n",
    "Plot the extracted photometric light curve produced by the pipeline. For the demo mode example, note the handful of outliers that were not addressed by the Tso3 pipeline, as well as the systematic offset in flux level in the last segment of integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0185e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_viz:\n",
    "    # Read in photometry file\n",
    "    phot_file = os.path.join(tso3_dir, 'Stage3_phot.ecsv')\n",
    "    data = ascii.read(phot_file, comment='#', delimiter=' ')\n",
    "\n",
    "    # Make normal plots\n",
    "    %matplotlib inline\n",
    "    # Interactive plots\n",
    "    #%matplotlib notebook\n",
    "\n",
    "    # Plot result\n",
    "    rc('axes', linewidth=2)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5), dpi=150)\n",
    "    ax.plot(data['MJD'], data['aperture_sum'], 'b.', ms=6)\n",
    "    plt.xlabel('MJD_UTC (d)')\n",
    "    plt.ylabel('Flux (Jy)')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(tso3_dir, 'imaging_tso_example_lc.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a59584",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b042aa",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_footer.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
