{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82982978",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d1b6b",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# MIRI LRS Slitless Mode TSO Pipeline Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bbb55c",
   "metadata": {},
   "source": [
    "**Authors**: Ian Wong; MIRI branch<br>\n",
    "**Last Updated**: May 5, 2025<br>\n",
    "**Pipeline Version**: 1.18.0 (Build 11.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c6347",
   "metadata": {},
   "source": [
    "**Purpose**:<br>\n",
    "This notebook provides a framework for processing generic Mid-Infrared\n",
    "Instrument (MIRI) Low Resolution Spectroscopy (LRS) slitless mode time series observations (TSO) data through all\n",
    "three James Webb Space Telescope (JWST) pipeline stages.  The data are assumed\n",
    "to be located in the observation directory located in the path set up below. \n",
    "It should not be necessary to edit any cells other than in the [Configuration](#1.-Configuration) section unless modifying the standard pipeline processing options.<br>\n",
    "\n",
    "A significant portion of the data processing workflow for LRS slitless TSOs is identical to the methods used to process (non-TSO) LRS slit mode observations, and much of this notebook mirrors the corresponding steps shown in the MIRI slit mode notebook.\n",
    "\n",
    "**Data**:<br>\n",
    "This example is set up to use observations of the A-type standard star HD 2811, which were obtained as part of the Cycle 2 JWST flux calibration campaign by Proposal ID (PID) 4496 Observations 4 and 5. The first of these observations is of the target, while the second is a linked dedicated background observation that will be used to remove the background flux contribution from the target exposures. No dithering is carried out for standard LRS slitless mode observations. It is common for LRS slitless TSOs to lack dedicated background observations, and this notebook allows for the user to turn off the background subtraction step. The example uncalibrated data will be downloaded automatically unless disabled (i.e., to run with user-supplied local files instead).<br>\n",
    "\n",
    "Most TSOs consist of a long series of integrations, sometimes lasting more than 10 hours, and are designed to capture transient events (e.g., exoplanet transits, phase curves). This example uses a shorter observation in order to facilitate quick execution of the full data processing workflow, while demonstrating the basic functionality of the JWST pipeline for TSOs.\n",
    "\n",
    "\n",
    "**JWST pipeline version and CRDS context**:<br>\n",
    "This notebook was written for the above-specified pipeline version and associated build context for this version of the JWST Calibration Pipeline. Information about this and other contexts can be found in the JWST Calibration Reference Data System (CRDS [server](https://jwst-crds.stsci.edu/)). If you use different pipeline versions, please refer to the table [here](https://jwst-crds.stsci.edu/display_build_contexts/) to determine what context to use. To learn more about the differences for the pipeline, read the relevant [documentation](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/jwst-operations-pipeline-build-information#references).\n",
    "\n",
    "Please note that pipeline software development is a continuous process, so results in some cases may be slightly different if a subsequent version is used. **For optimal results, users are strongly encouraged to reprocess their data using the most recent pipeline version and [associated CRDS context](https://jwst-crds.stsci.edu/display_build_contexts/), taking advantage of bug fixes and algorithm improvements.** Any [known issues](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/nirspec-known-issues/nirspec-ifu-known-issues#gsc.tab=0) for this build are noted in the notebook.<BR>\n",
    "\n",
    "**Updates**:<br>\n",
    "This notebook is regularly updated as improvements are made to the\n",
    "pipeline. Find the most up to date version of this notebook at:\n",
    "https://github.com/spacetelescope/jwst-pipeline-notebooks/\n",
    "\n",
    "**Recent Changes**:<br>\n",
    "Feb 1 2025: Notebook created.<br>\n",
    "May 5, 2025: Updated to jwst 1.18.0 (no significant changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44864703",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b4801",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Configuration](#1.-Configuration)\n",
    "2. [Package Imports](#2.-Package-Imports)\n",
    "3. [Demo Mode Setup](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "4. [Directory Setup](#4.-Directory-Setup)\n",
    "5. [Detector1 Pipeline](#5.-Detector1-Pipeline)\n",
    "6. [Spec2 Pipeline](#6.-Spec2-Pipeline)\n",
    "7. [Spec3 Pipeline](#7.-Spec3-Pipeline) \n",
    "8. [Plot white-light curve](#8.-Plot-white-light-curve)\n",
    "9. [Plot spectroscopic light curves](#9.-Plot-spectroscopic-light-curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944b039",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eaabd",
   "metadata": {},
   "source": [
    "## 1.-Configuration\n",
    "\n",
    "### Install dependencies\n",
    "To make sure that the pipeline version is compatabile with this notebook and the required dependencies and packages are installed,\n",
    "it is recommended that users create a new dedicated conda environment and install the provided\n",
    "`requirements.txt` file before starting this notebook: <br>\n",
    "```\n",
    "conda create -n lrs_demo python=3.11\n",
    "conda activate lrs_demo\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Set run parameters\n",
    "Set basic parameters to use with the notebook. These will affect\n",
    "what observation is used, where the uncalibrated data are located (if already on disk), which\n",
    "pipeline modules to run on the data, and whether background subtraction is carried out. The list of parameters are:\n",
    "\n",
    "Set the basic parameters to configure the notebook. These parameters determine what data gets used and where the data is located (if already on disk). The list of parameters includes:\n",
    "\n",
    "* `demo_mode`:\n",
    "    * Determines what data is used in the notebook.\n",
    "* **Directories with data**:\n",
    "    * `sci_dir`: Directory where science observation data is stored.\n",
    "    * `bkg_dir`: Directory where background observation data is stored.\n",
    "* **pipeline modules**:\n",
    "    * `do_det1 = True` runs the Detector1 module on the selected data.\n",
    "    * `do_spec2 = True` runs calwebb_spec2 module on the selected data.\n",
    "    * `do_tso3 = True` runs calwebb_tso3 module on the selected data.\n",
    "    * `do_viz = True` Creates plots to visualize calwebb_tso3 results.\n",
    "\n",
    "* **bkg_sub**:\n",
    "    * `bkg_data = True` set to False if not carrying out background substraction\n",
    "    * `bkg_sub = True` instructs the pipeline whether to carry out dedicated background subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import necessary for configuration\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8508f3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note that <code>demo_mode</code> must be set appropriately below.\n",
    "</div>\n",
    "\n",
    "Set <code>demo_mode = True </code> to run in demonstration mode. In this mode, this\n",
    "notebook will download the example data from the\n",
    "Barbara A. Mikulski Archive for Space Telescopes ([MAST](https://archive.stsci.edu/)) and process them through the pipeline.\n",
    "All input and output data will be stored in the local directory unless modified\n",
    "in [Section 3](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data)) below. \n",
    "\n",
    "Set <code>demo_mode = False</code> to process user-specified data that have already\n",
    "been downloaded and provide the location of the data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05351b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for demo_mode, data directory, and processing steps\n",
    "\n",
    "# -----------------------------Demo Mode---------------------------------\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode using online example data!')\n",
    "\n",
    "# --------------------------User Mode Directories------------------------\n",
    "# If demo_mode = False, look for user data in these paths\n",
    "if not demo_mode:\n",
    "    # Set directory paths for processing specific data; these will need\n",
    "    # to be changed to the user's local directory setup (paths below are given as\n",
    "    # examples).\n",
    "    basedir = os.path.join(os.getcwd(), 'lrs_tso_demo_data')\n",
    "\n",
    "    # Point to where the observation data are stored.\n",
    "    # Assumes uncalibrated data in sci_dir/uncal/ and bkg_dir/uncal/, with the results stored in stage1,\n",
    "    # stage2, stage3 directories.\n",
    "    sci_dir = os.path.join(basedir, 'PID04496Obs004/')\n",
    "    bkg_dir = os.path.join(basedir, 'PID04496Obs005/')\n",
    "\n",
    "# --------------------------Set Processing Steps--------------------------\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# Pipeline processing\n",
    "do_det1 = True  # calwebb_detector1\n",
    "do_spec2 = True  # calwebb_spec2\n",
    "do_tso3 = True  # calwebb_tso3\n",
    "do_viz = True  # Visualize calwebb_tso3 results\n",
    "\n",
    "# Background subtraction\n",
    "bkg_data = True  # Set as true if using background data\n",
    "bkg_sub = True  # Set as true to carry out dedicated background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c90aa",
   "metadata": {},
   "source": [
    "### Set CRDS context and server\n",
    "Before importing <code>CRDS</code> and <code>JWST</code> modules, we need to configure our environment. This includes defining a CRDS cache directory in which to keep the reference files that will be used by the calibration pipeline.\n",
    "\n",
    "If the root directory for the local CRDS cache directory has not been set already, it will be automatically created in the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Set CRDS context and paths------------------------\n",
    "# Each version of the calibration pipeline is associated with a specific CRDS\n",
    "# context file. The pipeline will select the appropriate context file behind\n",
    "# the scenes while running. However, if you wish to override the default context\n",
    "# file and run the pipeline with a different context, you can set that using\n",
    "# the CRDS_CONTEXT environment variable. Here we show how this is done,\n",
    "# although we leave the line commented out in order to use the default context.\n",
    "# If you wish to specify a different context, uncomment the line below.\n",
    "#os.environ['CRDS_CONTEXT'] = 'jwst_1322.pmap'  # CRDS context for 1.17.1\n",
    "\n",
    "# Check whether the local CRDS cache directory has been set.\n",
    "# If not, set it to the user home directory.\n",
    "if (os.getenv('CRDS_PATH') is None):\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds')\n",
    "\n",
    "# Check whether the CRDS server URL has been set.  If not, set it.\n",
    "if (os.getenv('CRDS_SERVER_URL') is None):\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Print out CRDS path and context that will be used\n",
    "print('CRDS local filepath:', os.environ['CRDS_PATH'])\n",
    "print('CRDS file server:', os.environ['CRDS_SERVER_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521abfd1",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696294f6",
   "metadata": {},
   "source": [
    "## 2.-Package Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be23f16",
   "metadata": {},
   "source": [
    "Automatically import necessary Python packages for use in the data processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire available screen width for this notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a088b",
   "metadata": {},
   "source": [
    "import system utilities and other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca96f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "# ----------------------General Imports------------------------------------\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------Astropy Imports-----------------------------------\n",
    "# Astropy utilities for opening FITS and ASCII files and downloading demo files\n",
    "from astropy.io import fits, ascii\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# -----------------------Plotting Imports----------------------------------\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------JWST Calibration Pipeline Imports---------------------------\n",
    "# Import the base JWST and calibration reference data packages\n",
    "import jwst\n",
    "import crds\n",
    "\n",
    "# JWST pipelines (each encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Tso3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst.associations import asn_from_list as afl  # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase  # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base  # Definition of a Lvl3 association file\n",
    "\n",
    "# Print out pipeline version and CRDS context that will be used\n",
    "print(\"JWST Calibration Pipeline Version = {}\".format(jwst.__version__))\n",
    "print(\"Using CRDS Context = {}\".format(crds.get_context_name('jwst')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4be7f3",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c05aa",
   "metadata": {},
   "source": [
    "## 3.-Demo Mode Setup (ignore if not using demo data)\n",
    "\n",
    "If running in demonstration mode, set up the program information to\n",
    "retrieve the uncalibrated data automatically from MAST using\n",
    "[astroquery](https://astroquery.readthedocs.io/en/latest/mast/mast.html).\n",
    "MAST allows for flexibility of searching by the proposal ID and the\n",
    "observation ID instead of just filenames.<br>\n",
    "\n",
    "More information about the JWST file naming conventions can be found at:\n",
    "https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the program information and paths for demo program\n",
    "if demo_mode:\n",
    "    program = \"04496\"\n",
    "    sci_obs = \"004\"\n",
    "    bkg_obs = \"005\"  # bkg_obs = \"\" if no background observations \n",
    "    basedir = os.path.join('.', 'lrs_tso_demo_data')\n",
    "    sci_dir = os.path.join(basedir, 'PID' + program + 'Obs' + sci_obs)\n",
    "    uncal_dir = os.path.join(sci_dir, 'uncal')\n",
    "    if bkg_obs and bkg_data:\n",
    "        bkg_dir = os.path.join(basedir, 'PID' + program + 'Obs' + bkg_obs)\n",
    "        uncal_bkgdir = os.path.join(bkg_dir, 'uncal')\n",
    "        print('Science data will be background subtracted')\n",
    "    elif bkg_sub:\n",
    "        if not bkg_data or not bkg_obs:\n",
    "            raise ValueError('bkg_data is set to False or directory for background observations not set')\n",
    "    else:\n",
    "        bkg_dir = ''\n",
    "        print('No background observations will be used')\n",
    "\n",
    "    # Ensure filepaths for input data exists\n",
    "    os.makedirs(uncal_dir, exist_ok=True)\n",
    "    if bkg_data:\n",
    "        os.makedirs(uncal_bkgdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158aaa4",
   "metadata": {},
   "source": [
    "Identify the list of uncalibrated files associated with the science and background observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of observation IDs for the specified demo program\n",
    "if demo_mode:\n",
    "    sci_obs_id_table = Observations.query_criteria(instrument_name=[\"MIRI/SLITLESS\"],\n",
    "                                                   provenance_name=[\"CALJWST\"],  # Executed observations\n",
    "                                                   obs_id=['jw' + program + '-o' + sci_obs + '*']\n",
    "                                                   )\n",
    "\n",
    "    if bkg_data:\n",
    "        bkg_obs_id_table = Observations.query_criteria(instrument_name=[\"MIRI/SLITLESS\"],\n",
    "                                                       provenance_name=[\"CALJWST\"],  # Executed observations\n",
    "                                                       obs_id=['jw' + program + bkg_obs + '*']\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae87ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list of observations into a list of uncalibrated data files\n",
    "if demo_mode:\n",
    "    # Define types of files to select\n",
    "    file_dict = {'uncal': {'product_type': 'SCIENCE', \n",
    "                           'productSubGroupDescription': 'UNCAL', \n",
    "                           'calib_level': [1]}}\n",
    "\n",
    "    # Science files\n",
    "    sci_files_to_download = []\n",
    "    # Loop over visits identifying uncalibrated files that are associated with them\n",
    "    for exposure in (sci_obs_id_table):\n",
    "        products = Observations.get_product_list(exposure)\n",
    "        for filetype, query_dict in file_dict.items():\n",
    "            filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                             productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                             calib_level=query_dict['calib_level'])\n",
    "            sci_files_to_download.extend(filtered_products['dataURI'])\n",
    "\n",
    "    # Background files\n",
    "    if bkg_data:\n",
    "        bkg_files_to_download = []\n",
    "        # Loop over visits identifying uncalibrated files that are associated with them\n",
    "        for exposure in (bkg_obs_id_table):\n",
    "            products = Observations.get_product_list(exposure)\n",
    "            for filetype, query_dict in file_dict.items():\n",
    "                filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                                 productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                                 calib_level=query_dict['calib_level'])\n",
    "                bkg_files_to_download.extend(filtered_products['dataURI'])\n",
    "\n",
    "    print(\"Number of science files selected for downloading: \", len(sci_files_to_download))\n",
    "    if bkg_data:\n",
    "        print(\"Number of background files selected for downloading: \", len(bkg_files_to_download))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f556ba8d",
   "metadata": {},
   "source": [
    "Download all the uncal files and place them into the appropriate directories.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Warning: If this notebook is halted during this step, the downloaded files may be incomplete and cause crashes later on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc769f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_mode:\n",
    "    for filename in sci_files_to_download:\n",
    "        obs_manifest = Observations.download_file(filename, local_path=os.path.join(uncal_dir, Path(filename).name))\n",
    "\n",
    "    if bkg_data:\n",
    "        for filename in bkg_files_to_download:\n",
    "            obs_manifest = Observations.download_file(filename, local_path=os.path.join(uncal_bkgdir, Path(filename).name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716f347",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43839c",
   "metadata": {},
   "source": [
    "## 4.-Directory Setup\n",
    "\n",
    "Set up detailed paths to input/output stages here. When running this notebook outside of demo mode, the uncalibrated pipeline input files must be placed into the appropriate directories before proceeding to the JWST pipeline processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04641a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output subdirectories to keep science data products organized\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal')     # Uncalibrated pipeline inputs should be here\n",
    "det1_dir = os.path.join(sci_dir, 'stage1')     # calwebb_detector1 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(sci_dir, 'stage2')    # calwebb_spec2 pipeline outputs will go here\n",
    "tso3_dir = os.path.join(sci_dir, 'stage3')     # calwebb_tso3 pipeline outputs will go here\n",
    "\n",
    "# Output subdirectories to keep background data products organized\n",
    "if bkg_data:\n",
    "    uncal_bkgdir = os.path.join(bkg_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "    det1_bkgdir = os.path.join(bkg_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "\n",
    "    # Create desired output directories, if needed\n",
    "    os.makedirs(det1_bkgdir, exist_ok=True)\n",
    "\n",
    "# Create desired output directories, if needed\n",
    "os.makedirs(det1_dir, exist_ok=True)\n",
    "os.makedirs(spec2_dir, exist_ok=True)\n",
    "os.makedirs(tso3_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048f37d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebd9f8",
   "metadata": {},
   "source": [
    "## 5.-Detector1 Pipeline\n",
    "\n",
    "In this section, the uncalibrated data are processed through the Detector1\n",
    "pipeline to create Stage 1 data products, which include 2D countrate\n",
    "images that have been averaged over all integrations (`*_rate.fits` files) and 3D cubes containing fitted ramp slopes for each integration (`*_rateints.fits` files).  For TSOs, the integrations must be calibrated separately in the subsequent stages, so the `*_rateints.fits` files are the relevant outputs. The Stage 1 data products have units of DN/s.<br>\n",
    "    \n",
    "Unlike in the case of MIRI LRS slit mode observations, the `firstframe` and `rscd` steps are skipped by default of TSOs.<br>\n",
    "\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_detector1 for a detailed overview of the various pipeline steps that comprise Detector1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c5cfa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "To override certain steps and reference files, use the examples provided below.<br>\n",
    "E.g., turn on detection of cosmic ray showers.\n",
    "</div>\n",
    "\n",
    "Set up a dictionary to define how the Detector1 pipeline should be configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_det1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate dictionary setup\n",
    "det1dict = {}\n",
    "det1dict['group_scale'], det1dict['dq_init'], det1dict['emicorr'], det1dict['saturation'], det1dict['ipc'] = {}, {}, {}, {}, {}\n",
    "det1dict['firstframe'], det1dict['lastframe'], det1dict['reset'], det1dict['linearity'], det1dict['rscd'] = {}, {}, {}, {}, {}\n",
    "det1dict['dark_current'], det1dict['refpix'], det1dict['charge_migration'], det1dict['jump'], det1dict['ramp_fit'] = {}, {}, {}, {}, {}\n",
    "det1dict['gain_scale'] = {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#det1dict['emicorr']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files.\n",
    "# If the files are not in the base local directory, provide full path.\n",
    "#det1dict['dq_init']['override_mask'] = 'myfile.fits' # Bad pixel mask\n",
    "#det1dict['saturation']['override_saturation'] = 'myfile.fits' # Saturation\n",
    "#det1dict['reset']['override_reset'] = 'myfile.fits' # Reset\n",
    "#det1dict['linearity']['override_linearity'] = 'myfile.fits' # Linearity\n",
    "#det1dict['rscd']['override_rscd'] = 'myfile.fits' # RSCD\n",
    "#det1dict['dark_current']['override_dark'] = 'myfile.fits' # Dark current subtraction\n",
    "#det1dict['jump']['override_gain'] = 'myfile.fits' # Gain used by jump step\n",
    "#det1dict['ramp_fit']['override_gain'] = 'myfile.fits' # Gain used by ramp fitting step\n",
    "#det1dict['jump']['override_readnoise'] = 'myfile.fits' # Read noise used by jump step\n",
    "#det1dict['ramp_fit']['override_readnoise'] = 'myfile.fits' # Read noise used by ramp fitting step\n",
    "\n",
    "# Turn on multi-core processing for jump step (off by default).  Choose what fraction of cores to use (quarter, half, or all)\n",
    "#det1dict['jump']['maximum_cores'] = 'half'\n",
    "\n",
    "# Turn on detection of cosmic ray showers if desired (off by default)\n",
    "det1dict['jump']['find_showers'] = True\n",
    "\n",
    "# Adjust the flagging threshold for cosmic rays (default is 3.0)\n",
    "det1dict['jump']['rejection_threshold'] = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b1274",
   "metadata": {},
   "source": [
    "### Processing Science Files\n",
    "\n",
    "Select for only the science data from the target observation, excluding target acquisition and/or pointing verification exposures. For the demo example, there should be only one file, corresponding to a contiguous segment of time-resolved integrations, but for longer TSOs, the full series of exposures may be split into several segments due to file size constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all downloaded uncal files\n",
    "uncal_files = sorted(glob.glob(os.path.join(uncal_dir, '*_uncal.fits')))\n",
    "\n",
    "# Only choose science exposures, which have the exposure type setting 'MIR_LRS-FIXEDSLIT'\n",
    "input_files = np.array([fi for fi in uncal_files if fits.getheader(fi, 'PRIMARY')['EXP_TYPE'] == 'MIR_LRS-SLITLESS'])\n",
    "\n",
    "print('Found ' + str(len(input_files)) + ' science uncal files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669b4b1",
   "metadata": {},
   "source": [
    "Run the Detector1 pipeline on the selected uncalibrated data using the call method. This process may take several minutes per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on the selected input files one by one with the custom parameter dictionary \n",
    "if do_det1:\n",
    "    for file in input_files:\n",
    "        Detector1Pipeline.call(file, steps=det1dict, save_results=True, output_dir=det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cd83e",
   "metadata": {},
   "source": [
    "### Processing Background Files\n",
    "\n",
    "Select for only the science data from the dedicated background observation, excluding target acquisition and/or pointing verification exposures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d770e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bkg_data:\n",
    "    # Grab all downloaded uncal files\n",
    "    uncal_files = sorted(glob.glob(os.path.join(uncal_bkgdir, '*_uncal.fits')))\n",
    "\n",
    "    # Only choose science exposures, which have the exposure type setting 'MIR_LRS-FIXEDSLIT'\n",
    "    input_files = np.array([fi for fi in uncal_files if fits.getheader(fi, 'PRIMARY')['EXP_TYPE'] == 'MIR_LRS-SLITLESS'])\n",
    "\n",
    "    print('Found ' + str(len(input_files)) + ' background uncal files')\n",
    "else:\n",
    "    print('No background data provided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d4a16",
   "metadata": {},
   "source": [
    "Run the Detector1 pipeline on the selected uncalibrated backgtound data using the call method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9cef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on the selected input files one by one with the custom parameter dictionary \n",
    "if do_det1 and bkg_data:\n",
    "    for file in input_files:\n",
    "        Detector1Pipeline.call(file, steps=det1dict, save_results=True, output_dir=det1_bkgdir)\n",
    "else:\n",
    "    print('Skipping Detector1 for background files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Detector1: {time1 - time_det1} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f7f15",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904ccc5",
   "metadata": {},
   "source": [
    "6.<font color='white'>-</font>Spec2 Pipeline<a class=\"anchor\" id=\"spec2\"></a>\n",
    "------------------\n",
    "\n",
    "This stage of the pipeline passes the per-integration countrate (ramp slope) images (`*_rateints.fits` files) previously generated from\n",
    "Detector1 through the Spec2 (calwebb_spec2) pipeline to yield Stage 2\n",
    "data products (i.e., flux-calibrated flat-fielded subarray images `*_calints.fits` and quick-look 1D extracted spectra `*_x1dints.fits`) for each exposure.  These data products have units of MJy/sr.\n",
    "\n",
    "If <code>bkg_sub = True</code>, an association file will be created that pairs each science file with the dedicated background files, and the `bkg_subtract` step will be activated to carry out pixel-by-pixel background subtraction. Otherwise, the countrate files will be passed directly through the Spec2 pipeline.\n",
    "\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_spec2 for a detailed overview of the various pipeline steps that comprise Spec2.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "To override certain steps and reference files, use the examples below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec2 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "spec2dict = {}\n",
    "spec2dict['assign_wcs'], spec2dict['badpix_selfcal'], spec2dict['bkg_subtract'], spec2dict['flat_field'], spec2dict['srctype'] = {}, {}, {}, {}, {}\n",
    "spec2dict['straylight'], spec2dict['fringe'], spec2dict['photom'], spec2dict['residual_fringe'], spec2dict['pixel_replace'] = {}, {}, {}, {}, {}\n",
    "spec2dict['cube_build'], spec2dict['extract_1d'] = {}, {}\n",
    "\n",
    "# Activate dedicated background subtraction, if requested\n",
    "if (bkg_sub is True) and bkg_data:\n",
    "    spec2dict['bkg_subtract']['skip'] = False\n",
    "else:\n",
    "    spec2dict['bkg_subtract']['skip'] = True\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#spec2dict['straylight']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#spec2dict['assign_wcs']['override_distortion'] = 'myfile.asdf' # Spatial distortion (ASDF file)\n",
    "#spec2dict['assign_wcs']['override_specwcs'] = 'myfile.asdf' # Spectral distortion (ASDF file)\n",
    "#spec2dict['flat_field']['override_flat'] = 'myfile.fits' # Pixel flatfield\n",
    "#spec2dict['photom']['override_photom'] = 'myfile.fits' # Photometric calibration array\n",
    "#spec2dict['extract_1d']['override_extract1d'] = 'myfile.asdf' # Spectral extraction parameters (ASDF file)\n",
    "#spec2dict['extract_1d']['override_apcorr'] = 'myfile.asdf' # Aperture correction parameters (ASDF file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e24c77",
   "metadata": {},
   "source": [
    "Define a function that creates an association file to enable pixel-based background subtraction within Spec2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel2asn(scifile, bkgfiles, asnfile):\n",
    "    # Define the basic association of the science exposure\n",
    "    asn = afl.asn_from_list([scifile], rule=DMSLevel2bBase)  # Wrap in array since input is single exposure\n",
    "\n",
    "    # Add the background exposure\n",
    "    for file in bkgfiles:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'background'})              \n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0dd85",
   "metadata": {},
   "source": [
    "Construct lists of science rate files and corresponding background exposures, ensuring the use of absolute paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c355906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get science rate files from the Detector1 output folder\n",
    "sci_files = sorted(glob.glob(os.path.join(det1_dir, '*rateints.fits')))\n",
    "\n",
    "# Use the absolute file paths\n",
    "for ii in range(len(sci_files)):\n",
    "    sci_files[ii] = os.path.abspath(sci_files[ii])\n",
    "sci_files = np.array(sci_files)\n",
    "\n",
    "# Get background rate files\n",
    "if bkg_data:\n",
    "    bkg_files = sorted(glob.glob(os.path.join(det1_bkgdir, '*rateints.fits')))\n",
    "\n",
    "    # Use the absolute file paths\n",
    "    for ii in range(len(bkg_files)):\n",
    "        bkg_files[ii] = os.path.abspath(bkg_files[ii])\n",
    "    bkg_files = np.array(bkg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345159b8",
   "metadata": {},
   "source": [
    "Run the Stage 1 rate files through the Spec2 pipeline. Create ASN files if <code>bkg_sub = True</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on the selected science rate files one by one with the custom parameter dictionary\n",
    "if do_spec2:\n",
    "    for ii, file in enumerate(sci_files):\n",
    "        if bkg_sub and bkg_data:\n",
    "            # Create association file to associate science and background observatons \n",
    "            asnfile = os.path.join(det1_dir, Path(file).stem + '_asn.json')\n",
    "            writel2asn(file, bkg_files, asnfile)\n",
    "            Spec2Pipeline.call(asnfile, steps=spec2dict, save_results=True, output_dir=spec2_dir)\n",
    "        else:\n",
    "            # For cases with only science observation in the data\n",
    "            Spec2Pipeline.call(file, steps=spec2dict, save_results=True, output_dir=spec2_dir)\n",
    "            print('Running Spec2 with no background subtraction')\n",
    "\n",
    "else:\n",
    "    print('Skipping Spec2 processing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dce385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Spec2: {time1 - time_spec2} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f997012",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef94ff",
   "metadata": {},
   "source": [
    "7.<font color='white'>-</font>Tso3 Pipeline<a class=\"anchor\" id=\"spec3\"></a>\n",
    "------------------\n",
    "The JWST calibration pipeline has a specialized final stage for TSOs (calwebb_tso3) that produces outlier-flagged multi-integration calibrated subarray images (`*crfints.fits` files) and corresponding 1D extracted spectra (`*x1dints.fits` files). In addition, the white-light photometric light curve is produced (`*whtlt.ecsv` file), which is computed by integrating the total flux contained within the extraction aperture in each integration.\n",
    "An association file containing the Stage 2 calibrated subarray images (`*calints.fits` files) is required for this stage.<br>\n",
    "\n",
    "Unlike for non-TSO MIRI slit mode observations, no resampling of the subarray is done. The `outlier_detection` step for TSOs has an additional functionality that uses moving median filtering across the stack of individual integrations for detecting outlier pixels, which works to prevent the pipeline from flagging points due to astrophysical variability.\n",
    "\n",
    "Note that pixel values in the composite image created by the JWST pipeline are in *surface brightness units* (MJy/sr), not flux units. If the user desires custom spectral extraction outside the context of the `extract1d` step contained within the Tso3 pipeline, the pixel values must be multiplied by the width of the extraction aperture in pixels and the pixel area in steradians in order to obtain a spectrum in the appropriate flux units. This correction is built into the pipeline's `extract1d` algorithm.\n",
    "The nominal pixel area in steradians is provided in the <code>PIXAR_SR</code> keyword and can be found in the SCI extension header of the image outputs.<br>\n",
    "\n",
    "The default pipeline extraction uses a fixed box of width 12 pixels centered on the expected spectral trace. No in-scene background subtraction is carried out by default. Users can alter the location and width of the extraction aperture and activate background subtraction strategies by providing a custom `extract1d` reference file. For details concerning the proper format and available parameter settings for such reference files, consult https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/reference_files.html#extract1d-reference-file.<br>\n",
    "\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_tso3 for a detailed overview of the various pipeline steps that comprise TSO3.\n",
    "    \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "To override certain steps and reference files, use the examples below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tso3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Tso3 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "tso3dict = {}\n",
    "tso3dict['outlier_detection'], tso3dict['extract_1d'], tso3dict['white_light'] = {}, {}, {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#tso3dict['outlier_detection']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#tso3dict['extract_1d']['override_extract1d'] = 'myfile.json'  # Spectral extraction parameters (ASDF file)\n",
    "#tso3dict['extract_1d']['override_apcorr'] = 'myfile.asdf'  # Aperture correction parameters (ASDF file)\n",
    "\n",
    "# Adjust moving median outlier detection parameters \n",
    "#tso3dict['outlier_detection']['rolling_window_width'] = 31  # Default is 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f8cf6",
   "metadata": {},
   "source": [
    "Define a function to create association files for Stage 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37086d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel3asn(cal_files, asnfile):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(cal_files, rule=DMS_Level3_Base, product_name='Stage3')\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41e073",
   "metadata": {},
   "source": [
    "Find and sort all of the Stage 2 cal files, ensuring the use of absolute paths, and create the association file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cal files from the Spec2 output folder\n",
    "cal_files = sorted(glob.glob(os.path.join(spec2_dir, '*calints.fits')))\n",
    "\n",
    "# Use the absolute file paths\n",
    "for ii in range(len(cal_files)):\n",
    "    cal_files[ii] = os.path.abspath(cal_files[ii])\n",
    "cal_files = np.array(cal_files)\n",
    "\n",
    "# Create association file\n",
    "asnfile = os.path.join(tso3_dir, 'stage3_asn.json')\n",
    "writel3asn(cal_files, asnfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d8a33",
   "metadata": {},
   "source": [
    "Run Tso3 using the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_tso3:\n",
    "    Tso3Pipeline.call(asnfile, steps=tso3dict, save_results=True, output_dir=tso3_dir)\n",
    "else:\n",
    "    print('Skipping Spec3 processing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a537d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Tso3: {time1 - time_tso3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e53a7",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118529a",
   "metadata": {},
   "source": [
    "8.<font color='white'>-</font>Plot white-light curve<a class=\"anchor\" id=\"plots-wlc\"></a>\n",
    "------------------\n",
    "Plot the extracted white-light photometric light curve. The fluxes are in units of Jy.<br>\n",
    "\n",
    "As of September 2024, the absolute flux calibration has been reported to be accurate to within a few percent between 5 and 12 microns. Future improvements are planned to expand the wavelength range for which reliable fluxes can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94145d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_viz:\n",
    "    # Get Stage 3 white-light photometric light curve\n",
    "    whtlt_file = os.path.join(tso3_dir, 'Stage3_whtlt.ecsv')\n",
    "    data = ascii.read(whtlt_file, comment='#', delimiter=' ')\n",
    "    mjd = data['MJD']\n",
    "    wlc = data['whitelight_flux']\n",
    "\n",
    "    # Make normal plots\n",
    "    %matplotlib inline\n",
    "    # Interactive plots\n",
    "    #%matplotlib notebook\n",
    "\n",
    "    # Plot light curve\n",
    "    rc('axes', linewidth=2)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 3), dpi=150)\n",
    "    ax.plot(mjd, wlc, 'b-', lw=2)\n",
    "    plt.xlabel('MJD_UTC (d)')\n",
    "    plt.ylabel('Flux (Jy)')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(tso3_dir, 'lrs_slitless_example_wlc.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606de7f1",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cedf9d7",
   "metadata": {},
   "source": [
    "9.<font color='white'>-</font>Plot spectroscopic light curves<a class=\"anchor\" id=\"plots-spec\"></a>\n",
    "------------------\n",
    "Plot a subset of the spectroscopic light curves contained in the `*x1dints.fits` file. The fluxes are in units of Jy.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_viz:\n",
    "    # Get x1dints files\n",
    "    x1d_file = os.path.join(tso3_dir, 'Stage3_x1dints.fits')\n",
    "\n",
    "    # Choose arbitrary wavelength elements to extract\n",
    "    wave_idxs = [100, 200, 350]\n",
    "    colors = ['r', 'g', 'b']\n",
    "\n",
    "    # Read in file\n",
    "    hdul = fits.open(x1d_file)\n",
    "    int_times = hdul[1].data['int_mid_MJD_UTC']\n",
    "    spec_tables = hdul[2:-1]\n",
    "    wave = spec_tables[0].data['WAVELENGTH']\n",
    "\n",
    "    # Plot light curves\n",
    "    rc('axes', linewidth=2)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5), dpi=150)\n",
    "    for ii in range(len(wave_idxs)):\n",
    "        lc = np.array([spec_tables[k].data['FLUX'][wave_idxs[ii]] for k in range(len(int_times))])\n",
    "        ax.plot(int_times, lc, colors[ii] + '-', lw=2, label=\"{:.3f} um\".format(wave[wave_idxs[ii]]))\n",
    "    plt.xlabel('MJD_UTC (d)')\n",
    "    plt.ylabel('Flux (Jy)')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(tso3_dir, 'lrs_slitless_example_speclcs.png'))\n",
    "    hdul.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061cf46",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c6862",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_footer.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
