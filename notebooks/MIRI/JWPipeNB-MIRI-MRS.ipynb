{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c89d8",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a6ac",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# MIRI MRS Pipeline Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fb85c",
   "metadata": {},
   "source": [
    "**Authors**: David Law, Kirsten Larson; MIRI branch<br>\n",
    "**Last Updated**: April 23, 2024<br>\n",
    "**Pipeline Version**: 1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988f765",
   "metadata": {},
   "source": [
    "**Purpose**:\n",
    "This notebook provides a framework for processing generic Mid-Infrared\n",
    "Instrument (MIRI) Medium Resolution Spectroscopy (MRS) data through all\n",
    "three James Webb Space Telescope (JWST) pipeline stages.  Data is assumed\n",
    "to be located in two observation folders (science and background)\n",
    "according to paths set up below.  It should not be necessary to edit any\n",
    "cells other than in the [Configuration](#2.-Configuration) section\n",
    "unless modifying the standard pipeline processing options.\n",
    "\n",
    "**Data**:\n",
    "This example is set up to use observations of the LMC planetary nebula\n",
    "SMP LMC 058 obtained by Proposal ID (PID) 1523 Observation 3. This is a\n",
    "point source that uses a standard 4-point dither in all three grating\n",
    "settings.  It incorporates a dedicated background in observation 4.\n",
    "Example input data to use will be downloaded automatically unless\n",
    "disabled (i.e., to use local files instead).\n",
    "\n",
    "**Updates**: \n",
    "This notebook is regularly updated as improvements are made to the\n",
    "pipeline. Find the most up to date version of this notebook at:\n",
    "https://github.com/spacetelescope/jwst-pipeline-notebooks/\n",
    "\n",
    "**Recent Changes**:<br>\n",
    "Sep 1 2022: Add some commentary and example on how to use multicore\n",
    "processing in Detector 1<br>\n",
    "Sep 12 2022: Disable unnecessary cube/1d spectra production for\n",
    "individual science exposures in Spec 2<br>\n",
    "Oct 14 2022: Include residual fringe correction in spec2 (note that this\n",
    "will CRASH earlier pipeline versions!)<br>\n",
    "Jun 29 2023: Update to latest 1.11.0 pipeline with photom, outlier\n",
    "detection, and x1d changes, add CRDS path options.  Change to SMP LMC 058\n",
    "demo.<br>\n",
    "Oct 11 2023: Update to 1.12.3 pipeline with 1d spectral residual fringe\n",
    "and auto-centroid options, 2d pixel replacement, and a variety of new\n",
    "cube build options.<br>\n",
    "Nov 17 2023: Incorporate CRDS default parameters for detector1 and\n",
    "spec2<br>\n",
    "Nov 20 2023: Significant revisions to allow use of associations in spec2,\n",
    "allow choice of pixel-based background subtraction.<br>\n",
    "Nov 24 2023: Additional comments and plotting examples.<br>\n",
    "Jan 31 2024: Update to 1.13.4 pipeline, enabling spectral leak\n",
    "correction<br>\n",
    "Apr 12 2024: Migrate from MRS_FlightNB1 notebook, adapt to .call()\n",
    "format, add post-hook example, add demo mode capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9868",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191a7b7-a02d-4063-b224-b39b384a8709",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Package Imports](#1.-Package-Imports)\n",
    "2. [Configuration](#2.-Configuration)\n",
    "3. [Demo Mode Setup](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "4. [Directory Setup](#4.-Directory-Setup)\n",
    "5. [Detector1 Pipeline](#5.-Detector1-Pipeline)\n",
    "6. [Spec2 Pipeline](#6.-Spec2-Pipeline)\n",
    "7. [Spec3 Pipeline](#7.-Spec3-Pipeline)\n",
    "8. [Plot the spectra](#8.-Plot-the-spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c995",
   "metadata": {},
   "source": [
    "1.<font color='white'>-</font>Package Imports<a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a015b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire available screen width for this notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "# ----------------------General Imports------------------------------------\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------Astropy Imports-----------------------------------\n",
    "# Astropy utilities for opening FITS and ASCII files, and downloading demo files\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# -----------------------Plotting Imports----------------------------------\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409eff9f-504f-48b9-8e35-5169ba31226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to be changed here.\n",
    "# It should not be necessary to edit cells below this in general unless\n",
    "# modifying pipeline processing steps.\n",
    "\n",
    "# ------------------------Set CRDS context and paths----------------------\n",
    "# Set CRDS context (if overriding to use a specific version of reference\n",
    "# files; leave commented out to use latest reference files by default)\n",
    "#%env CRDS_CONTEXT  jwst_1146.pmap\n",
    "\n",
    "# Set local CRDS directory if not set already in your .bashrc shell\n",
    "# configuration\n",
    "#os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds')\n",
    "# Set CRDS server directory if not set already in your .bashrc shell\n",
    "# configuration\n",
    "#os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "# Echo CRDS path in use\n",
    "print('CRDS local filepath:', os.environ['CRDS_PATH'])\n",
    "print('CRDS file server:', os.environ['CRDS_SERVER_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------JWST Calibration Pipeline Imports---------------------------\n",
    "# Import the base JWST and calibration reference data packages\n",
    "import jwst\n",
    "import crds\n",
    "\n",
    "# JWST pipelines (each encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels  # JWST datamodels\n",
    "from jwst.associations import asn_from_list as afl  # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase  # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base  # Definition of a Lvl3 association file\n",
    "\n",
    "from jwst.stpipe import Step  # Import the wrapper class for pipeline steps\n",
    "\n",
    "# Echo pipeline version and CRDS context in use\n",
    "print(\"JWST Calibration Pipeline Version = {}\".format(jwst.__version__))\n",
    "print(\"Using CRDS Context = {}\".format(crds.get_context_name('jwst')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410627e4-5d57-422e-bdf1-d53cca12c439",
   "metadata": {},
   "source": [
    "2.<font color='white'>-</font>Configuration<a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to be changed here. These include:\n",
    "# demo_mode, channel, band, data mode directories, and processing steps.\n",
    "\n",
    "# -----------------------------Demo Mode---------------------------------\n",
    "# If demo_mode = True, this notebook will download example data from MAST\n",
    "# to process. This will all happen in a local directory unless modified\n",
    "# in Section 4 below.\n",
    "#\n",
    "# Set this to False if you want to process your own data that has already\n",
    "# been downloaded.\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode using online example data!')\n",
    "\n",
    "# --------------------------User Mode Directories------------------------\n",
    "# If demo_mode = False, look for user data in these paths\n",
    "if not demo_mode:\n",
    "    # Set directory paths for processing specific data; these will need\n",
    "    # to be changed to your local directory setup (below are given as\n",
    "    # examples)\n",
    "    user_home_dir = os.path.expanduser('~')\n",
    "\n",
    "    # Point to where science observation data are\n",
    "    # Assumes uncalibrated data in sci_dir/uncal/ and results in stage1,\n",
    "    # stage2, stage3 directories\n",
    "    sci_dir = os.path.join(user_home_dir, 'FlightData/APT1523/data/Obs03/')\n",
    "\n",
    "    # Point to where science observation data are\n",
    "    # Assumes uncalibrated data in sci_dir/uncal/ and results in stage1,\n",
    "    # stage2, stage3 directories\n",
    "    bg_dir = os.path.join(user_home_dir, 'FlightData/APT1523/data/Obs04/')\n",
    "    #bg_dir = '' # If no background observation, use an empty string\n",
    "\n",
    "# --------------------------Set Processing Steps--------------------------\n",
    "# Whether or not to process only data from a given MRS band/channel (useful\n",
    "# if overriding reference files)\n",
    "# Note that BOTH parameters must be set in order to work\n",
    "use_ch = ''  # '12' or '34'\n",
    "use_band = ''  # 'SHORT', 'MEDIUM', or 'LONG'\n",
    "\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# Science processing\n",
    "dodet1 = True  # calwebb_detector1\n",
    "dospec2 = True  # calwebb_spec2\n",
    "dospec3 = True  # calwebb_spec3\n",
    "\n",
    "# Background processing\n",
    "dodet1bg = True  # calwebb_detector1\n",
    "dospec2bg = True  # calwebb_spec2 (needed for Master Background subtraction)\n",
    "\n",
    "# How should background subtraction using any dedicated backgrounds be done?\n",
    "# If none are selected, cubes will not be background subtracted.  1d spectra\n",
    "# will always use local annular background subtraction for point sources.\n",
    "# Note that if using master-background subtraction, backgrounds must be\n",
    "# selected above to process through spec2.\n",
    "master_bg = True  # Master-background subtraction in spec3 (subtract spectrum generated from the backgrounds).  This is the default pipeline setting.\n",
    "pixel_bg = False  # Pixel-based background subtraction in spec2 (direct pixel subtraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fc7ce",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfec9f",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>Demo Mode Setup<a class=\"anchor\" id=\"intro\"></a> (ignore if not using demo data)\n",
    "------------------\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "If running in demonstration mode, set up the base paths here, and\n",
    "retrieve the uncalibrated data from the MAST archive.  MAST allows for\n",
    "flexibility of searching by the proposal ID and the observation ID\n",
    "instead of just filenames.<br>\n",
    "\n",
    "More information about the JWST file naming conventions can be found at:\n",
    "https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the paths\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode and will download example data from MAST!')\n",
    "    program = \"01523\"\n",
    "    sci_observtn = \"003\"\n",
    "    back_observtn = \"004\"\n",
    "    visit = \"001\"\n",
    "    basedir = os.path.join('.', 'mrs_demo_data')\n",
    "    download_dir = basedir\n",
    "    sci_dir = os.path.join(basedir, 'Obs' + sci_observtn)\n",
    "    bg_dir = os.path.join(basedir, 'Obs' + back_observtn)\n",
    "    uncal_dir = os.path.join(sci_dir, 'uncal')\n",
    "    uncal_bgdir = os.path.join(bg_dir, 'uncal')\n",
    "\n",
    "    # Ensure filepaths for input data exist\n",
    "    if not os.path.exists(uncal_dir):\n",
    "        os.makedirs(uncal_dir)\n",
    "    if not os.path.exists(uncal_bgdir):\n",
    "        os.makedirs(uncal_bgdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of observation IDs for the specified demo program\n",
    "if demo_mode:\n",
    "    # Science data\n",
    "    sci_obs_id_table = Observations.query_criteria(instrument_name=[\"MIRI/IFU\"],\n",
    "                                                   provenance_name=[\"CALJWST\"],  # Executed observations\n",
    "                                                   obs_id=['jw' + program + '-o' + sci_observtn + '*']\n",
    "                                                   )\n",
    "\n",
    "    # Background data\n",
    "    bg_obs_id_table = Observations.query_criteria(instrument_name=[\"MIRI/IFU\"],\n",
    "                                                  provenance_name=[\"CALJWST\"],  # Executed observations\n",
    "                                                  obs_id=['jw' + program + '-o' + back_observtn + '*']\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list of visits into a list of uncalibrated data files\n",
    "if demo_mode:\n",
    "    # Define types of files to select\n",
    "    file_dict = {'uncal': {'product_type': 'SCIENCE', 'productSubGroupDescription': 'UNCAL', 'calib_level': [1]}}\n",
    "\n",
    "    # Science files\n",
    "    sci_files_to_download = []\n",
    "    # Loop over visits identifying uncalibrated files that are associated\n",
    "    # with them\n",
    "    for exposure in (sci_obs_id_table):\n",
    "        products = Observations.get_product_list(exposure)\n",
    "        for filetype, query_dict in file_dict.items():\n",
    "            filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                             productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                             calib_level=query_dict['calib_level'])\n",
    "            sci_files_to_download.extend(filtered_products['dataURI'])\n",
    "\n",
    "    # Background files\n",
    "    bg_files_to_download = []\n",
    "    # Loop over visits identifying uncalibrated files that are associated\n",
    "    # with them\n",
    "    for exposure in (bg_obs_id_table):\n",
    "        products = Observations.get_product_list(exposure)\n",
    "        for filetype, query_dict in file_dict.items():\n",
    "            filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                             productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                             calib_level=query_dict['calib_level'])\n",
    "            bg_files_to_download.extend(filtered_products['dataURI'])\n",
    "\n",
    "    # Cull to a unique list of files that contain 'mirifu' in the filename\n",
    "    # (i.e., not MIRI imager)\n",
    "    sci_files_to_download = np.unique([i for i in sci_files_to_download if 'mirifu' in i])\n",
    "    bg_files_to_download = np.unique([i for i in bg_files_to_download if 'mirifu' in i])\n",
    "\n",
    "    print(\"Science files selected for downloading: \", len(sci_files_to_download))\n",
    "    print(\"Background selected for downloading: \", len(bg_files_to_download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all the uncal files and place them into the appropriate\n",
    "# directories.\n",
    "# Warning: If this notebook is halted during this step the downloaded file may\n",
    "# be incomplete, and cause crashes later on!\n",
    "if demo_mode:\n",
    "    for filename in sci_files_to_download:\n",
    "        sci_manifest = Observations.download_file(filename, local_path=os.path.join(uncal_dir, Path(filename).name))\n",
    "    for filename in bg_files_to_download:\n",
    "        bg_manifest = Observations.download_file(filename, local_path=os.path.join(uncal_bgdir, Path(filename).name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8a852",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae87477",
   "metadata": {},
   "source": [
    "4.<font color='white'>-</font>Directory Setup<a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Set up detailed paths to input/output stages here.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output subdirectories to keep science data products organized\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "det1_dir = os.path.join(sci_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(sci_dir, 'stage2')  # calwebb_spec2 pipeline outputs will go here\n",
    "spec3_dir = os.path.join(sci_dir, 'stage3')  # calwebb_spec3 pipeline outputs will go here\n",
    "\n",
    "# Output subdirectories to keep background data products organized\n",
    "uncal_bgdir = os.path.join(bg_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "det1_bgdir = os.path.join(bg_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "spec2_bgdir = os.path.join(bg_dir, 'stage2')  # calwebb_spec2 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not\n",
    "# create them\n",
    "if not os.path.exists(det1_dir):\n",
    "    os.makedirs(det1_dir)\n",
    "if not os.path.exists(spec2_dir):\n",
    "    os.makedirs(spec2_dir)\n",
    "if not os.path.exists(spec3_dir):\n",
    "    os.makedirs(spec3_dir)\n",
    "if (bg_dir != ''):\n",
    "    if not os.path.exists(det1_bgdir):\n",
    "        os.makedirs(det1_bgdir)\n",
    "    if not os.path.exists(spec2_bgdir):\n",
    "        os.makedirs(spec2_bgdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no background folder, ensure we don't try to process it\n",
    "if (bg_dir == ''):\n",
    "    dodet1bg = False\n",
    "    dospec2bg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3084f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497228f1",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c9165",
   "metadata": {},
   "source": [
    "5.<font color='white'>-</font>Detector1 Pipeline<a class=\"anchor\" id=\"det1\"></a>\n",
    "------------------\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through the calwebb_detector1\n",
    "pipeline to create Stage 1 data products (i.e., uncalibrated slope\n",
    "images of the form *rate.fits).  These data products have units of DN/s.\n",
    "\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_detector1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Detector1 pipeline should be\n",
    "# configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "det1dict = {}\n",
    "det1dict['group_scale'], det1dict['dq_init'], det1dict['emicorr'], det1dict['saturation'], det1dict['ipc'] = {}, {}, {}, {}, {}\n",
    "det1dict['firstframe'], det1dict['lastframe'], det1dict['reset'], det1dict['linearity'], det1dict['rscd'] = {}, {}, {}, {}, {}\n",
    "det1dict['dark_current'], det1dict['refpix'], det1dict['charge_migration'], det1dict['jump'], det1dict['ramp_fit'] = {}, {}, {}, {}, {}\n",
    "det1dict['gain_scale'] = {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#det1dict['emicorr']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#det1dict['dq_init']['override_mask'] = 'myfile.fits' # Bad pixel mask\n",
    "#det1dict['saturation']['override_saturation'] = 'myfile.fits' # Saturation\n",
    "#det1dict['reset']['override_reset'] = 'myfile.fits' # Reset\n",
    "#det1dict['linearity']['override_linearity'] = 'myfile.fits' # Linearity\n",
    "#det1dict['rscd']['override_rscd'] = 'myfile.fits' # RSCD\n",
    "#det1dict['dark_current']['override_dark'] = 'myfile.fits' # Dark current subtraction\n",
    "#det1dict['jump']['override_gain'] = 'myfile.fits' # Gain used by jump step\n",
    "#det1dict['ramp_fit']['override_gain'] = 'myfile.fits' # Gain used by ramp fitting step\n",
    "#det1dict['jump']['override_readnoise'] = 'myfile.fits' # Read noise used by jump step\n",
    "#det1dict['ramp_fit']['override_readnoise'] = 'myfile.fits' # Read noise used by ramp fitting step\n",
    "\n",
    "# Turn on multi-core processing (off by default).  Choose what fraction of cores to use (quarter, half, or all)\n",
    "det1dict['jump']['maximum_cores'] = 'half'\n",
    "det1dict['ramp_fit']['maximum_cores'] = 'half'\n",
    "\n",
    "# This next parameter helps with very bright objects and/or very short ramps\n",
    "det1dict['jump']['three_group_rejection_threshold'] = 100\n",
    "\n",
    "# Turn on detection of cosmic ray showers (off by default)\n",
    "det1dict['jump']['find_showers'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f72f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example of how to insert custom pipeline steps using the\n",
    "# pre-hook/post-hook framework (see\n",
    "# https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/tips-and-tricks-for-working-with-the-jwst-pipeline)\n",
    "# we'll define a new step called XplyStep that multiplies everything by 1.0\n",
    "# I.e., it does nothing, but could be changed to do something more interesting.\n",
    "class XplyStep(Step):\n",
    "    spec = '''\n",
    "    '''\n",
    "    class_alias = 'xply'\n",
    "\n",
    "    def process(self, input_data):\n",
    "        with datamodels.open(input_data) as model:\n",
    "            result = model.copy()\n",
    "        sci = result.data\n",
    "        sci = sci * 1.0\n",
    "        result.data = sci\n",
    "        self.log.info('Multiplied everything by one in custom step!')\n",
    "        return result\n",
    "\n",
    "\n",
    "# And here we'll insert it into our pipeline dictionary to be run at the end right after the gain_scale step\n",
    "det1dict['gain_scale']['post_hooks'] = [XplyStep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc880b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for input files of the form *uncal.fits from the science observation\n",
    "sstring = os.path.join(uncal_dir, 'jw*mirifu*uncal.fits')\n",
    "uncal_files = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '') & (use_band != '')):\n",
    "    keep = np.zeros(len(uncal_files))\n",
    "    for ii in range(0, len(uncal_files)):\n",
    "        with fits.open(uncal_files[ii]) as hdu:\n",
    "            hdu.verify()\n",
    "            hdr = hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == use_ch) & (hdr['BAND'] == use_band)):\n",
    "                keep[ii] = 1\n",
    "    indx = np.where(keep == 1)\n",
    "    uncal_files = uncal_files[indx]\n",
    "\n",
    "print('Found ' + str(len(uncal_files)) + ' science input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over files using\n",
    "# our custom parameter dictionary\n",
    "if dodet1:\n",
    "    for file in uncal_files:\n",
    "        Detector1Pipeline.call(file, steps=det1dict, save_results=True, output_dir=det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing for SCI data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the backgroun\n",
    "# observation\n",
    "sstring = os.path.join(uncal_bgdir, 'jw*mirifu*uncal.fits')\n",
    "uncal_files = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '') & (use_band != '')):\n",
    "    keep = np.zeros(len(uncal_files))\n",
    "    for ii in range(0, len(uncal_files)):\n",
    "        with fits.open(uncal_files[ii]) as hdu:\n",
    "            hdu.verify()\n",
    "            hdr = hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == use_ch) & (hdr['BAND'] == use_band)):\n",
    "                keep[ii] = 1\n",
    "    indx = np.where(keep == 1)\n",
    "    uncal_files = uncal_files[indx]\n",
    "\n",
    "print('Found ' + str(len(uncal_files)) + ' background input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over files using\n",
    "# our custom parameter dictionary\n",
    "if dodet1bg:\n",
    "    for file in uncal_files:\n",
    "        Detector1Pipeline.call(file, steps=det1dict, save_results=True, output_dir=det1_bgdir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing for BG data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d984a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41541b35",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81419572",
   "metadata": {},
   "source": [
    "6.<font color='white'>-</font>Spec2 Pipeline<a class=\"anchor\" id=\"spec2\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our countrate (slope) image products from\n",
    "Stage 1 through the calwebb_spec2 pipeline in order to produce Stage 2\n",
    "data products (i.e., calibrated slope images and quick-look data cubes\n",
    "and 1d spectra).  These data products have units of MJy/sr (or Jy for\n",
    "extracted point-source spectra).<BR>\n",
    "\n",
    "If pixel-based background subtraction was chosen above, this will be\n",
    "applied during this stage.<BR>\n",
    "\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120d056-7135-4977-a48e-0c69306ef0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec2 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "spec2dict = {}\n",
    "spec2dict['assign_wcs'], spec2dict['bkg_subtract'], spec2dict['flat_field'], spec2dict['srctype'], spec2dict['straylight'] = {}, {}, {}, {}, {}\n",
    "spec2dict['fringe'], spec2dict['photom'], spec2dict['residual_fringe'], spec2dict['pixel_replace'], spec2dict['cube_build'] = {}, {}, {}, {}, {}\n",
    "spec2dict['extract_1d'] = {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#spec2dict['straylight']['skip'] = True\n",
    "\n",
    "# Pixel-based background usage was set up above, propagate that here\n",
    "if (pixel_bg is True):\n",
    "    spec2dict['bkg_subtract']['skip'] = False\n",
    "else:\n",
    "    spec2dict['bkg_subtract']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#spec2dict['assign_wcs']['override_distortion'] = 'myfile.asdf' # Spatial distortion (ASDF file)\n",
    "#spec2dict['assign_wcs']['override_regions'] = 'myfile.asdf' # IFU slice regions on detector (ASDF file)\n",
    "#spec2dict['assign_wcs']['override_specwcs'] = 'myfile.asdf' # Spectral distortion (ASDF file)\n",
    "#spec2dict['assign_wcs']['override_wavelengthrange'] = 'myfile.asdf' # Wavelength channel mapping (ASDF file)\n",
    "#spec2dict['flat_field']['override_flat'] = 'myfile.fits' # Pixel flatfield\n",
    "#spec2dict['straylight']['override_mrsxartcorr'] = 'myfile.fits' # Cross-artifact model parameters\n",
    "#spec2dict['fringe']['override_fringe'] = 'myfile.fits' # Static fringe-flat\n",
    "#spec2dict['photom']['override_photom'] = 'myfile.fits' # Photometric calibration array\n",
    "#spec2dict['cube_build']['override_cubepar'] = 'myfile.fits' # Cube-building parameters\n",
    "#spec2dict['extract_1d']['override_extract1d'] = 'myfile.asdf' # Spectral extraction parameters (ASDF file)\n",
    "#spec2dict['extract_1d']['override_apcorr'] = 'myfile.asdf' # Aperture correction parameters (ASDF file)\n",
    "\n",
    "# Turn on 2d residual fringe correction (off by default)\n",
    "# This can sometimes improve residual fringing in science results, but takes\n",
    "# a long time to run and often does not work as well as 1d residual fringe\n",
    "# correction (in calwebb_spec3)\n",
    "#spec2dict['residual_fringe']['skip'] = False\n",
    "\n",
    "# Run pixel replacement code to extrapolate values for otherwise bad pixels\n",
    "# This can help mitigate 5-10% negative dips in spectra of bright sources\n",
    "# Use the 'mingrad' algorithm\n",
    "spec2dict['pixel_replace']['skip'] = False\n",
    "spec2dict['pixel_replace']['algorithm'] = 'mingrad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b44e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl2 association file.  This will\n",
    "# enable use of the pixel-based background subtraction if chosen above.\n",
    "# This requires *one* input SCI file, but can have multiple input BG files\n",
    "# Note that the background will not be applied properly to all files if more\n",
    "# than *one* SCI file is included in the association.\n",
    "def writel2asn(onescifile, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list([onescifile], rule=DMSLevel2bBase, product_name=prodname)  # Wrap in array since input was single exposure\n",
    "\n",
    "    #Channel/band configuration for this sci file\n",
    "    with fits.open(onescifile) as hdu:\n",
    "        hdu.verify()\n",
    "        hdr = hdu[0].header\n",
    "        this_channel, this_band = hdr['CHANNEL'], hdr['BAND']\n",
    "\n",
    "    # If backgrounds were provided, find which are appropriate to this\n",
    "    # channel/band and add to association\n",
    "    nbg = len(bgfiles)\n",
    "    if (nbg > 0):\n",
    "        for ii in range(0, nbg):\n",
    "            with fits.open(bgfiles[ii]) as hdu:\n",
    "                hdu.verify()\n",
    "                hdr = hdu[0].header\n",
    "                if ((hdr['CHANNEL'] == this_channel) & (hdr['BAND'] == this_band)):\n",
    "                    asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97390df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files; ensure we're using absolute paths\n",
    "\n",
    "sstring = os.path.join(det1_dir, 'jw*mirifu*rate.fits')  # Use files from the detector1 output folder\n",
    "ratefiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(ratefiles)):\n",
    "    ratefiles[ii] = os.path.abspath(ratefiles[ii])\n",
    "ratefiles = np.array(ratefiles)\n",
    "\n",
    "# If desired, check that these sci files are the band/channel to use\n",
    "if ((use_ch != '') & (use_band != '')):\n",
    "    keep = np.zeros(len(ratefiles))\n",
    "    for ii in range(0, len(ratefiles)):\n",
    "        with fits.open(ratefiles[ii]) as hdu:\n",
    "            hdu.verify()\n",
    "            hdr = hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == use_ch) & (hdr['BAND'] == use_band)):\n",
    "                keep[ii] = 1\n",
    "    indx = np.where(keep == 1)\n",
    "    ratefiles = ratefiles[indx]\n",
    "\n",
    "# Background Files\n",
    "sstring = os.path.join(det1_bgdir, 'jw*mirifu*rate.fits')\n",
    "bgfiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(bgfiles)):\n",
    "    bgfiles[ii] = os.path.abspath(bgfiles[ii])\n",
    "bgfiles = np.array(bgfiles)\n",
    "\n",
    "# If desired, check that these bg files are the band/channel to use\n",
    "if ((use_ch != '') & (use_band != '')):\n",
    "    keep = np.zeros(len(bgfiles))\n",
    "    for ii in range(0, len(bgfiles)):\n",
    "        with fits.open(bgfiles[ii]) as hdu:\n",
    "            hdu.verify()\n",
    "            hdr = hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == use_ch) & (hdr['BAND'] == use_band)):\n",
    "                keep[ii] = 1\n",
    "    indx = np.where(keep == 1)\n",
    "    bgfiles = bgfiles[indx]\n",
    "\n",
    "print('Found ' + str(len(ratefiles)) + ' science files')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through each of the science files, using relevant associated\n",
    "# backgrounds in spec2 processing.\n",
    "# The background files are used in this step to perform pixel-based\n",
    "# background subtraction (if desired) otherwise background subtraction\n",
    "# is done later with Spec3 files\n",
    "\n",
    "# To save runtime, make a new version of our spec2 parameter dictionary\n",
    "# that turns off creation of quicklook cubes and 1d spectra for science\n",
    "# data\n",
    "spec2dict_sci = copy.deepcopy(spec2dict)\n",
    "spec2dict_sci['cube_build']['skip'] = True\n",
    "spec2dict_sci['extract_1d']['skip'] = True\n",
    "\n",
    "if dospec2:\n",
    "    for file in ratefiles:\n",
    "        asnfile = os.path.join(sci_dir, 'l2asn.json')\n",
    "        writel2asn(file, bgfiles, asnfile, 'Level2')\n",
    "        Spec2Pipeline.call(asnfile, steps=spec2dict_sci, save_results=True, output_dir=spec2_dir)\n",
    "else:\n",
    "    print('Skipping Spec2 processing for SCI data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reduce the backgrounds individually\n",
    "# This will be needed for the Master Background step in Spec3. You can skip\n",
    "# this if doing Spec2 pixel based background.\n",
    "if dospec2bg:\n",
    "    for file in bgfiles:\n",
    "        Spec2Pipeline.call(file, steps=spec2dict, save_results=True, output_dir=spec2_bgdir)\n",
    "else:\n",
    "    print('Skipping Spec2 processing for BG data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Spec2: {time1 - time_spec2} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6415",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336b000-19c7-4e76-b6a2-924a3ed08b7c",
   "metadata": {},
   "source": [
    "7.<font color='white'>-</font>Spec3 Pipeline<a class=\"anchor\" id=\"spec3\"></a>\n",
    "------------------\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we'll run the calwebb_spec3 pipeline to produce a\n",
    "composite data cube and extracted spectrumfrom all dithered exposures.\n",
    "We will need to create an association file from all science (*cal.fits)\n",
    "and background (*x1d.fits) data in order for the pipeline to use them\n",
    "appropriately.<br>\n",
    "\n",
    "If master background subtraction was selected above this will be applied\n",
    "during this step.<BR>\n",
    "\n",
    "Note that the data cubes created by the JWST pipeline are in SURFACE\n",
    "BRIGHTNESS units (MJy/steradian), not flux units.  What that means is\n",
    "that if you intend to sum spectra within an aperture you need to be sure\n",
    "to multiply by the pixel area in steradians first in order to get a\n",
    "spectrum in flux units (the PIXAR_SR keyword can be found in the SCI\n",
    "extension header).  This correction is already build into the pipeline\n",
    "Extract1D algorithm.<BR>\n",
    "\n",
    "Spectral extraction for point sources uses a conical aperture extraction\n",
    "whose radius increases with wavelength, with annular background\n",
    "subtraction and aperture correction.  Spectral extraction for extended\n",
    "sources sums the entire FOV at each wavelength plane (note this is\n",
    "different for each channel).<BR>\n",
    "\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b245a-070d-4e86-ad22-5c62ee35ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec3 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "spec3dict = {}\n",
    "spec3dict['assign_mtwcs'], spec3dict['master_background'], spec3dict['outlier_detection'], spec3dict['mrs_imatch'], spec3dict['cube_build'] = {}, {}, {}, {}, {}\n",
    "spec3dict['extract_1d'], spec3dict['spectral_leak'] = {}, {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#spec3dict['outlier_detection']['skip'] = True\n",
    "\n",
    "# Master background usage was set up above, propagate that here\n",
    "if (master_bg is True):\n",
    "    spec3dict['master_background']['skip'] = False\n",
    "else:\n",
    "    spec3dict['master_background']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#spec3dict['cube_build']['override_cubepar'] = 'myfile.fits'  # Cube-building parameters\n",
    "#spec3dict['extract_1d']['override_extract1d'] = 'myfile.asdf'  # Spectral extraction parameters (ASDF file)\n",
    "#spec3dict['extract_1d']['override_apcorr'] = 'myfile.asdf'  # Aperture correction parameters (ASDF file)\n",
    "\n",
    "# Options for adjusting performance for the outlier detection step\n",
    "#spec3dict['outlier_detection']['kernel_size'] = '11 1'  # Dial this to adjust the detector kernel size\n",
    "#spec3dict['outlier_detection']['threshold_percent'] = 99.5  # Dial this to be more/less aggressive in outlier flagging (values closer to 100% are less aggressive)\n",
    "\n",
    "# Options for adjusting the cube building step\n",
    "#spec3dict['cube_build']['output_file'] = 'mycube'  # Custom output name\n",
    "spec3dict['cube_build']['output_type'] = 'band'  # 'band', 'channel' (default), or 'multi' type cube output.  'band' is best for 1d residual fringe correction.\n",
    "#spec3dict['cube_build']['channel'] = '1'  # Build everything from just channel 1 into a single cube (we could also choose '2','3','4', or 'ALL')\n",
    "#spec3dict['cube_build']['weighting'] = 'drizzle'  # Algorithm used: 'emsm' or 'drizzle' (default)\n",
    "#spec3dict['cube_build']['coord_system'] = 'ifualign'  # Cube rotation: 'ifualign', 'skyalign' (default), or 'internal_cal'\n",
    "#spec3dict['cube_build']['scalexy'] = 0.5  # Output cube spaxel scale (arcsec) if setting it by hand\n",
    "#spec3dict['cube_build']['scalew'] = 0.002  # Output cube voxel depth in wavelength (micron) if setting it by hand\n",
    "#spec3dict['cube_build']['ra_center'] = 65.0  # Force cube to be centered at this R.A.\n",
    "#spec3dict['cube_build']['dec_center'] = -35.0  # Force cube to be centered at this Decl.\n",
    "#spec3dict['cube_build']['cube_pa'] = 45.0  # Force cube to have this position angle\n",
    "#spec3dict['cube_build']['nspax_x'] = 61  # Force cube to have this number of spaxels in cube X direction\n",
    "#spec3dict['cube_build']['nspax_y'] = 61  # Force cube to have this number of spaxels in cube Y direction\n",
    "#spec3dict['cube_build']['wavemin'] = 4.8  # Custom minimum wavelength for the cube\n",
    "#spec3dict['cube_build']['wavemax'] = 6.3  # Custom maximum wavelength for the cube\n",
    "\n",
    "# Options for adjusting the 1d spectral extraction\n",
    "#spec3dict['extract_1d']['ifu_set_srctype'] = 'POINT' # Force a certain type of spectral extraction ('POINT' or 'EXTENDED')\n",
    "#spec3dict['extract_1d']['ifu_rscale'] = 2  # Number of FWHM to use for point-source conical aperture extraction radius (default is 2)\n",
    "spec3dict['extract_1d']['ifu_autocen'] = True  # Enable auto-centering of the extraction aperture (default is True)\n",
    "#spec3dict['extract_1d']['center_xy'] = (20,20)  # Override aperture location if desired\n",
    "spec3dict['extract_1d']['ifu_rfcorr'] = True  # Turn on 1d residual fringe correction (default is False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl3 association file from an\n",
    "# input list.\n",
    "# Note that any background exposures have to be of type x1d.\n",
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "\n",
    "    # Add background files to the association\n",
    "    nbg = len(bgfiles)\n",
    "    for ii in range(0, nbg):\n",
    "        asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files; ensure we're using absolute paths\n",
    "\n",
    "# Science Files need the cal.fits files\n",
    "sstring = os.path.join(spec2_dir, 'jw*mirifu*cal.fits')\n",
    "calfiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(calfiles)):\n",
    "    calfiles[ii] = os.path.abspath(calfiles[ii])\n",
    "calfiles = np.array(calfiles)\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '') & (use_band != '')):\n",
    "    keep = np.zeros(len(calfiles))\n",
    "    for ii in range(0, len(calfiles)):\n",
    "        with fits.open(calfiles[ii]) as hdu:\n",
    "            hdu.verify()\n",
    "            hdr = hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == use_ch) & (hdr['BAND'] == use_band)):\n",
    "                keep[ii] = 1\n",
    "    indx = np.where(keep == 1)\n",
    "    calfiles = calfiles[indx]\n",
    "\n",
    "# Background Files need the x1d.fits files for Master Background subtraction\n",
    "sstring = os.path.join(spec2_bgdir, 'jw*mirifu*x1d.fits')\n",
    "bgfiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(bgfiles)):\n",
    "    bgfiles[ii] = os.path.abspath(bgfiles[ii])\n",
    "bgfiles = np.array(bgfiles)\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '') & (use_band != '')):\n",
    "    keep = np.zeros(len(bgfiles))\n",
    "    for ii in range(0, len(bgfiles)):\n",
    "        with fits.open(bgfiles[ii]) as hdu:\n",
    "            hdu.verify()\n",
    "            hdr = hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == use_ch) & (hdr['BAND'] == use_band)):\n",
    "                keep[ii] = 1\n",
    "    indx = np.where(keep == 1)\n",
    "    bgfiles = bgfiles[indx]\n",
    "\n",
    "print('Found ' + str(len(calfiles)) + ' science files to process')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9918b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an association file that includes all of the different exposures.\n",
    "# _cal.fits science files and _x1d.fits files if doing Master Background\n",
    "# subtraction\n",
    "asnfile = os.path.join(sci_dir, 'l3asn.json')\n",
    "if dospec3:\n",
    "    writel3asn(calfiles, bgfiles, asnfile, 'Level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c035f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run calwebb_spec3 on the association file\n",
    "if dospec3:\n",
    "    Spec3Pipeline.call(asnfile, steps=spec3dict, save_results=True, output_dir=spec3_dir)\n",
    "else:\n",
    "    print('Skipping Spec3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Spec3: {time1 - time_spec3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426db95",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966d5ff",
   "metadata": {},
   "source": [
    "8.<font color='white'>-</font>Plot the spectra<a class=\"anchor\" id=\"plots\"></a>\n",
    "------------------\n",
    "Here we'll plot the spectra to see what our source looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf94f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files\n",
    "\n",
    "# Science Files\n",
    "# Use the final extracted spectra (x1d.fits)\n",
    "sstring = sorted(glob.glob(os.path.join(spec3_dir, '*x1d.fits')))\n",
    "x1dfiles = np.array(sorted(sstring))\n",
    "\n",
    "# Restrict plot to only the selected channel and band\n",
    "if ((use_ch != '') & (use_band != '')):\n",
    "    keep = np.zeros(len(x1dfiles))\n",
    "    for ii in range(0, len(x1dfiles)):\n",
    "        with fits.open(x1dfiles[ii]) as hdu:\n",
    "            hdu.verify()\n",
    "            hdr = hdu[0].header\n",
    "            for sub_ch in use_ch:\n",
    "                if ((hdr['CHANNEL'] == sub_ch) & (hdr['BAND'] == use_band)):\n",
    "                    keep[ii] = 1\n",
    "    indx = np.where(keep == 1)\n",
    "    x1dfiles = x1dfiles[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal plots\n",
    "%matplotlib inline\n",
    "# Interactive plots\n",
    "#%matplotlib notebook\n",
    "\n",
    "rc('axes', linewidth=2)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 3), dpi=150)\n",
    "\n",
    "if (len(x1dfiles) > 0):\n",
    "    hdu = fits.open(x1dfiles[0])\n",
    "    objname = hdu[0].header['TARGPROP']\n",
    "    hdu.close()\n",
    "else:\n",
    "    objname = 'Unknown'\n",
    "\n",
    "ymin, ymax = np.nan, np.nan\n",
    "for file in x1dfiles:\n",
    "    x1d = fits.open(file)\n",
    "    x1ddata = x1d[1].data\n",
    "    wave = x1ddata['WAVELENGTH']\n",
    "    flux = x1ddata['FLUX']\n",
    "    ymin = np.nanmin([ymin, np.nanpercentile(flux, 2)])\n",
    "    ymax = np.nanmax([ymax, np.nanpercentile(flux, 99.5)])\n",
    "\n",
    "    # labels\n",
    "    label = x1d[0].header['CHANNEL'] + x1d[0].header['BAND']\n",
    "\n",
    "    plt.plot(wave, flux, label=label)\n",
    "\n",
    "    x1d.close()\n",
    "\n",
    "plt.xlabel(r'Wavelength ($\\mu$m)')\n",
    "plt.ylabel('Flux (Jy)')\n",
    "plt.title(objname)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.legend(fontsize=8, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid()\n",
    "plt.savefig('mrs_example_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e2553",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d16ec",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_footer.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
