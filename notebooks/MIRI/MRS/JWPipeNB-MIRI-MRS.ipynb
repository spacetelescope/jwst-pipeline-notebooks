{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c89d8",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a6ac",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# MIRI MRS Pipeline Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fb85c",
   "metadata": {},
   "source": [
    "**Authors**: David Law, Kirsten Larson; MIRI branch<br>\n",
    "**Last Updated**: July 16, 2025<br>\n",
    "**Pipeline Version**: 1.19.1 (Build 12.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988f765",
   "metadata": {},
   "source": [
    "**Purpose**:<BR>\n",
    "This notebook provides a framework for processing generic Mid-Infrared\n",
    "Instrument (MIRI) Medium Resolution Spectroscopy (MRS) data through all\n",
    "three James Webb Space Telescope (JWST) pipeline stages.  Data is assumed\n",
    "to be located in two observation folders (science and background)\n",
    "according to paths set up below.  It should not be necessary to edit any\n",
    "cells other than in the [Configuration](#1.-Configuration) section\n",
    "unless modifying the standard pipeline processing options.\n",
    "\n",
    "**Data**:<BR>\n",
    "This example is set up to use observations of the LMC planetary nebula\n",
    "SMP LMC 058 obtained by Proposal ID (PID) 1523 Observation 3. This is a\n",
    "point source that uses a standard 4-point dither in all three grating\n",
    "settings.  It incorporates a dedicated background in observation 4.\n",
    "Example input data to use will be downloaded automatically unless\n",
    "disabled (i.e., to use local files instead).\n",
    "\n",
    "**JWST pipeline version and CRDS context**:<BR>\n",
    "This notebook was written for the\n",
    "calibration pipeline version given above.  If you use it with a different pipeline\n",
    "version or specify a non-default reference file context please see the relevant\n",
    "release notes\n",
    "([here for pipeline](https://github.com/spacetelescope/jwst),\n",
    "[here for CRDS](https://jwst-crds.stsci.edu/)) for possibly relevant\n",
    "changes.<BR>\n",
    "\n",
    "**Updates**:<BR>\n",
    "This notebook is regularly updated as improvements are made to the\n",
    "pipeline. Find the most up to date version of this notebook at:\n",
    "https://github.com/spacetelescope/jwst-pipeline-notebooks/\n",
    "\n",
    "**Recent Changes**:<br>\n",
    "Jan 31 2024: Update to 1.13.4 pipeline, enabling spectral leak\n",
    "correction<br>\n",
    "Jul 1 2024: Migrate from MRS_FlightNB1 notebook, adapt to .call()\n",
    "format, add post-hook example, add demo mode capability.<br>\n",
    "Oct 11 2024: Update to Build 11.0 (jwst 1.15.1); move pixel_replacement to spec3 and enable by default, add option for bad pixel self-calibration in spec2.<br>\n",
    "Jan 16 2025: Update to Build 11.2 (jwst 1.17.1); no significant changes.<br>\n",
    "May 5 2025: Update to Build 11.3 (jwst 1.18.0); add optional command to remove residual showers, plot spectra from updated x1d.fits data model with rf-corrected columns.<br>\n",
    "May 22 2025: Update example plot use of regular and rf-corrected spectra.<br>\n",
    "July 16 2025: No significant updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9868",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191a7b7-a02d-4063-b224-b39b384a8709",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Configuration](#1.-Configuration)\n",
    "2. [Package Imports](#2.-Package-Imports)\n",
    "3. [Demo Mode Setup](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "4. [Directory Setup](#4.-Directory-Setup)\n",
    "5. [Detector1 Pipeline](#5.-Detector1-Pipeline)\n",
    "6. [Spec2 Pipeline](#6.-Spec2-Pipeline)\n",
    "7. [Spec3 Pipeline](#7.-Spec3-Pipeline)\n",
    "8. [Plot the spectra](#8.-Plot-the-spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd1599",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae53dc6",
   "metadata": {},
   "source": [
    "1.<font color='white'>-</font>Configuration<a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------\n",
    "Set basic parameters to use with notebook. These will affect\n",
    "what data is used, where data is located (if already in disk),\n",
    "pipeline modules run in this data, and type of background\n",
    "subtraction (if any). The list of parameters are:\n",
    "\n",
    "* demo_mode\n",
    "* channel\n",
    "* band\n",
    "* directories with data\n",
    "* pipeline modules\n",
    "* Backgroud subtraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import necessary for configuration\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88f6ee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note that <code>demo_mode</code> must be set appropriately below.\n",
    "</div>\n",
    "\n",
    "Set <code>demo_mode = True </code> to run in demonstration mode. In this mode this\n",
    "notebook will download example data from the\n",
    "Barbara A. Mikulski Archive for Space Telescopes (MAST) and process it through the pipeline.\n",
    "This will all happen in a local directory unless modified\n",
    "in [Section 3](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data)) below. \n",
    "\n",
    "Set <code>demo_mode = False</code> if you want to process your own data that has already\n",
    "been downloaded and provide the location of the data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f6fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for demo_mode, channel, band, data mode directories, and \n",
    "# processing steps.\n",
    "\n",
    "# -----------------------------Demo Mode---------------------------------\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode using online example data!')\n",
    "\n",
    "# --------------------------User Mode Directories------------------------\n",
    "# If demo_mode = False, look for user data in these paths\n",
    "if not demo_mode:\n",
    "    # Set directory paths for processing specific data; these will need\n",
    "    # to be changed to your local directory setup (below are given as\n",
    "    # examples)\n",
    "    user_home_dir = os.path.expanduser('~')\n",
    "\n",
    "    # Point to where science observation data are\n",
    "    # Assumes uncalibrated data in sci_dir/uncal/ and results in stage1,\n",
    "    # stage2, stage3 directories\n",
    "    sci_dir = os.path.join(user_home_dir, 'FlightData/APT1523/data/Obs003/')\n",
    "\n",
    "    # Point to where background observation data are\n",
    "    # Assumes uncalibrated data in bg_dir/uncal/ and results in stage1,\n",
    "    # stage2, stage3 directories\n",
    "    bg_dir = os.path.join(user_home_dir, 'FlightData/APT1523/data/Obs004/')\n",
    "    #bg_dir = '' # If no background observation, use an empty string\n",
    "\n",
    "# --------------------------Set Processing Steps--------------------------\n",
    "# Whether or not to process only data from a given MRS band/channel (useful\n",
    "# if overriding reference files)\n",
    "# Note that BOTH parameters must be set in order to work\n",
    "use_ch = ''  # '12' or '34'\n",
    "use_band = ''  # 'SHORT', 'MEDIUM', or 'LONG'\n",
    "\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# Science processing\n",
    "dodet1 = True  # calwebb_detector1\n",
    "dospec2 = True  # calwebb_spec2\n",
    "dospec3 = True  # calwebb_spec3\n",
    "doviz = True # Visualize calwebb_spec3 results\n",
    "\n",
    "# Background processing\n",
    "dodet1bg = True  # calwebb_detector1\n",
    "dospec2bg = True  # calwebb_spec2 (needed for Master Background subtraction)\n",
    "\n",
    "# How should background subtraction using any dedicated backgrounds be done?\n",
    "# If none are selected, cubes will not be background subtracted.  1d spectra\n",
    "# will always use local annular background subtraction for point sources.\n",
    "# Note that if using master-background subtraction, background observations\n",
    "# must be selected above to process through spec2 (dospec2bg = True).\n",
    "master_bg = True  # Master-background subtraction in spec3 (subtract spectrum generated from the backgrounds).  This is the default pipeline setting.\n",
    "pixel_bg = False  # Pixel-based background subtraction in spec2 (direct pixel subtraction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ef261",
   "metadata": {},
   "source": [
    "### Set CRDS context and server\n",
    "Before importing <code>CRDS</code> and <code>JWST</code> modules, we need to configure our environment. This includes defining a CRDS cache directory in which to keep the reference files that will be used by the calibration pipeline.\n",
    "\n",
    "If the root directory for the local CRDS cache directory has not been set already, it will be set to create one in the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c53535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Set CRDS context and paths----------------------\n",
    "\n",
    "# Set CRDS reference file context.  Leave commented-out to use the default context\n",
    "# (latest reference files associated with the calibration pipeline version)\n",
    "# or set a specific context here.\n",
    "#%env CRDS_CONTEXT  jwst_1295.pmap\n",
    "\n",
    "# Check whether the local CRDS cache directory has been set.\n",
    "# If not, set it to the user home directory\n",
    "if (os.getenv('CRDS_PATH') is None):\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds')\n",
    "# Check whether the CRDS server URL has been set.  If not, set it.\n",
    "if (os.getenv('CRDS_SERVER_URL') is None):\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Echo CRDS path and context in use\n",
    "print('CRDS local filepath:', os.environ['CRDS_PATH'])\n",
    "print('CRDS file server:', os.environ['CRDS_SERVER_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec7020",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c995",
   "metadata": {},
   "source": [
    "## 2.<font color='white'>-</font>Package Imports<a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire available screen width for this notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "# ----------------------General Imports------------------------------------\n",
    "import glob\n",
    "import copy\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------Astropy Imports-----------------------------------\n",
    "# Astropy utilities for opening FITS and ASCII files, and downloading demo files\n",
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# -----------------------Plotting Imports----------------------------------\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------JWST Calibration Pipeline Imports---------------------------\n",
    "# Import the base JWST and calibration reference data packages\n",
    "import jwst\n",
    "import crds\n",
    "\n",
    "# JWST pipelines (each encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels  # JWST datamodels\n",
    "from jwst.associations import asn_from_list as afl  # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase  # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base  # Definition of a Lvl3 association file\n",
    "\n",
    "from jwst.stpipe import Step  # Import the wrapper class for pipeline steps\n",
    "\n",
    "# Echo pipeline version and CRDS context in use\n",
    "print(\"JWST Calibration Pipeline Version = {}\".format(jwst.__version__))\n",
    "print(\"Using CRDS Context = {}\".format(crds.get_context_name('jwst')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6131e0",
   "metadata": {},
   "source": [
    "### Define convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convenience function to select only files of a given channel/band from an input set\n",
    "def select_ch_band_files(files, use_ch, use_band):\n",
    "    if ((use_ch != '') & (use_band != '')):\n",
    "        keep = np.zeros(len(files))\n",
    "        for ii in range(0, len(files)):\n",
    "            with fits.open(files[ii]) as hdu:\n",
    "                hdu.verify()\n",
    "                hdr = hdu[0].header\n",
    "                if ((hdr['CHANNEL'] == use_ch) & (hdr['BAND'] == use_band)):\n",
    "                    keep[ii] = 1\n",
    "        indx = np.where(keep == 1)\n",
    "        files_culled = files[indx]\n",
    "    else:\n",
    "        files_culled = files\n",
    "        \n",
    "    return files_culled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fc7ce",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfec9f",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>Demo Mode Setup<a class=\"anchor\" id=\"intro\"></a> (ignore if not using demo data)\n",
    "------------------\n",
    "If running in demonstration mode, set up the program information to\n",
    "retrieve the uncalibrated data automatically from MAST using\n",
    "[astroquery](https://astroquery.readthedocs.io/en/latest/mast/mast.html).\n",
    "MAST allows for flexibility of searching by the proposal ID and the\n",
    "observation ID instead of just filenames.<br>\n",
    "\n",
    "More information about the JWST file naming conventions can be found at:\n",
    "https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the program information and paths for demo program\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode and will download example data from MAST!')\n",
    "    program = \"01523\"\n",
    "    sci_observtn = \"003\"\n",
    "    back_observtn = \"004\"\n",
    "    visit = \"001\"\n",
    "    basedir = os.path.join('.', 'mrs_demo_data')\n",
    "    download_dir = basedir\n",
    "    sci_dir = os.path.join(basedir, 'Obs' + sci_observtn)\n",
    "    bg_dir = os.path.join(basedir, 'Obs' + back_observtn)\n",
    "    uncal_dir = os.path.join(sci_dir, 'uncal')\n",
    "    uncal_bgdir = os.path.join(bg_dir, 'uncal')\n",
    "\n",
    "    # Ensure filepaths for input data exist\n",
    "    if not os.path.exists(uncal_dir):\n",
    "        os.makedirs(uncal_dir)\n",
    "    if not os.path.exists(uncal_bgdir):\n",
    "        os.makedirs(uncal_bgdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668f138",
   "metadata": {},
   "source": [
    "Identify list of science (SCI) and background (BG) uncalibrated files associated with visits.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Selects only <i>mirifu</i> data (ignores MIRI imager).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of observation IDs for the specified demo program\n",
    "if demo_mode:\n",
    "    # Science data\n",
    "    sci_obs_id_table = Observations.query_criteria(instrument_name=[\"MIRI/IFU\"],\n",
    "                                                   provenance_name=[\"CALJWST\"],  # Executed observations\n",
    "                                                   obs_id=['jw' + program + '-o' + sci_observtn + '*']\n",
    "                                                   )\n",
    "\n",
    "    # Background data\n",
    "    bg_obs_id_table = Observations.query_criteria(instrument_name=[\"MIRI/IFU\"],\n",
    "                                                  provenance_name=[\"CALJWST\"],  # Executed observations\n",
    "                                                  obs_id=['jw' + program + '-o' + back_observtn + '*']\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list of visits into a list of uncalibrated data files\n",
    "if demo_mode:\n",
    "    # Define types of files to select\n",
    "    file_dict = {'uncal': {'product_type': 'SCIENCE', 'productSubGroupDescription': 'UNCAL', 'calib_level': [1]}}\n",
    "\n",
    "    # Science files\n",
    "    sci_files_to_download = []\n",
    "    # Loop over visits identifying uncalibrated files that are associated with them\n",
    "    for exposure in (sci_obs_id_table):\n",
    "        products = Observations.get_product_list(exposure)\n",
    "        for filetype, query_dict in file_dict.items():\n",
    "            filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                             productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                             calib_level=query_dict['calib_level'])\n",
    "            sci_files_to_download.extend(filtered_products['dataURI'])\n",
    "\n",
    "    # Background files\n",
    "    bg_files_to_download = []\n",
    "    # Loop over visits identifying uncalibrated files that are associated with them\n",
    "    for exposure in (bg_obs_id_table):\n",
    "        products = Observations.get_product_list(exposure)\n",
    "        for filetype, query_dict in file_dict.items():\n",
    "            filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                             productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                             calib_level=query_dict['calib_level'])\n",
    "            bg_files_to_download.extend(filtered_products['dataURI'])\n",
    "\n",
    "    # Cull to a unique list of files that contain 'mirifu' in the filename\n",
    "    # (i.e., not MIRI imager)\n",
    "    sci_files_to_download = np.unique([i for i in sci_files_to_download if 'mirifu' in i])\n",
    "    bg_files_to_download = np.unique([i for i in bg_files_to_download if 'mirifu' in i])\n",
    "\n",
    "    print(\"Science files selected for downloading: \", len(sci_files_to_download))\n",
    "    print(\"Background selected for downloading: \", len(bg_files_to_download))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c0f1b",
   "metadata": {},
   "source": [
    "Download all the uncal files and place them into the appropriate directories.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Warning: If this notebook is halted during this step the downloaded file may be incomplete, and cause crashes later on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee926e",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "if demo_mode:\n",
    "    for filename in sci_files_to_download:\n",
    "        sci_manifest = Observations.download_file(filename, local_path=os.path.join(uncal_dir, Path(filename).name))\n",
    "    for filename in bg_files_to_download:\n",
    "        bg_manifest = Observations.download_file(filename, local_path=os.path.join(uncal_bgdir, Path(filename).name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8a852",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae87477",
   "metadata": {},
   "source": [
    "4.<font color='white'>-</font>Directory Setup<a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------\n",
    "Set up detailed paths to input/output stages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output subdirectories to keep science data products organized\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "det1_dir = os.path.join(sci_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(sci_dir, 'stage2')  # calwebb_spec2 pipeline outputs will go here\n",
    "spec3_dir = os.path.join(sci_dir, 'stage3')  # calwebb_spec3 pipeline outputs will go here\n",
    "\n",
    "# Output subdirectories to keep background data products organized\n",
    "uncal_bgdir = os.path.join(bg_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "det1_bgdir = os.path.join(bg_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "spec2_bgdir = os.path.join(bg_dir, 'stage2')  # calwebb_spec2 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if not os.path.exists(det1_dir):\n",
    "    os.makedirs(det1_dir)\n",
    "if not os.path.exists(spec2_dir):\n",
    "    os.makedirs(spec2_dir)\n",
    "if not os.path.exists(spec3_dir):\n",
    "    os.makedirs(spec3_dir)\n",
    "if (bg_dir != ''):\n",
    "    if not os.path.exists(det1_bgdir):\n",
    "        os.makedirs(det1_bgdir)\n",
    "    if not os.path.exists(spec2_bgdir):\n",
    "        os.makedirs(spec2_bgdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f9ad78",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "If there is no background folder, ensure we don't try to process it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (bg_dir == ''):\n",
    "    dodet1bg = False\n",
    "    dospec2bg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3084f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497228f1",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c9165",
   "metadata": {},
   "source": [
    "5.<font color='white'>-</font>Detector1 Pipeline<a class=\"anchor\" id=\"det1\"></a>\n",
    "------------------\n",
    "In this section we process our data through the calwebb_detector1\n",
    "pipeline to create Stage 1 data products (i.e., uncalibrated slope\n",
    "images of the form *rate.fits).  These data products have units of DN/s.<BR><BR>\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_detector1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08d81a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "To override certain steps and reference files, use the examples provided below.<br>\n",
    "E.g., turn on detection of cosmic ray showers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Detector1 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "det1dict = {}\n",
    "det1dict['group_scale'], det1dict['dq_init'], det1dict['emicorr'], det1dict['saturation'], det1dict['ipc'] = {}, {}, {}, {}, {}\n",
    "det1dict['firstframe'], det1dict['lastframe'], det1dict['reset'], det1dict['linearity'], det1dict['rscd'] = {}, {}, {}, {}, {}\n",
    "det1dict['dark_current'], det1dict['refpix'], det1dict['charge_migration'], det1dict['jump'], det1dict['ramp_fit'] = {}, {}, {}, {}, {}\n",
    "det1dict['gain_scale'] = {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#det1dict['emicorr']['skip'] = True\n",
    "\n",
    "# Option to use the first frame for very bright MIRI data that otherwise saturates fast enough to provide no slope\n",
    "#det1dict['firstframe']['bright_use_group1'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#det1dict['dq_init']['override_mask'] = 'myfile.fits' # Bad pixel mask\n",
    "#det1dict['saturation']['override_saturation'] = 'myfile.fits' # Saturation\n",
    "#det1dict['reset']['override_reset'] = 'myfile.fits' # Reset\n",
    "#det1dict['linearity']['override_linearity'] = 'myfile.fits' # Linearity\n",
    "#det1dict['rscd']['override_rscd'] = 'myfile.fits' # RSCD\n",
    "#det1dict['dark_current']['override_dark'] = 'myfile.fits' # Dark current subtraction\n",
    "#det1dict['jump']['override_gain'] = 'myfile.fits' # Gain used by jump step\n",
    "#det1dict['ramp_fit']['override_gain'] = 'myfile.fits' # Gain used by ramp fitting step\n",
    "#det1dict['jump']['override_readnoise'] = 'myfile.fits' # Read noise used by jump step\n",
    "#det1dict['ramp_fit']['override_readnoise'] = 'myfile.fits' # Read noise used by ramp fitting step\n",
    "\n",
    "# Turn on multi-core processing for jump step (off by default).  Choose what fraction of cores to use (quarter, half, or all)\n",
    "det1dict['jump']['maximum_cores'] = 'half'\n",
    "\n",
    "# Toggle detection of cosmic ray showers if desired (on by default)\n",
    "#det1dict['jump']['find_showers'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84c859",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Below an example of how to insert custom pipeline steps using the\n",
    "pre-hook/post-hook framework.\n",
    "\n",
    "For more information see [Tips and Trick for working with the JWST Pipeline](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/tips-and-tricks-for-working-with-the-jwst-pipeline)\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f72f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new step called XplyStep that multiplies everything by 1.0\n",
    "# I.e., it does nothing, but could be changed to do something more interesting.\n",
    "class XplyStep(Step):\n",
    "    spec = '''\n",
    "    '''\n",
    "    class_alias = 'xply'\n",
    "\n",
    "    def process(self, input_data):\n",
    "        with datamodels.open(input_data) as model:\n",
    "            result = model.copy()\n",
    "        sci = result.data\n",
    "        sci = sci * 1.0\n",
    "        result.data = sci\n",
    "        self.log.info('Multiplied everything by one in custom step!')\n",
    "        return result\n",
    "\n",
    "\n",
    "# And here we'll insert it into our pipeline dictionary to be run at the end right after the gain_scale step\n",
    "det1dict['gain_scale']['post_hooks'] = [XplyStep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e52a45",
   "metadata": {},
   "source": [
    "### Calibrating Science Files\n",
    "Look for input science files and run calwebb_detector1 pipeline using the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc880b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for input files of the form *uncal.fits from the science observation\n",
    "sstring = os.path.join(uncal_dir, 'jw*mirifu*uncal.fits')\n",
    "uncal_files = np.array(sorted(glob.glob(sstring)))\n",
    "# Check that these are the band/channel to use\n",
    "uncal_files = select_ch_band_files(uncal_files, use_ch, use_band)\n",
    "\n",
    "print('Found ' + str(len(uncal_files)) + ' science input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5acc8",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over files using\n",
    "# our custom parameter dictionary\n",
    "if dodet1:\n",
    "    for file in uncal_files:\n",
    "        Detector1Pipeline.call(file, steps=det1dict, save_results=True, output_dir=det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing for SCI data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411a0d7",
   "metadata": {},
   "source": [
    "### Calibrating Background Files\n",
    "Look for input background files and run calwebb_detector1\n",
    "pipeline using the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the background\n",
    "# observations\n",
    "sstring = os.path.join(uncal_bgdir, 'jw*mirifu*uncal.fits')\n",
    "uncal_files = np.array(sorted(glob.glob(sstring)))\n",
    "# Check that these are the band/channel to use\n",
    "uncal_files = select_ch_band_files(uncal_files, use_ch, use_band)\n",
    "\n",
    "print('Found ' + str(len(uncal_files)) + ' background input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb2c84",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over files using\n",
    "# our custom parameter dictionary\n",
    "if dodet1bg:\n",
    "    for file in uncal_files:\n",
    "        Detector1Pipeline.call(file, steps=det1dict, save_results=True, output_dir=det1_bgdir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing for BG data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d984a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41541b35",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81419572",
   "metadata": {},
   "source": [
    "6.<font color='white'>-</font>Spec2 Pipeline<a class=\"anchor\" id=\"spec2\"></a>\n",
    "------------------\n",
    "\n",
    "In this section we process our countrate (slope) image products from\n",
    "Stage 1 (calwebb_detector1) through the Spec2 (calwebb_spec2) pipeline\n",
    "in order to produce Stage 2\n",
    "data products (i.e., calibrated slope images and quick-look data cubes\n",
    "and 1d spectra).  These data products have units of MJy/sr (or Jy for\n",
    "extracted point-source spectra).\n",
    "\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_spec2\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "If pixel-based background subtraction was chosen above, this will be\n",
    "applied during this stage.\n",
    "\n",
    "To override certain steps and reference files use the examples below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120d056-7135-4977-a48e-0c69306ef0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec2 pipeline should be configured.\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "spec2dict = {}\n",
    "spec2dict['assign_wcs'], spec2dict['badpix_selfcal'], spec2dict['bkg_subtract'], spec2dict['flat_field'], spec2dict['srctype'] = {}, {}, {}, {}, {}\n",
    "spec2dict['straylight'], spec2dict['fringe'], spec2dict['photom'], spec2dict['residual_fringe'], spec2dict['pixel_replace'] = {}, {}, {}, {}, {}\n",
    "spec2dict['cube_build'], spec2dict['extract_1d'] = {}, {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#spec2dict['straylight']['skip'] = True\n",
    "\n",
    "# Pixel-based background usage was set up above, propagate that here\n",
    "if (pixel_bg is True):\n",
    "    spec2dict['bkg_subtract']['skip'] = False\n",
    "else:\n",
    "    spec2dict['bkg_subtract']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#spec2dict['assign_wcs']['override_distortion'] = 'myfile.asdf' # Spatial distortion (ASDF file)\n",
    "#spec2dict['assign_wcs']['override_regions'] = 'myfile.asdf' # IFU slice regions on detector (ASDF file)\n",
    "#spec2dict['assign_wcs']['override_specwcs'] = 'myfile.asdf' # Spectral distortion (ASDF file)\n",
    "#spec2dict['assign_wcs']['override_wavelengthrange'] = 'myfile.asdf' # Wavelength channel mapping (ASDF file)\n",
    "#spec2dict['flat_field']['override_flat'] = 'myfile.fits' # Pixel flatfield\n",
    "#spec2dict['straylight']['override_mrsxartcorr'] = 'myfile.fits' # Cross-artifact model parameters\n",
    "#spec2dict['fringe']['override_fringe'] = 'myfile.fits' # Static fringe-flat\n",
    "#spec2dict['photom']['override_photom'] = 'myfile.fits' # Photometric calibration array\n",
    "#spec2dict['cube_build']['override_cubepar'] = 'myfile.fits' # Cube-building parameters\n",
    "#spec2dict['extract_1d']['override_extract1d'] = 'myfile.asdf' # Spectral extraction parameters (ASDF file)\n",
    "#spec2dict['extract_1d']['override_apcorr'] = 'myfile.asdf' # Aperture correction parameters (ASDF file)\n",
    "\n",
    "# Turn on residual cosmic-ray shower correction (off by default)\n",
    "# (see https://jwst-docs.stsci.edu/known-issues-with-jwst-data/shower-and-snowball-artifacts)\n",
    "#spec2dict['straylight']['clean_showers'] = True\n",
    "\n",
    "# Turn on 2d residual fringe correction (off by default)\n",
    "# This can sometimes improve residual fringing in science results, but takes\n",
    "# a long time to run and often does not work as well as 1d residual fringe\n",
    "# correction (in calwebb_spec3)\n",
    "#spec2dict['residual_fringe']['skip'] = False\n",
    "\n",
    "# Turn on bad pixel self-calibration, where all exposures on a given detector are used to find and\n",
    "# flag bad pixels that may have been missed by the bad pixel mask.\n",
    "# This step is experimental, and works best when dedicated background observations are included\n",
    "#spec2dict['badpix_selfcal']['skip'] = False\n",
    "#spec2dict['badpix_selfcal']['flagfrac_upper']=0.005 # Fraction of pixels to flag (dial as desired; 1.0 would be 100% of pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f534112",
   "metadata": {},
   "source": [
    "Define a function to create association files for Stage 2. This will enable use of the pixel-based background subtraction, if chosen above. This requires *one* input SCI file, but can have multiple input background files.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note that the background will not be applied properly to all files if more than *one* SCI file is included in the association.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b44e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel2asn(onescifile, bgfiles, selfcalfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list([onescifile], rule=DMSLevel2bBase, product_name=prodname)  # Wrap in array since input was single exposure\n",
    "\n",
    "    #Channel/band configuration for this sci file\n",
    "    with fits.open(onescifile) as hdu:\n",
    "        hdu.verify()\n",
    "        hdr = hdu[0].header\n",
    "        this_channel, this_band = hdr['CHANNEL'], hdr['BAND']\n",
    "\n",
    "    # If backgrounds were provided, find which are appropriate to this\n",
    "    # channel/band and add to association\n",
    "    for file in bgfiles:\n",
    "        with fits.open(file) as hdu:\n",
    "            hdu.verify()\n",
    "            if ((hdu[0].header['CHANNEL'] == this_channel) & (hdu[0].header['BAND'] == this_band)):\n",
    "                asn['products'][0]['members'].append({'expname': file, 'exptype': 'background'})\n",
    "                \n",
    "    # If provided with a list of files to use for bad pixel self-calibration, find which\n",
    "    # are appropriate to this detector and add to association\n",
    "    for file in selfcalfiles:\n",
    "        with fits.open(file) as hdu:\n",
    "            hdu.verify()\n",
    "            if (hdu[0].header['CHANNEL'] == this_channel):\n",
    "                asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})                \n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893f72c",
   "metadata": {},
   "source": [
    "Find and sort all of the input files, ensuring use of absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97390df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sstring = os.path.join(det1_dir, 'jw*mirifu*rate.fits')  # Use files from the detector1 output folder\n",
    "ratefiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(ratefiles)):\n",
    "    ratefiles[ii] = os.path.abspath(ratefiles[ii])\n",
    "ratefiles = np.array(ratefiles)\n",
    "# Check that these are the band/channel to use\n",
    "ratefiles = select_ch_band_files(ratefiles, use_ch, use_band)\n",
    "\n",
    "# Background Files\n",
    "sstring = os.path.join(det1_bgdir, 'jw*mirifu*rate.fits')\n",
    "bgfiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(bgfiles)):\n",
    "    bgfiles[ii] = os.path.abspath(bgfiles[ii])\n",
    "bgfiles = np.array(bgfiles)\n",
    "# Check that these are the band/channel to use\n",
    "bgfiles = select_ch_band_files(bgfiles, use_ch, use_band)\n",
    "\n",
    "# Define any files to use for self-calibration (if step enabled)\n",
    "# Typically this is all science and background exposures\n",
    "selfcalfiles = ratefiles.copy()\n",
    "selfcalfiles = np.append(selfcalfiles, bgfiles)\n",
    "\n",
    "print('Found ' + str(len(ratefiles)) + ' science files')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files')\n",
    "print('Found ' + str(len(selfcalfiles)) + ' potential selfcal files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefec2c",
   "metadata": {},
   "source": [
    "Step through each of the science files, using relevant associated backgrounds in calwebb_spec2 processing.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "The background files are used in this step to perform pixel-based background subtraction (if desired), otherwise background subtraction is done later with Spec3 files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa076d",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# To save runtime, make a new version of our spec2 parameter dictionary\n",
    "# that turns off creation of quicklook cubes and 1d spectra for science\n",
    "# data\n",
    "spec2dict_sci = copy.deepcopy(spec2dict)\n",
    "spec2dict_sci['cube_build']['skip'] = True\n",
    "spec2dict_sci['extract_1d']['skip'] = True\n",
    "\n",
    "if dospec2:\n",
    "    for file in ratefiles:\n",
    "        asnfile = os.path.join(sci_dir, 'l2asn.json')\n",
    "        writel2asn(file, bgfiles, selfcalfiles, asnfile, 'Level2')\n",
    "        Spec2Pipeline.call(asnfile, steps=spec2dict_sci, save_results=True, output_dir=spec2_dir)\n",
    "else:\n",
    "    print('Skipping Spec2 processing for SCI data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e3bd3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Reduce the backgrounds individually.  This will be needed for the Master Background step in calwebb_spec3, but is unnecessary if doing calwebb_spec2 pixel based background instead.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3199a",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "if dospec2bg:\n",
    "    for file in bgfiles:\n",
    "        asnfile = os.path.join(bg_dir, 'l2asn.json')\n",
    "        writel2asn(file, '', selfcalfiles, asnfile, 'Level2')\n",
    "        Spec2Pipeline.call(asnfile, steps=spec2dict, save_results=True, output_dir=spec2_bgdir)\n",
    "else:\n",
    "    print('Skipping Spec2 processing for BG data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Spec2: {time1 - time_spec2} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6415",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64771f88",
   "metadata": {},
   "source": [
    "7.<font color='white'>-</font>Spec3 Pipeline<a class=\"anchor\" id=\"spec3\"></a>\n",
    "------------------\n",
    "In this section we'll run the Spec3 (calwebb_spec3) pipeline to produce a\n",
    "composite data cube and extracted spectrumfrom all dithered exposures.\n",
    "We will need to create an association file from all science (*cal.fits)\n",
    "and background (*x1d.fits) data in order for the pipeline to use them\n",
    "appropriately.<br>\n",
    "\n",
    "Note that the data cubes created by the JWST pipeline are in SURFACE\n",
    "BRIGHTNESS units (MJy/steradian), not flux units. What that means is\n",
    "that if you intend to sum spectra within an aperture you need to be sure\n",
    "to multiply by the pixel area in steradians first in order to get a\n",
    "spectrum in flux units. This correction is already build into the pipeline\n",
    "Extract1D algorithm.\n",
    "The nominal pixel area in steradians is provided in the \n",
    "<code>PIXAR_SR</code> keyword and can be found in the SCI\n",
    "extension header.<BR>\n",
    "\n",
    "Spectral extraction for point sources uses a conical aperture extraction whose radius increases with wavelength, with annular background subtraction and aperture correction.  Spectral extraction for extended sources sums the entire image at each wavelength plane (note this is different for each channel). <BR>\n",
    "\n",
    "See https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing/calwebb_spec3\n",
    "    \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "If master background subtraction was selected above this will be applied\n",
    "during this stage.<BR><BR>\n",
    "To override certain steps and reference files use the examples below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b245a-070d-4e86-ad22-5c62ee35ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec3 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "spec3dict = {}\n",
    "spec3dict['assign_mtwcs'], spec3dict['master_background'], spec3dict['outlier_detection'], spec3dict['mrs_imatch'], spec3dict['cube_build'] = {}, {}, {}, {}, {}\n",
    "spec3dict['pixel_replace'], spec3dict['extract_1d'], spec3dict['spectral_leak'] = {}, {}, {}\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#spec3dict['outlier_detection']['skip'] = True\n",
    "\n",
    "# Master background usage was set up above, propagate that here\n",
    "if (master_bg is True):\n",
    "    spec3dict['master_background']['skip'] = False\n",
    "else:\n",
    "    spec3dict['master_background']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#spec3dict['cube_build']['override_cubepar'] = 'myfile.fits'  # Cube-building parameters\n",
    "#spec3dict['extract_1d']['override_extract1d'] = 'myfile.asdf'  # Spectral extraction parameters (ASDF file)\n",
    "#spec3dict['extract_1d']['override_apcorr'] = 'myfile.asdf'  # Aperture correction parameters (ASDF file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0af713",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Set certain parameters here to:\n",
    "\n",
    "* adjusting performance for the outlier detection step\n",
    "* adjust the cube building step\n",
    "* adjust the 1d spectral extraction\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08929a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options for adjusting performance for the outlier detection step\n",
    "#spec3dict['outlier_detection']['kernel_size'] = '11 1'  # Dial this to adjust the detector kernel size\n",
    "#spec3dict['outlier_detection']['threshold_percent'] = 99.5  # Dial this to be more/less aggressive in outlier flagging (values closer to 100% are less aggressive)\n",
    "\n",
    "# Run pixel replacement code to extrapolate values for otherwise bad pixels\n",
    "# This can help mitigate 5-10% negative dips in spectra of bright sources\n",
    "# Use the 'mingrad' algorithm\n",
    "spec3dict['pixel_replace']['skip'] = False\n",
    "spec3dict['pixel_replace']['algorithm'] = 'mingrad'\n",
    "#spec3dict['pixel_replace']['save_results'] = True # Enable if desired to write out these files for spot checking\n",
    "\n",
    "# Options for adjusting the cube building step\n",
    "#spec3dict['cube_build']['output_file'] = 'mycube'  # Custom output name\n",
    "spec3dict['cube_build']['output_type'] = 'band'  # 'band', 'channel' (default), or 'multi' type cube output.  'band' is best for 1d residual fringe correction.\n",
    "#spec3dict['cube_build']['channel'] = '1'  # Build everything from just channel 1 into a single cube (we could also choose '2','3','4', or 'ALL')\n",
    "#spec3dict['cube_build']['weighting'] = 'drizzle'  # Algorithm used: 'emsm' or 'drizzle' (default)\n",
    "#spec3dict['cube_build']['coord_system'] = 'ifualign'  # Cube rotation: 'ifualign', 'skyalign' (default), or 'internal_cal'\n",
    "#spec3dict['cube_build']['scalexy'] = 0.5  # Output cube spaxel scale (arcsec) if setting it by hand\n",
    "#spec3dict['cube_build']['scalew'] = 0.002  # Output cube voxel depth in wavelength (micron) if setting it by hand\n",
    "#spec3dict['cube_build']['ra_center'] = 65.0  # Force cube to be centered at this R.A.\n",
    "#spec3dict['cube_build']['dec_center'] = -35.0  # Force cube to be centered at this Decl.\n",
    "#spec3dict['cube_build']['cube_pa'] = 45.0  # Force cube to have this position angle\n",
    "#spec3dict['cube_build']['nspax_x'] = 61  # Force cube to have this number of spaxels in cube X direction\n",
    "#spec3dict['cube_build']['nspax_y'] = 61  # Force cube to have this number of spaxels in cube Y direction\n",
    "#spec3dict['cube_build']['wavemin'] = 4.8  # Custom minimum wavelength for the cube\n",
    "#spec3dict['cube_build']['wavemax'] = 6.3  # Custom maximum wavelength for the cube\n",
    "\n",
    "# Options for adjusting the 1d spectral extraction\n",
    "#spec3dict['extract_1d']['ifu_set_srctype'] = 'POINT' # Force a certain type of spectral extraction ('POINT' or 'EXTENDED')\n",
    "#spec3dict['extract_1d']['ifu_rscale'] = 2  # Number of FWHM to use for point-source conical aperture extraction radius (default is 2)\n",
    "spec3dict['extract_1d']['ifu_autocen'] = True  # Enable auto-centering of the extraction aperture (default is True)\n",
    "#spec3dict['extract_1d']['center_xy'] = (20,20)  # Override aperture location if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1952f",
   "metadata": {},
   "source": [
    "Define a function to create association files for Stage 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "\n",
    "    # Add background files to the association\n",
    "    for file in bgfiles:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'background'})\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb1e41",
   "metadata": {},
   "source": [
    "Find and sort all of the input files, ensuring use of absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science Files need the cal.fits files\n",
    "sstring = os.path.join(spec2_dir, 'jw*mirifu*_cal.fits')\n",
    "calfiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(calfiles)):\n",
    "    calfiles[ii] = os.path.abspath(calfiles[ii])\n",
    "calfiles = np.array(calfiles)\n",
    "# Check that these are the band/channel to use\n",
    "calfiles = select_ch_band_files(calfiles, use_ch, use_band)\n",
    "\n",
    "# Background Files need the x1d.fits files for Master Background subtraction\n",
    "sstring = os.path.join(spec2_bgdir, 'jw*mirifu*x1d.fits')\n",
    "bgfiles = sorted(glob.glob(sstring))\n",
    "for ii in range(0, len(bgfiles)):\n",
    "    bgfiles[ii] = os.path.abspath(bgfiles[ii])\n",
    "bgfiles = np.array(bgfiles)\n",
    "# Check that these are the band/channel to use\n",
    "bgfiles = select_ch_band_files(bgfiles, use_ch, use_band)\n",
    "\n",
    "print('Found ' + str(len(calfiles)) + ' science files to process')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files to process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad933a4",
   "metadata": {},
   "source": [
    "Make an association file that includes all of the different exposures. If using Master Background subtraction include the background data.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note that science data must be of type cal.fits and background exposures must be of type x1d.fits\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9918b7",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "asnfile = os.path.join(sci_dir, 'l3asn.json')\n",
    "if dospec3:\n",
    "    writel3asn(calfiles, bgfiles, asnfile, 'Level3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280528b8",
   "metadata": {},
   "source": [
    "Run calwebb_spec3 using the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c035f3",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "if dospec3:\n",
    "    Spec3Pipeline.call(asnfile, steps=spec3dict, save_results=True, output_dir=spec3_dir)\n",
    "else:\n",
    "    print('Skipping Spec3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Spec3: {time1 - time_spec3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426db95",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966d5ff",
   "metadata": {},
   "source": [
    "8.<font color='white'>-</font>Plot the spectra<a class=\"anchor\" id=\"plots\"></a>\n",
    "------------------\n",
    "Here we'll plot the spectra to see what our source looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf94f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    # Find and sort all of the input files\n",
    "\n",
    "    # Science Files\n",
    "    # Use the final extracted spectra (x1d.fits)\n",
    "    sstring = sorted(glob.glob(os.path.join(spec3_dir, '*x1d.fits')))\n",
    "    x1dfiles = np.array(sorted(sstring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    # Make normal plots\n",
    "    %matplotlib inline\n",
    "    # Interactive plots\n",
    "    #%matplotlib notebook\n",
    "\n",
    "    rc('axes', linewidth=2)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 3), dpi=150)\n",
    "\n",
    "    if (len(x1dfiles) > 0):\n",
    "        hdu = fits.open(x1dfiles[0])\n",
    "        objname = hdu[0].header['TARGPROP']\n",
    "        hdu.close()\n",
    "    else:\n",
    "        objname = 'Unknown'\n",
    "\n",
    "    ymin, ymax = np.nan, np.nan\n",
    "    for file in x1dfiles:\n",
    "        x1d = fits.open(file)\n",
    "        x1ddata = x1d[1].data\n",
    "        wave = x1ddata['WAVELENGTH']\n",
    "        # MRS x1d files have both regular ('flux') and residual-fringe (RF) corrected ('rf_flux') spectra.\n",
    "        # The RF-corrected spectra will have NaN values if RF correction was disabled or failed to converge.\n",
    "        # Plot the RF corrected spectrum if available, otherwise plot the regular spectrum.\n",
    "        if np.nansum(x1ddata['RF_FLUX'] != 0):\n",
    "            flux = x1ddata['RF_FLUX']\n",
    "        else:\n",
    "            flux = x1ddata['FLUX']\n",
    "        ymin = np.nanmin([ymin, np.nanpercentile(flux, 2)])\n",
    "        ymax = np.nanmax([ymax, np.nanpercentile(flux, 99.5)])\n",
    "\n",
    "        # labels\n",
    "        label = x1d[0].header['CHANNEL'] + x1d[0].header['BAND']\n",
    "\n",
    "        plt.plot(wave, flux, label=label)\n",
    "\n",
    "        x1d.close()\n",
    "\n",
    "    plt.xlabel(r'Wavelength ($\\mu$m)')\n",
    "    plt.ylabel('Flux (Jy)')\n",
    "    plt.title(objname)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.legend(fontsize=8, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mrs_example_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e2553",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d16ec",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_footer.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
