{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2904b3c4-177a-4462-b15a-2f7e18d8be31",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5382792-7180-4bfb-907e-1e5757dd8727",
   "metadata": {},
   "source": [
    "# NIRISS Wide Field Slitless Spectroscopy (WFSS) Pipeline Notebook\n",
    "\n",
    "**Authors**: R. Plesha<br>\n",
    "**Last Updated**: September 10, 2025<br>\n",
    "**Pipeline Version**: 1.19.1 (Build 12.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92487e8-9a90-40db-9336-da4b8f6bb823",
   "metadata": {},
   "source": [
    "# **Purpose**:\n",
    "\n",
    "This notebook provides a framework for processing generic Near-Infrared Imager and Slitless Spectrograph (NIRISS) wide field slitless spectroscopy (WFSS) data through the James Webb Space Telescope (JWST) pipeline. Data from a single proposal and observation ID is assumed to be located in one observation folder according to paths set up below. It should not be necessary to edit any cells other than in the [Configuration](#1.-Configuration) section unless modifying the standard pipeline processing options. Additional notebooks showing how to optimize and modify sources being extracted for NIRISS WFSS data can be found on the [JDAT notebooks github](https://github.com/spacetelescope/jdat_notebooks/tree/main/notebooks/NIRISS/NIRISS_WFSS_advanced).\n",
    "\n",
    "**Data**:\n",
    "This example uses data from the [Program ID 2079](https://www.stsci.edu/jwst/science-execution/program-information?program=2079) observation 004 (PI: Finkelstein) observing the Hubble Ultra Deep Field (HUDF). The observations are in three [NIRISS filters](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-pupil-and-filter-wheels): F115W, F150W, and F200W use both GR150R and GR150C [grisms](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-gr150-grisms). In this example we are only looking at data from one of the two observations using the F200W filter. A [NIRISS WFSS observation sequence](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-observing-strategies/niriss-wfss-recommended-strategies) typically consists of a direct image followed by a grism observation in the same blocking filter to help identify the sources in the field. In program 2079, the exposure sequence follows the pattern: direct image -> GR150R -> direct image -> GR150C -> direct image.\n",
    "\n",
    "Example input data to use will be downloaded automatically unless disabled by the `dodownload` parameter (i.e., to use local files instead).\n",
    "\n",
    "**JWST pipeline version and CRDS context**: \n",
    "This notebook was written for the calibration pipeline version given above. The JWST Calibration Reference Data System (CRDS) context used is associated with the pipeline version as listed [here](https://jwst-crds.stsci.edu/display_build_contexts/). If you use different pipeline version or CRDS context, please read the relevant release notes ([here for pipeline](https://github.com/spacetelescope/jwst/releases), [here for CRDS](https://jwst-crds.stsci.edu/display_context_history/)) for possibly relevant changes. The results of this notebook may differ from the products hosted on the [MAST archive](https://mast.stsci.edu/search/ui/#/jwst) depending on the pipeline version and CRDS context you are using.\n",
    "\n",
    "**Updates**:\n",
    "This notebook is regularly updated as improvements are made to the pipeline. Find the most up to date version of this notebook at: https://github.com/spacetelescope/jwst-pipeline-notebooks/\n",
    "\n",
    "**Recent Changes**:<br>\n",
    "September 10, 2025: original notebook released<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff20aae-383e-4799-a667-8291b249b08f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7afd0-ce14-43f5-8fad-82d471465e8d",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Configuration](#1.-Configuration) \n",
    "2. [Package Imports](#2.-Package-Imports)\n",
    "3. [Directory Setup](#3.-Directory-Setup)\n",
    "4. [Demo Mode Setup (Data Download)](#4.-Demo-Mode-Setup-(Data-Download))\n",
    "5. [Detector 1 Pipeline](#5.-Detector1-Pipeline)\n",
    "6. [Image2 Pipeline](#6.-Image2-Pipeline)\n",
    "7. [Image3 Pipeline](#7.-Image3-Pipeline)\n",
    "8. [Visualize the source catalog](#8.-Visualize-the-source-catalog)\n",
    "9. [Spec2 Pipeline](#9.-Spec2-Pipeline)\n",
    "10. [Spec3 Pipeline](#10.-Spec3-Pipeline)\n",
    "11. [Visualize the Spectra](#11.-Visualize-the-spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34701d-f980-4bc4-93f6-94f8db4c39f3",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b817ff-06c6-44c3-b8dd-4ace478df95c",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "------------------\n",
    "Set basic configuration for running notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05157d9-a3ba-41aa-9118-0d9df5b03c8a",
   "metadata": {},
   "source": [
    "#### Install dependencies and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c0246-4681-405e-82f4-bb448615cb9f",
   "metadata": {},
   "source": [
    "To make sure that the pipeline version is compatabile with the steps\n",
    "discussed below and the required dependencies and packages are installed,\n",
    "you can create a fresh conda environment and install the provided\n",
    "`requirements.txt` file:\n",
    "```\n",
    "conda create -n niriss_wfss_pipeline python=3.12\n",
    "conda activate niriss_wfss_pipeline\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Set the basic parameters to use with this notebook. These will affect\n",
    "what data is used, where data is located (if already in disk), names of any outputs, and\n",
    "pipeline modules run in this data. The list of parameters are:\n",
    "\n",
    "* demo_mode\n",
    "* sci_dir (directory where the data is / will be)\n",
    "* dodownload (download the data locally)\n",
    "* pipeline modules:\n",
    "  * dodet1 (run detector1)\n",
    "  * doimage2 (run image2)\n",
    "  * doimage3 (run image3)\n",
    "  * dospec2 (run spec2)\n",
    "  * dospec3 (run spec3)\n",
    "* doviz (show visualizations of the data within the notebook)\n",
    "* program (proposal ID of your data for the level 3 association files)\n",
    "* sci_observtn (observation of your data for level 3 the association files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57829d6-848d-45eb-8988-2f73b869524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import necessary for configuration\n",
    "import os\n",
    "\n",
    "# establishing what directory we're currently working in\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83f50b-5797-4252-9d58-5a69d45af8ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Adjust any parameters in the cell directly below this before running to ensure <code>demo_mode</code> runs correctly.\n",
    "</div>\n",
    "\n",
    "Set <code>demo_mode = True</code> to run in demonstration mode. In this mode this notebook will download example data from the Barbara A.\n",
    "Mikulski Archive for Space Telescopes (MAST) and process everything through the pipeline. This will all happen in a local directory unless modified in the configuration below (variable `data_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5906221-f721-49ac-b55d-6ea7e602c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------Demo Mode---------------------------------\n",
    "# use the provided example for demonstration purposes\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    program = 2079\n",
    "    sci_observtn = '004' # as an example; 001 also exists for this program\n",
    "\n",
    "    # creating a directory for the data called \"nis_wfss_demo_data\" \n",
    "    #   located in the directory you are currently in\n",
    "    data_dir = os.path.join(cwd, 'nis_wfss_demo_data')\n",
    "    sci_dir = os.path.join(data_dir, f\"PID{program}/obs{sci_observtn}\")\n",
    "\n",
    "    print(f'Running in demonstration mode using example data from program {program} obs{sci_observtn}!')\n",
    "    print(f'Data located in: {sci_dir}')\n",
    "\n",
    "    # you will want to download the demo data\n",
    "    dodownload = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ced1b2-de14-4a97-92c5-7c35c0fe2a3f",
   "metadata": {},
   "source": [
    "Set <code>demo_mode = False</code> if you want to process your own data that has already been downloaded. To do so, in the cell below, provide the program ID in the `program` variable, the observation ID in the `sci_observtn` variable, and the top path level location of the data in the `sci_dir` variable. The notebook expects that the `uncal` files are in a directory under `sci_dir` called `uncal`.\n",
    "\n",
    "If you would like to additionally download the data for a specific program through this notebook, you can additionally set the `dodownload` variable to True below, and the data will be downloaded to the provided `sci_dir` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0e6f2-9f0f-4be8-bb3d-687f471dc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------User Mode Directories------------------------\n",
    "# If demo_mode = False, look for user data in these paths\n",
    "if not demo_mode:\n",
    "    # Set directory paths for processing specific data; these will need\n",
    "    # to be changed to your local directory setup (below are given as\n",
    "    # examples)\n",
    "    user_home_dir = os.path.expanduser('~')\n",
    "\n",
    "    # Point to where science observation data are\n",
    "    # Assumes uncalibrated data in sci_dir/uncal/ and results in stage1,\n",
    "    # stage2, stage3 directories\n",
    "    program = 2079 # modify this to your specific program\n",
    "    sci_observtn = '004' # modify this to your specific program\n",
    "    sci_dir = os.path.join(user_home_dir, f'nis_wfss_demo_data/PID{program}/obs{sci_observtn}/')\n",
    "    dodownload = False # if you would like to download your data using astroquery, set to True & don't skip Demo mode setup section\n",
    "\n",
    "    print(f'Running using user input data from: {sci_dir} for program {program} obs{sci_observtn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cad701-4001-468e-b24c-9903cc217ebf",
   "metadata": {},
   "source": [
    "Set any of the variables below to be True (do the processing) or False (don't do the processing). To run this notebook from start to completion, it is expected that the output products from each of the stages below are located in the appropriate directories as set in [#3.-Directory Setup](#3.-Directory-Setup). If these output products do not exist, any of the later stages of the pipeline may not work as intended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6c55b-206a-400a-a28c-950ea92e761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------Set Processing Steps--------------------------\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# visualization of products within the notebook\n",
    "doviz = True # Visualize outputs\n",
    "\n",
    "# Science processing\n",
    "dodet1 = True  # calwebb_detector1; files saved in \"stage1\" directory\n",
    "doimage2 = True  # calwebb_image2; files saved in \"stage2_img\" directory\n",
    "doimage3 = True  # calwebb_image3; files saved in \"stage3_img\" directory\n",
    "dospec2 = True # calwebb_spec2; files saved in \"stage2_spec\" directory\n",
    "dospec3 = True # calwebb_spec3; files saved in \"stage3_spec\" directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692c95f-46fa-4275-baab-41082d7e3473",
   "metadata": {},
   "source": [
    "### Set CRDS context and server\n",
    "Before importing <code>CRDS</code> and <code>JWST</code> modules, we need to configure our environment. This includes defining a CRDS cache directory in which to keep the reference files that will be used by the calibration pipeline. The pipeline will fetch and download the needed reference files to this directory.\n",
    "\n",
    "If the root directory for the local CRDS cache directory has not been set already, it will be set to create one in the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a5f1a-5888-4836-b4b1-68e172627207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Set CRDS context and paths----------------------\n",
    "\n",
    "# Set CRDS context (if overriding to use a specific version of reference\n",
    "# files; leave commented out to use latest reference files by default)\n",
    "# %env CRDS_CONTEXT  jwst_1413.pmap\n",
    "\n",
    "# Check whether the local CRDS cache directory has been set.\n",
    "# If not, set it to the user home directory\n",
    "if (os.getenv('CRDS_PATH') is None):\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds')\n",
    "# Check whether the CRDS server URL has been set.  If not, set it.\n",
    "if (os.getenv('CRDS_SERVER_URL') is None):\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Echo CRDS path in use\n",
    "print(f\"CRDS local filepath: {os.environ['CRDS_PATH']}\")\n",
    "print(f\"CRDS file server: {os.environ['CRDS_SERVER_URL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca8bb0-dd9f-4bfb-9b6a-588bbb077a66",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb23955-6cda-4eab-aabc-0093fdfe5aed",
   "metadata": {},
   "source": [
    "## 2. Package Imports\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9ad1b-0b15-48a2-8cac-b3a95f2433e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "# ----------------------General Imports------------------------------------\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Data calculations and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------Plotting Imports----------------------------------\n",
    "from matplotlib import pyplot as plt\n",
    "# interactive plots within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# -----------------------Astronomy Imports--------------------------------\n",
    "# ASCII files, and downloading demo files\n",
    "from astroquery.mast import MastMissions\n",
    "from astroquery.mast.utils import remove_duplicate_products\n",
    "\n",
    "# Astropy routines for visualizing detected sources:\n",
    "from astropy.io import fits\n",
    "from astropy.io.fits import getheader\n",
    "from astropy.table import Table, vstack\n",
    "\n",
    "# for JWST calibration pipeline\n",
    "import jwst\n",
    "import crds\n",
    "\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Image2Pipeline\n",
    "from jwst.pipeline import Image3Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels\n",
    "from jwst.associations import asn_from_list  # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase  # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base  # Definition of a Lvl3 association file\n",
    "\n",
    "# Echo pipeline version and CRDS context in use\n",
    "print(f\"JWST Calibration Pipeline Version: {jwst.__version__}\")\n",
    "print(f\"Using CRDS Context: {crds.get_context_name('jwst')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75c9a4-c030-436e-af5f-c1f291d73f77",
   "metadata": {},
   "source": [
    "### Define convenience functions\n",
    "\n",
    "These functions are used within the notebook and assist with plotting, finding the appropriate extension for a specific source in spec2 cal data, and verifying what steps and reference files were used for a provided file. These may be useful for your own analysis outside of this notebook, but are written for this notebook in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0b979-53b1-48f6-bf07-3cc5d6f98693",
   "metadata": {},
   "source": [
    "#### Plotting Spec2 & Spec3 convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ee940-d127-4842-bfe5-e1073a5dfaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used to plot the i2d image for a specific source as well as the catalog x/y centroid for that source\n",
    "def plot_i2d_plus_source(catname, source_id, ax):\n",
    "    # open the i2d & catalog and find the associated source number            \n",
    "    i2dname = catname.replace('cat.ecsv', 'i2d.fits')\n",
    "    \n",
    "    cat = Table.read(catname)\n",
    "    cat_line = cat[cat['label'] == source_id]\n",
    "    \n",
    "    # plot the image\n",
    "    with fits.open(i2dname) as i2d:\n",
    "        display_vals = [np.nanpercentile(i2d[1].data, 1), np.nanpercentile(i2d[1].data, 98)]\n",
    "        ax_i2d.imshow(i2d[1].data, vmin=display_vals[0], vmax=display_vals[1], origin='lower', cmap='gist_gray_r')\n",
    "    \n",
    "    # plot up the source catalog\n",
    "    xcentroid = cat_line['xcentroid'][0]\n",
    "    ycentroid = cat_line['ycentroid'][0]\n",
    "    ax.set_xlim(xcentroid-20, xcentroid+20)\n",
    "    ax.set_ylim(ycentroid-20, ycentroid+20)\n",
    "    if cat_line['is_extended'] is True:\n",
    "        cat_color = 'deepskyblue'\n",
    "        cat_marker = 'o'\n",
    "    else:\n",
    "        cat_color = 'deeppink'\n",
    "        cat_marker = 's'\n",
    "    ax.scatter(xcentroid, ycentroid, s=20, facecolors='None', edgecolors=cat_color, marker=cat_marker, alpha=0.9)\n",
    "    ax.annotate(source_id, \n",
    "                (xcentroid+0.5, ycentroid+0.5), \n",
    "                fontsize=10,\n",
    "                color=cat_color)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77dc1ca-196f-452f-846d-a02ff68e5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to plot the wavelength vs. flux values for x1d & c1d spectra for a specific source\n",
    "def plot_spectrum(specfile, source_fluxes, ax, image3_dir, ext=1, legend=True):\n",
    "\n",
    "    # trimming off some of the edges where the flux is unreliable\n",
    "    plot_limits = {'F090W': {'wavemin': 0.85, 'wavemax': 0.9},\n",
    "                   'F115W': {'wavemin': 0.9, 'wavemax': 1.25},\n",
    "                   'F150W': {'wavemin': 1.35, 'wavemax': 1.65},\n",
    "                   'F200W': {'wavemin': 1.75, 'wavemax': 2.2},\n",
    "                   'F140M': {'wavemin': 1.25, 'wavemax': 1.5},\n",
    "                   'F158M': {'wavemin': 1.45, 'wavemax': 1.65},\n",
    "                   }\n",
    "\n",
    "    with fits.open(specfile) as spec:\n",
    "\n",
    "        # pull out relevant keywords\n",
    "        grism = spec[0].header['FILTER']\n",
    "        pupil = spec[0].header['PUPIL']\n",
    "        catname = os.path.join(image3_dir, spec[0].header['SCATFILE'])\n",
    "        try:\n",
    "            label = f\"{grism} dither {spec[0].header['DIT_PATT']}\"\n",
    "        except KeyError:\n",
    "            label = f\"{grism}\" # there is no dither in the c1d files\n",
    "\n",
    "        # find where in the file the source data are\n",
    "        wh_spec_source = np.where(spec[ext].data['SOURCE_ID'] == source_id)[0]\n",
    "        \n",
    "        # if the source isn't in the file, then return a blank axis\n",
    "        if not len(wh_spec_source):\n",
    "            print(f'Source {source_id} not found in {specfile}')\n",
    "            return ax, catname, source_fluxes, grism\n",
    "                  \n",
    "        # grab the wavelength & flux data and trim off the edges for visalization purposes\n",
    "        wave = spec[ext].data['WAVELENGTH'][wh_spec_source]\n",
    "        flux = spec[ext].data['FLUX'][wh_spec_source]\n",
    "        \n",
    "        wavemin = plot_limits[pupil]['wavemin']\n",
    "        wavemax = plot_limits[pupil]['wavemax']\n",
    "        wh_wave = np.where((wave >= wavemin) & (wave <= wavemax)) # cutting off the edges\n",
    "        wave = wave[wh_wave]\n",
    "        flux = flux[wh_wave]\n",
    "\n",
    "        if len(flux[np.isnan(flux)]) == len(flux):\n",
    "            print(f'There are no valid pixels for {os.path.basename(specfile)} source {source_id} {grism}. Source likely on edge of detector; not plotting')\n",
    "        else:\n",
    "            source_fluxes.extend(flux) # keep the flux to set the limits of the plot later\n",
    "    \n",
    "    if grism == 'GR150R':\n",
    "        linestyle = '-'\n",
    "    else:\n",
    "        linestyle = '--'\n",
    "\n",
    "    ax.plot(wave, flux, label=label, ls=linestyle)\n",
    "    if legend:\n",
    "        ax.legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "    return ax, catname, source_fluxes, grism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0959686-5baf-47c0-82ad-04f6a49608bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is used to plot the spec2 cal files for a specific source\n",
    "def plot_spec2_cal(x1dfile, source_id, ax, transpose=False):\n",
    "\n",
    "    cal_file = x1dfile.replace('x1d.fits', 'cal.fits')\n",
    "    with fits.open(cal_file) as cal_hdu:\n",
    "        wh_cal = find_source_ext(cal_hdu, source_id)\n",
    "\n",
    "        # if the source isn't in the file, then return a blank axis\n",
    "        if wh_cal == -999:\n",
    "            print(f'Source {source_id} not found in {cal_file}')\n",
    "            return ax\n",
    "            \n",
    "        if transpose is True:\n",
    "            # we flip the GR150R data so that we can look at the two cal images along the same dispersion axis\n",
    "            cal_data = np.transpose(cal_hdu[wh_cal].data)\n",
    "        else:\n",
    "            cal_data = cal_hdu[wh_cal].data\n",
    "\n",
    "        cal_display_vals = [np.nanpercentile(cal_data, 5), np.nanpercentile(cal_data, 90)]        \n",
    "        ax.imshow(cal_data, vmin=cal_display_vals[0], vmax=cal_display_vals[1], origin='lower', aspect='auto')\n",
    "\n",
    "        # the dispersion is in the -x direction, so flip the axis for ease in visualization\n",
    "        ax.invert_xaxis()\n",
    "   \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4267e40-8274-4c20-97cd-3aea0f8644b3",
   "metadata": {},
   "source": [
    "#### Other convienence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d23c1-aa2f-4659-999c-21f70b2cb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to use to find the extension the source is located in the cal files\n",
    "def find_source_ext(cal_hdu, source_id, info=True):    \n",
    "    # look for cal extension, too, but only in the SCI extension; \n",
    "    # fill in with a source ID of -999 for all other extensions to get the right extension value\n",
    "    cal_source_ids = np.array([cal_hdu[ext].header['SOURCEID'] if cal_hdu[ext].header['EXTNAME'] == 'SCI'\n",
    "                               else -999 for ext in range(len(cal_hdu))[1:-1]]) \n",
    "\n",
    "    try:\n",
    "        wh_cal = np.where(cal_source_ids == source_id)[0][0] + 1 # need to add 1 for the primary header\n",
    "    except IndexError:\n",
    "        # this source doesn't exist\n",
    "        return -999\n",
    "\n",
    "    if info:\n",
    "        print(f\"Extension {wh_cal} in {cal_hdu[0].header['FILENAME']} contains the data for source {source_id} from our catalog\")\n",
    "\n",
    "    return wh_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93df0a6-3b60-4f2f-bb6e-86d92f9bd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to quickly see all of the steps that were run on a specified file\n",
    "def check_steps_run(filename):\n",
    "    \n",
    "    # Read in file as datamodel\n",
    "    dm = datamodels.open(filename)\n",
    "    \n",
    "    # Check which steps were run\n",
    "    print(f\"{dm.meta.filename} - {dm.meta.exposure.type}\")\n",
    "    for step, status in dm.meta.cal_step.instance.items():\n",
    "        print(f\"{step}: {status}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d022b3-85ca-4153-84b0-a985b6d924d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to quickly see all of the reference files that were used on a specified file\n",
    "def check_ref_file_used(filename):\n",
    "\n",
    "    # Read in file as datamodel\n",
    "    dm = datamodels.open(filename)\n",
    "\n",
    "    # Check which reference files were used\n",
    "    print(f\"{dm.meta.filename} - {dm.meta.exposure.type}\")\n",
    "    for step, status in dm.meta.ref_file.instance.items():\n",
    "        print(f\"{step}: {status}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa58a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747bd62-32b6-464b-8cad-45494e86b92c",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b0acc",
   "metadata": {},
   "source": [
    "# 3. Directory Setup\n",
    "------------------\n",
    "Set up detailed paths to input/output stages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8040c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output subdirectories to keep science data products organized\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "det1_dir = os.path.join(sci_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "image2_dir = os.path.join(sci_dir, 'stage2_img')  # calwebb_image2 pipeline outputs will go here\n",
    "image3_dir = os.path.join(sci_dir, 'stage3_img')  # calwebb_image3 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(sci_dir, 'stage2_spec')  # calwebb_spec2 pipeline outputs will go here\n",
    "spec3_dir = os.path.join(sci_dir, 'stage3_spec')  # calwebb_spec3 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "for cal_dir in [sci_dir, uncal_dir, det1_dir, image2_dir, image3_dir, spec2_dir, spec3_dir]:\n",
    "    os.makedirs(cal_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72421b4",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d43e3-69e2-47c4-a4e5-c6c1f23fa366",
   "metadata": {},
   "source": [
    "# 4. Demo Mode Setup (Data Download)\n",
    "------------------\n",
    "\n",
    "If running in demonstration mode, we will retrieve uncalibrated data automatically from MAST using [astroquery](https://astroquery.readthedocs.io/en/latest/mast/mast.html). Here we will be using the [MastMissions](https://spacetelescope.github.io/mast_notebooks/notebooks/multi_mission/missions_mast_search/missions_mast_search.html) interface which allows for flexibility in search criteria, and is equivalent to using the [JWST mission specific search](https://mast.stsci.edu/search/ui/#/jwst) on MAST. <br>\n",
    "\n",
    "For illustrative purposes, we focus on data taken through the NIRISS [F200W filter](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-filters) and start with uncalibrated data products, or `_uncal` files. To search for additional filters, update the `niriss_pupil` field in `query_criteria` to be a comma separated list of filters in a single string value, i.e. \"F200W, F115W\". To search for a specific grism used, add the `opticalElements` field in `query_criteria`, setting the value equal to \"GR150R\" or \"GR150C\". Note that searching based on a specific grism will not download the associated direct images.\n",
    "\n",
    "Information about the JWST file naming conventions can be found at: https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html\n",
    "\n",
    "Note -- if for some reason this section does not work, this is equivalet to downloading the `_uncal.fits` files from this MAST search:<br>\n",
    "https://mast.stsci.edu/search/ui/#/jwst/results?instruments=NIRISS&program_id=2079&obs_id=004&custom_col_val_0=1b&custom_col_sel_1=niriss_pupil&custom_col_val_1=F200W&"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634f5a6-600a-41e8-ab9d-a58edf8973ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "This demo selects only filter <b>F200W</b> data by default; the demo mode for this observation contains data for the F115W and F150W filters, too\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a407f-0b9a-4139-8534-19896b58148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dodownload:\n",
    "    print(f'Using the Missions MAST interface to find data for Program {program} observation {sci_observtn}:')\n",
    "    missions = MastMissions(mission='jwst')\n",
    "\n",
    "    if demo_mode:\n",
    "        # query the data for demo mode; sometimes this step can take a bit of time\n",
    "        datasets = missions.query_criteria(instrume='NIRISS',  # From Near-Infrared Imager and Slitless Spectrograph\n",
    "                                           #opticalElements='GR150R', # uncomment to filter on only GR150R grism data (no direct images)\n",
    "                                           niriss_pupil='F200W',  # Download only the F200W filter data for this example\n",
    "                                           program=program,  # Proposal number 2079\n",
    "                                           observtn=sci_observtn, # observation 004\n",
    "                                           )\n",
    "    else:\n",
    "        # query the data for user input; sometimes this step can take a bit of time\n",
    "        datasets = missions.query_criteria(instrume='NIRISS',  # From Near-Infrared Imager and Slitless Spectrograph\n",
    "                                           program=program,  # As specified in the configuration\n",
    "                                           observtn=sci_observtn, # As specified in the configuration\n",
    "                                           )        \n",
    "\n",
    "    # get_product_list times out for large datasets, so this is a wrap around it so that it does not time out\n",
    "    batch_size = 5 # 5 is the reccomended number of batches\n",
    "    batches = [datasets[i:i+batch_size] for i in range(0, len(datasets), batch_size)]\n",
    "\n",
    "    for index, batch in enumerate(batches):\n",
    "        if index == 0:\n",
    "            product_df = missions.get_product_list(batch)\n",
    "        else:\n",
    "            temp_df = missions.get_product_list(batch)\n",
    "            product_df = vstack([product_df, temp_df])\n",
    "\n",
    "    products = remove_duplicate_products(product_df, 'filename')\n",
    "    print(f'Total number of unique products found: {len(products)}')\n",
    "\n",
    "    # filter down to only the files that we need from MAST\n",
    "    # other suffixes that may be useful to download are '_asn', '_pool', and '_rate'\n",
    "    files_to_download = missions.filter_products(products, file_suffix=['_uncal'])\n",
    "    \n",
    "    print(f'Total number of uncal files to download: {len(files_to_download)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcf963-3590-4252-9000-b6ef054d6287",
   "metadata": {},
   "source": [
    "Download all of the uncal files for the provided program, observation, and filter.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Warning: If this notebook is halted during this step the downloaded file\n",
    "may be incomplete, and cause crashes later on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b669f-e0db-4c00-8a08-fe4296fa25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dodownload:\n",
    "    print('Downloading the data:')\n",
    "    # download uncal file\n",
    "    manifest = missions.download_products(files_to_download, flat=True, download_dir=uncal_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba42ef-bce9-4c36-a62c-d6c98cc6fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time_download_end = time.perf_counter()\n",
    "print(f\"Runtime for downloading data: {(time_download_end - time0)/60:0.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b9e41c-4e56-4ded-a582-75b532ba3e1f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb4e97-7072-4bdf-ac14-9119221f4fab",
   "metadata": {},
   "source": [
    "# 5. Detector1 Pipeline\n",
    "------------------\n",
    "In this section we run the `*_uncal.fits` files through the [Detector1](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_detector1) stage of the pipeline to apply detector level calibrations and create a countrate data product where slopes are fit to the integration ramps. These `*_rate.fits` products are 2D (nrows x ncols), averaged over all integrations. 3D countrate data products (`*_rateints.fits`) are also created (nintegrations x nrows x ncols) which have the fitted ramp slopes for each integration.\n",
    "\n",
    "If there are no modifications to the steps at this stage needed, you can also save time by downloading these `*_rate.fits` files directly from MAST and starting at stage2. However, it is best to ensure that you are using the same pipeline version as MAST which can be checked in the `CAL_VER` header keyword. \n",
    "\n",
    "The parameters in each of the [Detector1 steps](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html#calwebb-detector1) can be modified from the default values, including overwriting reference files that are used. This dictionary of the modified parameters for each of the steps is then fed into the `steps` parameter of the `Detector1Pipeline` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eac4a-176d-4275-bd16-373102d46135",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_det1_start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09216b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Detector1 pipeline should be configured\n",
    "\n",
    "# this sets up any entry to det1dict to be a dictionary itself\n",
    "det1dict = defaultdict(dict)\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "# Example overrides for various reference files\n",
    "#   Files should be in the base local directory or provide full path\n",
    "# det1dict['dq_init']['override_mask'] = 'myfile.fits' # Bad pixel mask\n",
    "# det1dict['saturation']['override_saturation'] = 'myfile.fits' # Saturation\n",
    "# det1dict['linearity']['override_linearity'] = 'myfile.fits' # Linearity\n",
    "# det1dict['dark_current']['override_dark'] = 'myfile.fits' # Dark current subtraction\n",
    "# det1dict['jump']['override_gain'] = 'myfile.fits' # Gain used by jump step\n",
    "# det1dict['ramp_fit']['override_gain'] = 'myfile.fits' # Gain used by ramp fitting step\n",
    "# det1dict['jump']['override_readnoise'] = 'myfile.fits' # Read noise used by jump step\n",
    "# det1dict['ramp_fit']['override_readnoise'] = 'myfile.fits' # Read noise used by ramp fitting step\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "# Example overrides for whether or not certain steps should be skipped;\n",
    "# det1dict['persistence']['skip'] = True # skipping the persistence step\n",
    "\n",
    "# Example of turning on multi-core processing for the jump step (a single core is used by default).\n",
    "#   Choose what fraction of cores to use (quarter, half, all, or the default=1); This will speed up the calibration time\n",
    "# det1dict['jump']['maximum_cores'] = 'half'\n",
    "\n",
    "# Example of altering parameters to optimize removal of snowball residuals\n",
    "# det1dict['jump']['expand_large_events'] = True\n",
    "# det1dict['charge_migration']['signal_threshold'] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3dd5d-a3a1-43b3-b75a-3eac5c3c58fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncal_files = sorted(glob.glob(os.path.join(uncal_dir, '*_uncal.fits')))\n",
    "\n",
    "# Run Detector1 stage of pipeline, specifying:\n",
    "#   output directory to save *_rateints.fits files\n",
    "#   save_results flag set to True so the files are saved locally\n",
    "if dodet1:\n",
    "    for uncal in uncal_files:\n",
    "        rate_result = Detector1Pipeline.call(uncal, output_dir=det1_dir, steps=det1dict, save_results=True)\n",
    "else:\n",
    "    print('Skipping Detector1 processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b7650-14c3-47dd-ba47-8d79bd0be432",
   "metadata": {},
   "source": [
    "### Inspect Detector1 Output Products\n",
    "In the Detector1 stage, both the direct images (`EXP_TYPE=NIS_IMAGE`) and dispersed grism images (`EXP_TYPE=NIS_WFSS`) are calibrated. In addition to the `EXP_TYPE` keyword, the keyword `FILTER` can be used to distinguish exposure types for NIRISS WFSS data. `FILTER=CLEAR` indicates a direct image while `FILTER=GR150R` or `FILTER=GR150C` indicates a dispersed image. The keyword `PUPIL` is the blocking filter used in both direct images and dispersed images. We can also use the `PATT_NUM`, `XOFFSET`, and `YOFFSET` header keywords to see the dither pattern that was used for both the direct images and the dispersed images. The multiple direct image dithers will be combined in image3, while the multiple dithers in the dispersed images are combined as individual sources after extraction in spec3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about each rate file\n",
    "rate_files = sorted(glob.glob(os.path.join(det1_dir, \"*rate.fits\")))\n",
    "\n",
    "for file_num, ratefile in enumerate(rate_files):\n",
    "    rate_hdr = fits.getheader(ratefile) # Primary header for each rate file\n",
    "    \n",
    "    # information we want to store that might be useful to us later for evaluating the data\n",
    "    temp_hdr_dict = {\"PATHNAME\": os.path.abspath(ratefile), # full path to the filename to be used later\n",
    "                     \"FILENAME\": rate_hdr['FILENAME'], # base filename for printing readability\n",
    "                     \"EXP_TYPE\": [rate_hdr['EXP_TYPE']], # NIS_IMAGE or NIS_WFSS\n",
    "                     \"FILTER\": [rate_hdr[\"FILTER\"]], # Grism; GR150R/GR150C\n",
    "                     \"PUPIL\": [rate_hdr[\"PUPIL\"]], # Filter used; F090W, F115W, F140M, F150W F158M, F200W\n",
    "                     \"EXPSTART\": [rate_hdr['EXPSTART']], # Exposure start time (MJD)\n",
    "                     \"PATT_NUM\": [rate_hdr[\"PATT_NUM\"]], # Position number within dither pattern for WFSS\n",
    "                     \"NUMDTHPT\": [rate_hdr[\"NUMDTHPT\"]], # Total number of points in entire dither pattern\n",
    "                     \"XOFFSET\": [rate_hdr[\"XOFFSET\"]], # X offset from pattern starting position for NIRISS (arcsec)\n",
    "                     \"YOFFSET\": [rate_hdr[\"YOFFSET\"]], # Y offset from pattern starting position for NIRISS (arcsec)\n",
    "                     \"CAL_VER\": [rate_hdr[\"CAL_VER\"]], # JWST pipeline calibration version\n",
    "                     }\n",
    "\n",
    "    # Turn the dictionary into a pandas dataframe to make it easier to read & use later\n",
    "    if file_num == 0:\n",
    "        # if this is the first file, make an initial dataframe\n",
    "        rate_df = pd.DataFrame(temp_hdr_dict)\n",
    "    else:\n",
    "        # otherwise, append to the dataframe for each file\n",
    "        new_data_df = pd.DataFrame(temp_hdr_dict)\n",
    "        # merge the two dataframes together to create a dataframe with all \n",
    "        rate_df = pd.concat([rate_df, new_data_df], ignore_index=True, axis=0)\n",
    "\n",
    "rate_dfsort = rate_df.sort_values('EXPSTART', ignore_index=False) # sort by exposure start time\n",
    "\n",
    "# Look at the resulting dataframe\n",
    "rate_dfsort[['FILENAME', 'EXP_TYPE', 'FILTER', 'PUPIL', 'EXPSTART', 'PATT_NUM', 'NUMDTHPT', 'XOFFSET', 'YOFFSET', 'CAL_VER']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0243904-5001-4ed0-921a-5dca11e913d3",
   "metadata": {},
   "source": [
    "Shown below are the rate files to give an idea of the above sequence visually. Grid lines are shown as a visual guide for the dithers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc561b-4115-443b-8233-d4ee3da93cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot to visually illustrate the table above showing the\n",
    "#   direct image and grism sequence for the downloaded data\n",
    "if doviz:\n",
    "    # plot set up\n",
    "    fig = plt.figure(figsize=(20, 35))\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(len(rate_dfsort['PATHNAME']) / cols))\n",
    "    \n",
    "    # loop over the rate files and plot them\n",
    "    for plt_num, rf in enumerate(rate_dfsort['PATHNAME']):\n",
    "    \n",
    "        # determine where the subplot should be\n",
    "        xpos = (plt_num % 40) % cols\n",
    "        ypos = ((plt_num % 40) // cols) # // to make it an int.\n",
    "    \n",
    "        # make the subplot\n",
    "        ax = plt.subplot2grid((rows, cols), (ypos, xpos))\n",
    "    \n",
    "        # open the data and plot it\n",
    "        with fits.open(rf) as hdu:\n",
    "            data = hdu[1].data\n",
    "            data[np.isnan(data)] = 0 # filling in nan data with 0s to help with the matplotlib color scale.\n",
    "            \n",
    "            display_vals = [np.nanpercentile(data, 1), np.nanpercentile(data, 99.5)]\n",
    "            ax.imshow(data, vmin=display_vals[0], vmax=display_vals[1], origin='lower')\n",
    "    \n",
    "            # adding in grid lines as a visual aid\n",
    "            for gridline in [500, 1000, 1500]:\n",
    "                ax.axhline(gridline, color='black', alpha=0.5)\n",
    "                ax.axvline(gridline, color='black', alpha=0.5)\n",
    "            \n",
    "            ax.set_title(f\"#{plt_num+1}: {hdu[0].header['EXP_TYPE']} {hdu[0].header['FILTER']} {hdu[0].header['PUPIL']} Dither{hdu[0].header['PATT_NUM']}\")\n",
    "            \n",
    "    fig.suptitle(f'PID{program} o{sci_observtn} Observing Sequence rate Images (pixel space)', fontsize=16, x=0.5, y=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db066d-e92c-458b-badb-4bf7ee486ff8",
   "metadata": {},
   "source": [
    "Additionally, you can look into what steps were performed and reference files used during the Detector1 stage of the pipeline. These calls can be used at any stage of the pipeline to see or confirm what different steps or reference files were used. We show both the direct image and the dispersed (grism) images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3e85c-2174-4467-8b10-eb27d4581569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first look at the direct images\n",
    "dir_img_rate = rate_dfsort[rate_dfsort['EXP_TYPE'] == 'NIS_IMAGE']['PATHNAME'].iloc[0]\n",
    "check_steps_run(dir_img_rate)\n",
    "\n",
    "# then look at the dispersed, grism images\n",
    "grism_img_rate = rate_dfsort[rate_dfsort['EXP_TYPE'] == 'NIS_WFSS']['PATHNAME'].iloc[0]\n",
    "check_steps_run(grism_img_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb325be-1708-41f0-84c2-b371d2e141b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ref_file_used(dir_img_rate) # direct image\n",
    "check_ref_file_used(grism_img_rate) # dispersed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0cfdc-5477-407a-9612-b02fff8ec30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time_det1_end = time.perf_counter()\n",
    "print(f\"Runtime for Detector1: {(time_det1_end - time_det1_start)/60:0.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad1036-ad28-4afe-892a-d2b63b7a4fe8",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e00a63-7cba-4eb9-8a43-3cbbb20592cd",
   "metadata": {},
   "source": [
    "# 6. Image2 Pipeline\n",
    "------------------\n",
    "\n",
    "This section focuses on calibrating only the direct images in order to obtain a source catalog and segmentation mapping of the field to use as input into the Spec2 stage later. \n",
    "\n",
    "In the [Image2 stage of the pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_image2.html), calibrated unrectified data products are created (`*_cal.fits` files). In this pipeline processing stage, the [world coordinate system (WCS)](https://jwst-pipeline.readthedocs.io/en/latest/jwst/assign_wcs/index.html#assign-wcs-step) is assigned, the data are [flat fielded](https://jwst-pipeline.readthedocs.io/en/latest/jwst/flatfield/index.html#flatfield-step), and a [photometric calibration](https://jwst-pipeline.readthedocs.io/en/latest/jwst/photom/index.html#photom-step) is applied to convert from units of countrate (ADU/s) to surface brightness (MJy/sr).\n",
    "\n",
    "By default, the [background subtraction step](https://jwst-pipeline.readthedocs.io/en/latest/jwst/background_step/index.html#background-step) and the [resampling step](https://jwst-pipeline.readthedocs.io/en/latest/jwst/resample/index.html#resample-step) are not performed for NIRISS at this stage of the pipeline. The background subtraction is turned off since there is no background template for the imaging mode and the local background is removed during the background correction for photometric measurements around individual sources. The resampling step occurs during the Image3 stage by default. While the resampling step can be turned on during the Image2 stage to, e.g., generate a source catalog for each image, the data quality from the Image3 stage will be better since the bad pixels, which adversely affect\n",
    "both the centroids and photometry in individual images, will be mostly removed.\n",
    "\n",
    "For NIRISS imaging, it is equivalent to run the Image2 pipeline directly on the imaging rate files versus on the Image2 association files. Therefore, here we will simply use the dataframe table we set up in the Detector1 stage to filter on the imaging rate files and calibrate those directly rather than calibrating with the association files. To use the association files, simply replace the rate filename in the call with the association filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3f0ee-e476-4c3c-bbb9-707b3be43c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_image2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a197a2e-fa2f-420e-8b4e-8944fd31b9e5",
   "metadata": {},
   "source": [
    "The parameters in each of the [Image2 steps](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_image2.html) can be modified from the default values, including overwriting reference files that are used during this stage. This dictionary of the modified parameters for each of the steps is then fed into the `steps` parameter of the `Image2Pipeline` call. The syntax for modifying some of these parameters is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Image2 pipeline should be configured.\n",
    "\n",
    "# this sets up any entry to image2dict to be a dictionary itself\n",
    "image2dict = defaultdict(dict)\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "# Example overrides for whether or not certain steps should be skipped\n",
    "# image2dict['resample']['skip'] = False\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "# Example overrides for various reference files\n",
    "#   Files should be in the base local directory or provide full path\n",
    "# image2dict['assign_wcs']['override_distortion'] = 'myfile.asdf'  # Spatial distortion (ASDF file)\n",
    "# image2dict['assign_wcs']['override_filteroffset'] = 'myfile.asdf'  # Imager filter offsets (ASDF file)\n",
    "# image2dict['assign_wcs']['override_specwcs'] = 'myfile.asdf'  # Spectral distortion (ASDF file)\n",
    "# image2dict['assign_wcs']['override_wavelengthrange'] = 'myfile.asdf'  # Wavelength channel mapping (ASDF file)\n",
    "# image2dict['flat_field']['override_flat'] = 'myfile.fits'  # Pixel flatfield\n",
    "# image2dict['photom']['override_photom'] = 'myfile.fits'  # Photometric calibration array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e1d7e-9124-4f2f-a2dd-a3f62f60de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rate_files = rate_dfsort[rate_dfsort['EXP_TYPE'] == 'NIS_IMAGE']['PATHNAME']\n",
    "\n",
    "print(f'Found {str(len(img_rate_files))} imaging rate files to process for level 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d1edd-7f5b-4d77-a2cb-7de3ccb5c3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Image2 stage of pipeline, specifying:\n",
    "# output directory to save *_cal.fits files\n",
    "# save_results flag set to True so the rate files are saved\n",
    "\n",
    "if doimage2:\n",
    "    for rate in img_rate_files:\n",
    "        img2 = Image2Pipeline.call(rate, output_dir=image2_dir, steps=image2dict, save_results=True)\n",
    "else:\n",
    "    print(\"Skipping Image2 processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700dff41-2446-4f76-9304-716d6e420bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time_image2_end = time.perf_counter()\n",
    "print(f\"Runtime for Image2: {(time_image2_end - time_image2):0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f016ae0-deb1-4f8b-b7f7-f68c391b24c7",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c0aa6-017c-464b-a8b0-992c2a0849d2",
   "metadata": {},
   "source": [
    "# 7. Image3 Pipeline\n",
    "------------------\n",
    "\n",
    "In this section we continue calibrating the direct images with the Image3 stage of the pipeline to obtain a source catalog and segmentation mapping of the field to use as input into the Spec2 stage later. In the [Image3 stage of the pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_image3.html), the individual `*_cal.fits` files for each of the dither positions are combined to one single distortion corrected image (`*_i2d.fits` files).\n",
    "\n",
    "By default, the Image3 stage of the pipeline performs the following steps on NIRISS data:\n",
    "* [tweakreg](https://jwst-pipeline.readthedocs.io/en/latest/jwst/tweakreg/README.html) - creates source catalogs of pointlike sources for each input image. The source catalog for each input image is compared to each other to derive coordinate transforms to align the images relative to each other.\n",
    "* As of CRDS context jwst_1156.pmap and later, the pars-tweakreg parameter reference file for NIRISS performs an absolute astrometric correction to GAIA data release 3 by default (i.e., the abs_refcat parameter is set to GAIADR3). Though this default correction generally improves results compared with not doing this alignment, it could potentially result in poor performance in crowded or sparse fields, so users are encouraged to check astrometric accuracy and revisit this step if necessary.\n",
    "* As of pipeline version 1.14.0, the default source finding algorithm for NIRISS is IRAFStarFinder which testing shows returns good accuracy for undersampled NIRISS PSFs at short wavelengths ([Goudfrooij 2022](https://www.stsci.edu/files/live/sites/www/files/home/jwst/documentation/technical-documents/_documents/JWST-STScI-008324.pdf)).\n",
    "* [skymatch](https://jwst-pipeline.readthedocs.io/en/latest/jwst/skymatch/description.html) - measures the background level from the sky to use as input into the subsequent outlier detection and resample steps.\n",
    "* outlier detection - flags any remaining cosmic rays, bad pixels, or other artifacts not already flagged during the detector1 stage of the pipeline, using all input images to create a median image so that outliers in individual images can be identified.\n",
    "* [resample](https://jwst-pipeline.readthedocs.io/en/latest/jwst/resample/main.html) - resamples each input image based on its WCS and distortion information and creates a single undistorted image.\n",
    "* [source catalog](https://jwst-pipeline.readthedocs.io/en/latest/jwst/source_catalog/main.html) - creates a catalog of detected sources along with measured photometries and morphologies (i.e., point-like vs extended). Useful for quicklooks, but optimization is likely needed for specific science cases, which is an on-going investigation for the NIRISS team. Users may wish to experiment with changing the snr_threshold and deblend options. Modifications to the following parameters will not significantly improve data quality and it is advised to keep them at their default values: aperture_ee1, aperture_ee2, aperture_ee3, ci1_star_threshold, ci2_star_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc700a1-2794-4ca0-816c-f7dfb9271919",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_image3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99361271",
   "metadata": {},
   "source": [
    "Find and sort all of the input image2 cal files, ensuring use of absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb386be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science Files need the cal.fits files\n",
    "sstring = os.path.join(image2_dir, 'jw*cal.fits')\n",
    "img3_cal_files = sorted(glob.glob(sstring))\n",
    "for ii, cal_relpath in enumerate(img3_cal_files):\n",
    "    img3_cal_files[ii] = os.path.abspath(cal_relpath)\n",
    "img3_cal_files = np.array(img3_cal_files)\n",
    "\n",
    "print(f'Found {str(len(img3_cal_files))} imaging cal files to process for level 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c1fb4",
   "metadata": {},
   "source": [
    "### Create Image3 Association Files\n",
    "\n",
    "An association file lists the exposures to calibrated together in the Image3 stage of the pipeline. Note that an association file is available for download from MAST, with a filename of `*image3_asn.json`. Additionally, you can download the `_pool.csv` file for a specific observation and create associations directly from the pool file using the [asn_generate](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/asn_generate.html) function with the latest version of the pipeline. In both of these cases, the pipeline is expecting the files being calibrated to exist in the same directory that the association is in. Below, we show how to create an image3 association file by providing a [list of exposures](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/asn_from_list.html) that we have processed through the pipeline and saved in separate directories. Also note that the output products will have a rootname that is specified by the `product_name` in the association file. For this tutorial, the rootname of the output products will end with `image3_asn.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05e3f9-d9e5-4e37-8514-e4ec1b13c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Level 3 Associations for each pupil (blocking filter) type\n",
    "if doimage3:\n",
    "\n",
    "    # Parameters to be used for the NIRISS imaging 3 association creation\n",
    "    img3_pid = str(program) # associations are only set up to combine for the same program & observation\n",
    "    img3_obs = str(sci_observtn) # associations are only set up to combine for the same program & observation\n",
    "    img3_filt = 'CLEAR' # For imaging mode, the second filter wheel is set to clear\n",
    "    img3_ins = 'NIRISS'\n",
    "\n",
    "    # Identify the unique filters used (keyword=PUPIL) for the NIRISS images\n",
    "    img3_all_pupils = np.array([fits.getval(cf, 'PUPIL') for cf in img3_cal_files])\n",
    "    img3_uniq_pupils = np.unique(img3_all_pupils)\n",
    "    \n",
    "    # Loop over unique pupil values\n",
    "    for img3_pupil in img3_uniq_pupils:\n",
    "        img3_indx = np.where(img3_all_pupils == img3_pupil)[0]\n",
    "        img3_pupil_files = img3_cal_files[img3_indx]\n",
    "\n",
    "        # setting up the association filename to match the default pipeline level3 naming output\n",
    "        img3_product_name = f\"jw{img3_pid}-o{img3_obs}_{img3_ins}_{img3_filt}-{img3_pupil}\".lower()\n",
    "        img3_asn_filename = img3_product_name + '_image3_asn.json'\n",
    "    \n",
    "        img3_association = asn_from_list.asn_from_list(img3_pupil_files, rule=DMS_Level3_Base,\n",
    "                                                       product_name=img3_product_name)\n",
    "    \n",
    "        img3_association.data['asn_type'] = 'image3'\n",
    "        img3_association.data['program'] = img3_pid\n",
    "    \n",
    "        # Format association as .json file\n",
    "        _, serialized = img3_association.dump(format=\"json\")\n",
    "\n",
    "        # Write out association file\n",
    "        img3_asn_pathname = os.path.join(sci_dir, img3_asn_filename)\n",
    "        with open(img3_asn_pathname, \"w\") as fd:\n",
    "            fd.write(serialized)\n",
    "        print(f'Writing image3 association: {img3_asn_pathname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51d864-70cf-4078-a6af-5ca94941f1dd",
   "metadata": {},
   "source": [
    "Take a quick look at the contents of the first image3 association file to get a feel for what is being associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f223d96-9bb6-433a-9605-07ec09e45388",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doimage3:\n",
    "    image3_asns = glob.glob(os.path.join(sci_dir, \"*image3*_asn.json\"))\n",
    "    \n",
    "    # open the image3 association to look at\n",
    "    image3_asn_data = json.load(open(image3_asns[0]))\n",
    "    print(f'asn_type : {image3_asn_data[\"asn_type\"]}')\n",
    "    print(f'code_version : {image3_asn_data[\"code_version\"]}')\n",
    "    \n",
    "    # in particular, take a closer look at the product filenames with the association file:\n",
    "    for product in image3_asn_data['products']:\n",
    "        for key, value in product.items():\n",
    "            if key == 'members':\n",
    "                print(f\"{key}:\")\n",
    "                for member in value:\n",
    "                    print(f\"    {member['expname']} {member['exptype']}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfe205-a451-4349-8128-2160091b8cf8",
   "metadata": {},
   "source": [
    "### Run Image3\n",
    "\n",
    "In Image3, the `*_cal.fits` individual pointing files will be calibrated into a single combined `*_i2d.fits` image. The parameters in each of the [Image3 steps](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_image3.html) can be modified from the default values, including overwriting reference files that are used during this stage. This dictionary of the modified parameters for each of the steps is then fed into the `steps` parameter of the `Image3Pipeline` call. The syntax for modifying some of these parameters is below; the full list of parameters can be found in the [tweakreg](https://jwst-pipeline.readthedocs.io/en/latest/jwst/tweakreg/README.html) and [sourcecatalog](https://jwst-pipeline.readthedocs.io/en/latest/jwst/source_catalog/main.html) documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a12f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Image3 pipeline should be configured\n",
    "\n",
    "# this sets up any entry to image3dict to be a dictionary itself\n",
    "image3dict = defaultdict(dict)\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "# Example overrides for whether or not certain steps should be skipped\n",
    "#   Some of these example values differ from default values to improve the demo scene\n",
    "# image3dict['outlier_detection']['skip'] = True\n",
    "\n",
    "# Example parameters for the source_catalog step\n",
    "# image3dict['source_catalog']['kernel_fwhm'] = 5.0\n",
    "# image3dict['source_catalog']['snr_threshold'] = 10.0\n",
    "# image3dict['source_catalog']['npixels'] = 50\n",
    "# image3dict['source_catalog']['deblend'] = True\n",
    "\n",
    "# Example parameters for the tweakreg step\n",
    "# image3dict['tweakreg']['snr_threshold'] = 20\n",
    "# image3dict['tweakreg']['abs_refcat'] = 'GAIADR3'\n",
    "# image3dict['tweakreg']['searchrad'] = 3.0,\n",
    "# image3dict['tweakreg']['kernel_fwhm'] = 2.302\n",
    "# image3dict['tweakreg']['fitgeometry'] = 'shift'\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "# Example overrides for various reference files\n",
    "#   Files should be in the base local directory or provide full path\n",
    "# image3dict['source_catalog']['override_apcorr'] = 'myfile.fits'  # Aperture correction parameters\n",
    "# image3dict['source_catalog']['override_abvegaoffset'] = 'myfile.asdf'  # Data to convert from AB to Vega magnitudes (ASDF file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Image3\n",
    "if doimage3:\n",
    "    asn_files = np.sort(glob.glob(os.path.join(sci_dir, '*image3_asn.json')))\n",
    "    for asn in asn_files:\n",
    "        img3 = Image3Pipeline.call(asn, output_dir=image3_dir, steps=image3dict, save_results=True)\n",
    "else:\n",
    "    print('Skipping Image3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time_image3_end = time.perf_counter()\n",
    "print(f\"Runtime for Image3: {(time_image3_end - time_image3)/60:0.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc11722",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea0c45-6634-4749-a406-a727bba33284",
   "metadata": {},
   "source": [
    "# 8. Visualize Image3 Output Products\n",
    "------------------\n",
    "\n",
    "Using the combined image (`*_i2d.fits` ), the segmentation map files (`*_segm.fits`), and the source catalog  (`*cat.ecsv`) produced by the Image3 stage of the pipeline, we can visually inspect if we agree with where the sources were found to use further in the Spec2 stage of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92903acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the outputs of the Image3 pipeline, which will be needed for processing the spectral data\n",
    "# Print which outputs were found for reference\n",
    "\n",
    "# Combined image over multiple dithers/mosaic\n",
    "image3_i2d = np.sort(glob.glob(os.path.join(image3_dir, '*i2d.fits')))\n",
    "print('Direct images:')\n",
    "for i2d_filename in image3_i2d:\n",
    "    print(f\"  {os.path.basename(i2d_filename)}\")\n",
    "\n",
    "# Segmentation map that defines the extent of a source\n",
    "image3_segm = np.sort(glob.glob(os.path.join(image3_dir, '*segm.fits')))\n",
    "print('Segmentation maps:')\n",
    "for seg_filename in image3_segm:\n",
    "    print(f\"  {os.path.basename(seg_filename)}\")\n",
    "    \n",
    "# Source catalog that defines the RA/Dec of a source at a particular pixel\n",
    "image3_cat = np.sort(glob.glob(os.path.join(image3_dir, '*cat.ecsv')))\n",
    "print('Source catalogs:')\n",
    "for cat_filename in image3_cat:\n",
    "    print(f\"  {os.path.basename(cat_filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ef348-28d4-4dfd-8d3b-897f55dbd9e7",
   "metadata": {},
   "source": [
    "### i2d & segementation mapping\n",
    "\n",
    "The segmentation maps are used the help determine the source catalog. Let's take a look at those to ensure we agree with what is being defined as a source. In the following figures, the combined image is shown on the left and the the segmentation map is shown on the right, where each black blob in the segmentation map should correspond to a physical target. The sources identified in the source catalog are overlayed on top of both of these, where what has been defined as an extended source by the pipeline is shown as a blue circle, and what has been defined as a point source by the pipeline is shown as a pink square. This definition affects the extraction box in the WFSS images as well as in the contamination correction step of the pipeline, so it is important to get correct.\n",
    "\n",
    "There are cases where sources can be blended, in which case the parameters for making the segmentation map and source catalog should be modified. If using the demo data, an example of this can be seen in the Observation 004 F200W filter image where two galaxies at ~(1600, 1300) have been blended into one source. This is discussed in more detail in the custom Image3 run in the [NIRISS WFSS JDAT notebooks](https://github.com/spacetelescope/jdat_notebooks/tree/main/notebooks/NIRISS/NIRISS_WFSS_advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e9b0a-3844-44a5-9a94-a759f0826994",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:            \n",
    "    cols = 2\n",
    "    rows = len(image3_i2d)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 15*(rows/2)))\n",
    "    \n",
    "    for plt_num, img in enumerate(np.sort(np.concatenate([image3_segm, image3_i2d]))):\n",
    "    \n",
    "        # determine where the subplot should be\n",
    "        xpos = (plt_num % 40) % cols\n",
    "        ypos = ((plt_num % 40) // cols) # // to make it an int.\n",
    "    \n",
    "        # make the subplot\n",
    "        ax = plt.subplot2grid((rows, cols), (ypos, xpos))\n",
    "    \n",
    "        if 'i2d' in img:\n",
    "            cat = Table.read(img.replace('i2d.fits', 'cat.ecsv'))\n",
    "            cmap = 'gist_gray_r'\n",
    "        else:\n",
    "            cmap = 'gist_gray_r'\n",
    "            \n",
    "        # plot the image\n",
    "        with fits.open(img) as hdu:\n",
    "            display_vals = [np.nanpercentile(hdu[1].data, 1), np.nanpercentile(hdu[1].data, 99)]\n",
    "            ax.imshow(hdu[1].data, vmin=display_vals[0], vmax=display_vals[1], origin='lower', cmap=cmap)\n",
    "            title = f\"{hdu[0].header['PUPIL']}\"\n",
    "    \n",
    "        # also plot the associated catalog\n",
    "        extended_sources = cat[cat['is_extended'] == 1] # 1 is True; i.e. is extended\n",
    "        point_sources = cat[cat['is_extended'] == 0] # 0 is False; i.e. is a point source\n",
    "\n",
    "        for color, sources, source_type, marker in zip(['deepskyblue', 'deeppink'], [extended_sources, point_sources], ['Extended Source', 'Point Source'], ['o', 's']):\n",
    "            # plotting the sources\n",
    "            ax.scatter(sources['xcentroid'], sources['ycentroid'], marker=marker, s=150, facecolors='None', edgecolors=color, alpha=0.9)\n",
    "    \n",
    "            # adding source labels \n",
    "            for i, source_num in enumerate(sources['label']):\n",
    "                ax.annotate(source_num, \n",
    "                            (sources['xcentroid'][i]+1, sources['ycentroid'][i]+1), \n",
    "                            fontsize=10,\n",
    "                            color=color)\n",
    "            ax.scatter(-999, -999, marker=marker, label=source_type, s=150, facecolors='None', edgecolors=color, alpha=0.9)\n",
    "            \n",
    "        # setting titles\n",
    "        if 'i2d' in img:\n",
    "            ax.set_title(f\"{title} combined image\\n(i2d)\", fontsize=16)\n",
    "        else:\n",
    "            ax.set_title(f\"{title} segmentation map\\n(segm)\", fontsize=16)\n",
    "        \n",
    "        # zooming in on a smaller region\n",
    "        ax.set_xlim(1250, 1750)\n",
    "        ax.set_ylim(1250, 1750)\n",
    "\n",
    "        ax.legend(framealpha=0.6, fontsize=14, loc='upper left')\n",
    "\n",
    "    # more labels \n",
    "    fig.supxlabel('x-pixel', fontsize=14)\n",
    "    fig.supylabel('y-pixel', fontsize=14, x=0)\n",
    "    \n",
    "    # Helps to make the axes not overlap ; you can also set this manually if this doesn't work\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4c778-a6d7-4007-a6ba-3b6c56952c74",
   "metadata": {},
   "source": [
    "In addition to the segmentation mapping, the source catalog itself can be useful to look at to examine the source centroids, calculated fluxes, and source extents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38946dad-6c63-4a89-a367-96002f29f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a source catalogs to illustrate the contents\n",
    "cat = Table.read(image3_cat[0])\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad8b21-a896-4277-bc4d-d448a3de4066",
   "metadata": {},
   "source": [
    "In all likelihood, you will need to rerun Image3 with different parameters in order to return an optimal source catalog to use with your NIRISS WFSS data. You can additionally refine the source catalog so that Spec2 and Spec3 only run on the sources that you care most about. Some examples of this can be found in the [NIRISS WFSS JDAT notebooks](https://github.com/spacetelescope/jdat_notebooks/tree/main/notebooks/NIRISS/NIRISS_WFSS_advanced)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbd2d7-a11c-431f-9440-8751a97a4ff8",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada80c4-b305-405a-8f34-45f7d0188f0d",
   "metadata": {},
   "source": [
    "# 9. Spec2 Pipeline\n",
    "------------------\n",
    "After running Image3 and thus getting the the segmentation map and source catalog, the [Spec2 pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html#calwebb-spec2) is ready to be run. The spec2 pipeline first runs [assign_wcs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/assign_wcs/main.html), [background](https://jwst-pipeline.readthedocs.io/en/latest/jwst/background_subtraction/description.html), and [flat_field](https://jwst-pipeline.readthedocs.io/en/latest/jwst/flatfield/main.html) corrections first on the full-frame `*_rate.fits` files. The [srctype](https://jwst-pipeline.readthedocs.io/en/latest/jwst/srctype/description.html) step is run to determine the extent of the extraction box size before the [extract_2d](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_2d/main.html) step is run, producing individual cutouts for the brightest 100 sources defined in the Image3 source catalog. The [wfss_contam](https://jwst-pipeline.readthedocs.io/en/latest/jwst/wfss_contam/description.html) step is run towards the end of the [extract_2d](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_2d/main.html) step and is currently not on by default as the step is being improved. The [photom](https://jwst-pipeline.readthedocs.io/en/latest/jwst/photom/main.html) step is then run on the cutouts, producing flux calibrated 2-D spectral (`*_cal.fits`) files. The [extract_1d](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html) step is run last, producing level 2 `*_x1d.fits` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593909d8-0cdc-4b13-8e9c-48715281edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1cfedf",
   "metadata": {},
   "source": [
    "### Create Spec2 Association File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad19ad",
   "metadata": {},
   "source": [
    "As with the imaging part of the pipeline, there are association files for spec2. These are a bit more complex in that they need to have the science (WFSS) data, direct image, source catalog, and segmentation map included as members. For the science data, the rate files are used as inputs, similar to Image2. Also like Image2, there should be one association file for each dispersed image dither position in an observing sequence.\n",
    "\n",
    "Like Image3, we are creating a spec2 association file manually by providing a [list of exposures](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/asn_from_list.html) that we have processed through the pipeline and saved in separate directories rather than downloading directly from MAST or using the [pool files](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/asn_generate.html). Note that the output products will have a rootname that is specified by the `product_name` in the association file. For this tutorial, the rootname of the output products will end with `_spec2_asn.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spec2asn(grismfile, dimagefiles, catalogfiles, segmfiles, prodname):\n",
    "    \n",
    "    # Define the basic association of science files\n",
    "    asn = asn_from_list.asn_from_list([grismfile], rule=DMSLevel2bBase, product_name=prodname)  # Wrap in list since input is single exposure\n",
    "\n",
    "    # Which pupil element (blocking filter) does the dispersed image use?\n",
    "    grism_pupil = fits.getval(grismfile, 'PUPIL')\n",
    "    grism_pid = fits.getval(grismfile, 'PROGRAM')\n",
    "    grism_obs = fits.getval(grismfile, 'OBSERVTN')\n",
    "\n",
    "    # Find the direct images with the same matching program, observation, and pupil to use\n",
    "    dir_img_match = []\n",
    "    for dir_img in dimagefiles:\n",
    "        img_pid = fits.getval(dir_img, 'PROGRAM')\n",
    "        img_obs = fits.getval(dir_img, 'OBSERVTN')\n",
    "        img_pupil = fits.getval(dir_img, 'PUPIL')\n",
    "\n",
    "        if img_pupil == grism_pupil and img_pid == grism_pid and img_obs == grism_obs:\n",
    "            dir_img_match.append(dir_img)\n",
    "\n",
    "    # ensure that there is only one match found for the grism image\n",
    "    if len(dir_img_match) == 0:\n",
    "        raise ValueError(f'Could not find a matching i2d image for {scifile}. Please ensure that you have processed the appropriate data for {grism_pupil} PID {grism_pid} o{grism_obs}')\n",
    "    elif len(dir_img_match) > 1:\n",
    "        raise ValueError(f'Multiple i2ds found matching: {grism_pupil} PID {grism_pid} o{grism_obs}. Please download the associations directly from MAST to proceed further.')\n",
    "    else:\n",
    "        # there should only be one match per filter/program/observation combination, so grab that one\n",
    "        dir_img_match = dir_img_match[0]\n",
    "        \n",
    "    # There should be a set of i2d, segm, and cat that have the same rootname, so we will just replace the filetype suffix\n",
    "    dir_seg_match = dir_img_match.replace('_i2d.fits', '_segm.fits')\n",
    "    dir_cat_match = dir_img_match.replace('_i2d.fits', '_cat.ecsv')\n",
    "        \n",
    "    # Add the direct image, catalog, and segmentation files\n",
    "    asn['products'][0]['members'].append({'expname': dir_img_match, 'exptype': 'direct_image'})\n",
    "    asn['products'][0]['members'].append({'expname': dir_cat_match, 'exptype': 'sourcecat'})\n",
    "    asn['products'][0]['members'].append({'expname': dir_seg_match, 'exptype': 'segmap'})\n",
    "    \n",
    "    spec2_asnfile = os.path.join(sci_dir, os.path.basename(grismfile).replace('rate.fits', 'spec2_asn.json'))\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(spec2_asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)\n",
    "\n",
    "    print(f'Writing spec2 association: {spec2_asnfile}')\n",
    "    return spec2_asnfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88158ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rate files using our dataframe table that we created in the detector1 stage of the notebook\n",
    "grism_rate_files = rate_dfsort[rate_dfsort['EXP_TYPE'] == 'NIS_WFSS']['PATHNAME']\n",
    "\n",
    "print(f'Found {str(len(grism_rate_files))} grism rate files to process for level 2')\n",
    "\n",
    "# use the rate files and image3 output products to define spec2 association files\n",
    "if dospec2:\n",
    "    for file in grism_rate_files:\n",
    "        asnfile = write_spec2asn(file, image3_i2d, image3_cat, image3_segm, 'Level2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff01133-fc9a-4ea2-97e2-b41881b65a53",
   "metadata": {},
   "source": [
    "Take a quick look at the contents of an example spec2 association file to get a feel for what is being associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac2e3d-0542-4e5b-b583-1152e453de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec2:\n",
    "    spec2_asns = glob.glob(os.path.join(sci_dir, \"*spec2*_asn.json\"))\n",
    "    \n",
    "    # look at one of the association files\n",
    "    asn_data = json.load(open(spec2_asns[0]))\n",
    "    print(f'asn_type : {asn_data[\"asn_type\"]}')\n",
    "    print(f'code_version : {asn_data[\"code_version\"]}')\n",
    "    \n",
    "    # in particular, take a closer look at the product filenames with the association file:\n",
    "    for product in asn_data['products']:\n",
    "        for key, value in product.items():\n",
    "            if key == 'members':\n",
    "                print(f\"{key}:\")\n",
    "                for member in value:\n",
    "                    print(f\"    {member['expname']} : {member['exptype']}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99deff1e-23cc-41db-8847-36cb5ef8ad03",
   "metadata": {},
   "source": [
    "### Run Spec2\n",
    "\n",
    "In Spec2, the `*_rate.fits` files run through various corrections before using the source catalog to extract the 100 brightest sources into 1-D spectra (level 2 `*_x1d.fits` files). The parameters in each of the [Spec2 steps](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html) can be modified from the default values, including overwriting reference files that are used during this stage and saving additional files. This dictionary of the modified parameters for each of the steps is then fed into the `steps` parameter of the `Spec2Pipeline` call. The syntax for modifying some of these parameters is below. In particular, we show the option of how to turn on the contamination step as an option, although there are several known bugs still with this stage as of pipeline version 1.19.1, so we caution use of this step currently. We also show how to save the background subtracted full-frame file as an intermediate product (*_bsub.fits). These background products are expected to be a default output in an upcoming pipeline build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec2 pipeline should be configured.\n",
    "\n",
    "# this sets up any entry to spec2dict to be a dictionary itself\n",
    "spec2dict = defaultdict(dict)\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "# Overrides for various reference files (example).\n",
    "#   Files should be in the base local directory or provide full path.\n",
    "# spec2dict['assign_wcs']['override_distortion'] = 'myfile.asdf'  # Spatial distortion (ASDF file)\n",
    "# spec2dict['assign_wcs']['override_filteroffset'] = 'myfile.asdf'  # Imager filter offsets (ASDF file)\n",
    "# spec2dict['assign_wcs']['override_specwcs'] = 'myfile.asdf'  # Spectral distortion (ASDF file)\n",
    "# spec2dict['assign_wcs']['override_wavelengthrange'] = 'myfile.asdf'  # Wavelength channel mapping (ASDF file)\n",
    "# spec2dict['bkg_subtract']['override_bkg'] = 'myfile.fits' # WFSS Background subtraction\n",
    "# spec2dict['extract_2d']['override_wavelengthrange'] = 'myfile.asdf'  # Wavelength channel mapping (ASDF file)\n",
    "# spec2dict['flat_field']['override_flat'] = 'myfile.fits'  # Pixel flatfield\n",
    "# spec2dict['wfss_contam']['override_wavelengthrange'] = 'myfile.asdf'  # Wavelength channel mapping (ASDF file)\n",
    "# spec2dict['wfss_contam']['override_photom'] = 'myfile.fits'  # Photometric calibration array\n",
    "# spec2dict['photom']['override_photom'] = 'myfile.fits'  # Photometric calibration array\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "# Overrides for whether or not certain steps should be skipped (example).\n",
    "# spec2dict['bkg_subtract']['skip'] = True # don't perform the background subtraction\n",
    "# spec2dict['bkg_subtract']['save_results'] = True # save background subtracted full-frame images\n",
    "# spec2dict['flat_field']['save_results'] = True # save the background subtracted, flat-field corrected, full-frame images\n",
    "# spec2dict['extract_2d']['wfss_nbright'] = 10 # only extract the 10 brightest sources\n",
    "# spec2dict['wfss_contam']['skip'] = False # uncomment to turn on contamination correction\n",
    "# spec2dict['wfss_contam']['save_simulated_image'] = True # save the simulated images produced by the pipeline\n",
    "# spec2dict['wfss_contam']['maximum_cores'] = 'half' # (quarter, half, all, or the default=1); This will speed up the calibration time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec2:\n",
    "    for spec2_asn in spec2_asns:\n",
    "        os.chdir(image3_dir) # This is necessary since the pipeline looks in the current directory for the catalog\n",
    "        spec2 = Spec2Pipeline.call(spec2_asn, steps=spec2dict, save_results=True, output_dir=spec2_dir)\n",
    "        os.chdir(cwd) # change back into your original directory\n",
    "else:\n",
    "    print('Skipping Spec2 processing for SCI data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a9be3-5cfb-4e2d-8e27-3e9d86b84387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time_spec2_end = time.perf_counter()\n",
    "print(f\"Runtime for Spec2: {(time_spec2_end - time_spec2)/60:0.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339d8ac-f013-4502-8391-ef522afaed07",
   "metadata": {},
   "source": [
    "### Visualize Spec2 Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b6177-fe6f-43b3-b899-7f341994d97a",
   "metadata": {},
   "source": [
    "In NIRISS WFSS data there are many sources of interest to look at. In this visualization we look at, for five selected sources, the source as it appears in the i2d image, two example grism `*_cal.fits` 2-D spectral cutouts (if available, otherwise they may appear blank), and the level 2 `*_x1d.fits` 1-D extracted spectra for all grism dithers where available. With the contamination step currently turned off, the contamination can be easily visible when comparing the 1-D and 2-D spectra of the two grisms. If using the demo mode data, this is especially visible for source 505 where you can see an order 0 contaminant in the GR150C example `*_cal.fits` image at ~(75, 5), which appears as a large emission line for the GR150C 1-D spectrum.\n",
    "\n",
    "Note that the `*_cal.fits` data for GR150R are transposed so that the dispersion direction is along the -x axis. For both GR150R and GR150C `*_cal.fits` files, the axis is then flipped for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e4600-e035-4329-bb25-a403ee4378bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we look at the source as identified by the source catalog in the i2d image, the two grism cal files, and the x1d files\n",
    "if doviz:\n",
    "    # grab the spec2 x1d output products\n",
    "    spec2_x1d_files = sorted(glob.glob(os.path.join(spec2_dir, '*nis_x1d.fits*')))\n",
    "\n",
    "    # If there are multiple pupils (blocking filters) pick one for illustration\n",
    "    spec2_unique_pupils = np.unique([fits.getval(x1d, 'PUPIL') for x1d in spec2_x1d_files])\n",
    "    pupil_x1ds = [x1d for x1d in spec2_x1d_files if fits.getval(x1d, 'PUPIL') == spec2_unique_pupils[0]]\n",
    "\n",
    "    if demo_mode:\n",
    "        # define some cool sources to look at if using the demo mode data\n",
    "        sources = [417, 422, 505, 1296, 606]\n",
    "        nsources = len(sources)\n",
    "    else:\n",
    "        # or grab some sources from the first x1d file\n",
    "        nsources = 5 # 100 sources are extracted by default\n",
    "        source_offset = 10 # offsetting what nsources to plot to avoid extra bright sources\n",
    "        with fits.open(pupil_x1ds[0]) as temp_x1d:\n",
    "            sources = temp_x1d[1].data['SOURCE_ID'][source_offset:nsources+source_offset]\n",
    "\n",
    "    # setting up the figure\n",
    "    cols = 4\n",
    "    rows = nsources\n",
    "    fig = plt.figure(figsize=(15, 4*(rows/2)))\n",
    "    fig.suptitle(f\"Spec2 Products for PID{program} o{sci_observtn} {spec2_unique_pupils[0]}\")\n",
    "    \n",
    "    # looping through the different sources to plot; one per row\n",
    "    for nsource, source_id in enumerate(sources):\n",
    "        # we are only plotting a single cal file cutout for each grism\n",
    "        plot_gr150r = True\n",
    "        plot_gr150c = True\n",
    "\n",
    "        # setting up the subplots for a single source\n",
    "        ypos = nsource\n",
    "        ax_i2d = plt.subplot2grid((rows, cols), (ypos, 0)) \n",
    "        ax_cal_r = plt.subplot2grid((rows, cols), (ypos, 1)) \n",
    "        ax_cal_c = plt.subplot2grid((rows, cols), (ypos, 2)) \n",
    "        ax_x1d = plt.subplot2grid((rows, cols), (ypos, 3))\n",
    "    \n",
    "        source_fluxes = [] # save the source flux to set the plot limits\n",
    "                \n",
    "        # plot all of the 1-D spectra from the x1d files\n",
    "        for nfile, x1dfile in enumerate(pupil_x1ds):\n",
    "\n",
    "            ax_x1d, catname, source_fluxes, grism = plot_spectrum(x1dfile, source_fluxes, ax_x1d, image3_dir, legend=False)\n",
    "            \n",
    "            # plot the direct image of the source based on the source number from the source catalog\n",
    "            if nfile == 0:\n",
    "                \n",
    "                ax_i2d = plot_i2d_plus_source(catname, source_id, ax_i2d)\n",
    "            \n",
    "            # plot one example cal image from the GR150R grism, transposed to disperse in the same direction as GR150C\n",
    "            if plot_gr150r and grism == 'GR150R':\n",
    "                ax_cal_r = plot_spec2_cal(x1dfile, source_id, ax_cal_r, transpose=True)\n",
    "                plot_gr150r = False\n",
    "                \n",
    "            # plot one example cal image from the GR150C grism\n",
    "            if plot_gr150c and grism == 'GR150C':\n",
    "                ax_cal_c = plot_spec2_cal(x1dfile, source_id, ax_cal_c)\n",
    "                plot_gr150c = False\n",
    "\n",
    "        if len(source_fluxes):\n",
    "            # there may not have been data to extract if everything was saturated\n",
    "            ax_x1d.set_ylim(np.nanmin(source_fluxes), np.nanmax(source_fluxes))\n",
    "            ax_x1d.legend(bbox_to_anchor=(1, 1), ncols=np.ceil(len(pupil_x1ds)/6))\n",
    "        \n",
    "        # Add labels to the subplots\n",
    "        if nsource == 0:\n",
    "            ax_cal_r.set_title('Example Transposed GR150R cutout\\n(cal)')\n",
    "            ax_cal_c.set_title('Example GR150C cutout\\n(cal)')\n",
    "            ax_i2d.set_title('Direct Image\\n(i2d)')\n",
    "            ax_x1d.set_title('All Collapsed 1-D Spectrum\\n(level 2 x1d)')\n",
    "            \n",
    "        ax_i2d.set_ylabel(f'Source\\n{source_id}', fontsize=15)\n",
    "        ax_cal_r.set_xlabel('dispersion --->')\n",
    "        ax_cal_c.set_xlabel('dispersion --->')\n",
    "        ax_x1d.set_xlabel('Wavelength (microns)')\n",
    "        ax_x1d.set_ylabel('F_nu')\n",
    "        ax_x1d.ticklabel_format(axis='y', style='sci', scilimits=(0, 0)) # forcing scientific notation for the spectra\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb37a1-1da0-4482-a879-2b96a056767f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ad37f-16d4-43f0-85b7-e02fe62b8ee2",
   "metadata": {},
   "source": [
    "# 10. Spec3 Pipeline\n",
    "------------------\n",
    "\n",
    "NIRISS WFSS data are minimally processed through the [Spec3 stage of the pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html) to combine calibrated data from multiple dithers within an observation. The spec3 products are unique for a specific grism and blocking filter combination; the different grism data are not combined by default. As of pipeline version 1.19.1, the level 3 source-based `*_cal.fits` files created in this step in the [exp_to_source](https://jwst-pipeline.readthedocs.io/en/latest/jwst/exp_to_source/main.html) step are no longer saved by default, and the `*_x1d.fits` files created in the [extract_1d](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html) and the `*_c1d.fits` files created in the [combine_1d](https://jwst-pipeline.readthedocs.io/en/latest/jwst/combine_1d/description.html) step are now saved as a single file per grism and filter combination with all of the extracted sources contained within that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf3d7b-bd7c-4e21-9e25-c5c60e9149d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af440cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the cal.fits files\n",
    "sstring = os.path.join(spec2_dir, 'jw*cal.fits')\n",
    "spec2_cal_files = sorted(glob.glob(sstring))\n",
    "for ii, cal_relpath in enumerate(spec2_cal_files):\n",
    "    spec2_cal_files[ii] = os.path.abspath(cal_relpath)\n",
    "spec2_cal_files = np.array(spec2_cal_files)\n",
    "\n",
    "print(f'Found {str(len(spec2_cal_files))} grism spectroscopy cal files to process for level 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06608690",
   "metadata": {},
   "source": [
    "### Create Spec3 Association Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b88335-0219-426c-9354-d9a35c3a05f3",
   "metadata": {},
   "source": [
    "There will be one spec3 association per blocking filter and grism combination, in which all of the extracted 1-D spectra within an observation with that filter and grism combination are coadded into a single spectrum for each source. If using only one blocking filter (e.g., F200W) with both grisms (GR150R & GR150C) for example, we would expect two spec3 association files, each of which contains all of the corresponding cal.fits files to combine.\n",
    "\n",
    "Like with Image3 and Spec2 before, we will be creating Image3 associations by providing a [list of exposures](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/asn_from_list.html) that we have processed through the pipeline and saved in separate directories rather than downloading directly from MAST or using the [pool files](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/asn_generate.html). Note that the output products will have a rootname that is specified by the `product_name` in the association file. For this tutorial, the rootname of the output products will end with `_spec3_asn.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Level 3 Associations for each pupil (blocking filter) type\n",
    "if dospec3:\n",
    "\n",
    "    # Parameters to be used for the NIRISS spec3 association creation\n",
    "    spec3_pid = str(program) # associations are only set up to combine for the same program & observation\n",
    "    spec3_obs = str(sci_observtn) # associations are only set up to combine for the same program & observation\n",
    "    spec3_ins = 'NIRISS'\n",
    "\n",
    "    # Identify the unique filters & grisms used for the NIRISS WFSS cal files\n",
    "    spec3_dict = {}\n",
    "    spec3_dict['PUPIL'] = np.array([fits.getval(cf, 'PUPIL') for cf in spec2_cal_files])\n",
    "    spec3_dict['FILTER'] = np.array([fits.getval(cf, 'FILTER') for cf in spec2_cal_files])\n",
    "    spec3_dict['PATHNAMES'] = np.array(spec2_cal_files)\n",
    "\n",
    "    spec3_df = pd.DataFrame(spec3_dict)\n",
    "\n",
    "    # Loop over unique pupil values\n",
    "    for spec3_filter in spec3_df['PUPIL'].unique():        \n",
    "        # Loop over unique filter values\n",
    "        for spec3_grism in spec3_df['FILTER'].unique():\n",
    "            # find the files specific to each of the filters & grisms\n",
    "            spec3_files = spec3_df[(spec3_df['PUPIL'] == spec3_filter) & (spec3_df['FILTER'] == spec3_grism)]['PATHNAMES']\n",
    "\n",
    "            # build the association names to match the default names from the pipeline\n",
    "            product_name = f\"jw{spec3_pid}-o{spec3_obs}_{spec3_ins}_{spec3_grism}-{spec3_filter}\".lower()\n",
    "            spec3_asn_filename = product_name + '_spec3_asn.json'\n",
    "    \n",
    "            spec3_association = asn_from_list.asn_from_list(spec3_files, rule=DMS_Level3_Base,\n",
    "                                                            product_name=product_name)\n",
    "    \n",
    "            spec3_association.data['asn_type'] = 'spec3'\n",
    "            spec3_association.data['program'] = spec3_pid\n",
    "    \n",
    "            # Format association as .json file\n",
    "            _, serialized = spec3_association.dump(format=\"json\")\n",
    "\n",
    "            # Write out association file\n",
    "            association_spec3 = os.path.join(sci_dir, spec3_asn_filename)\n",
    "            with open(association_spec3, \"w\") as fd:\n",
    "                fd.write(serialized)\n",
    "\n",
    "            print(f'Writing spec3 association: {association_spec3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d06405-3f22-459c-9810-c683d3cc478b",
   "metadata": {},
   "source": [
    "Take a quick look at the contents of the first spec3 association file to get a feel for what is being associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5dcb4f-48fc-40c2-a189-b05e24d490e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec3:\n",
    "    spec3_asns = glob.glob(os.path.join(sci_dir, \"*spec3_asn.json\"))\n",
    "    \n",
    "    # open the image3 association to look at\n",
    "    spec3_asn_data = json.load(open(spec3_asns[0]))\n",
    "    print(f'asn_type : {spec3_asn_data[\"asn_type\"]}')\n",
    "    print(f'code_version : {spec3_asn_data[\"code_version\"]}')\n",
    "    \n",
    "    # in particular, take a closer look at the product filenames with the association file:\n",
    "    for product in spec3_asn_data['products']:\n",
    "        for key, value in product.items():\n",
    "            if key == 'members':\n",
    "                print(f\"{key}:\")\n",
    "                for member in value:\n",
    "                    print(f\"    {member['expname']} {member['exptype']}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0a87a-d59a-4e7c-9605-eceab9875633",
   "metadata": {},
   "source": [
    "### Run Spec3\n",
    "\n",
    "In Spec3, the `*_cal.fits` files are reorganized based on source number from the Image3 Pipeline's source catalog, extracted into level 3 `*_x1d.fits` files, and then combined into a single 1-D spectrum (`*_c1d.fits` files) for each source. The parameters in each of the [Spec3 steps](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html) can be modified from the default values, including overwriting reference files that are used during this stage. This dictionary of the modified parameters for each of the steps is then fed into the `steps` parameter of the `Spec3Pipeline` call. The syntax for modifying some of these parameters is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec3 pipeline should be configured.\n",
    "\n",
    "# this sets up any entry to spec3dict to be a dictionary itself\n",
    "spec3dict = defaultdict(dict)\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example).\n",
    "# spec3dict['pixel_replace']['skip'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0588f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 3\n",
    "if dospec3:\n",
    "    for spec3_asn in spec3_asns:\n",
    "        os.chdir(spec3_dir)\n",
    "        spec3 = Spec3Pipeline.call(spec3_asn, output_dir=spec3_dir, steps=spec3dict, save_results=True)\n",
    "        os.chdir(cwd) # change back into the directory you started in\n",
    "else:\n",
    "    print('Skipping Spec3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time_spec3_end = time.perf_counter()\n",
    "print(f\"Runtime for Spec3: {(time_spec3_end - time_spec3)/60:0.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f7e47",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9187c",
   "metadata": {},
   "source": [
    "# 11. Understanding the Spec3 Outputs\n",
    "------------------\n",
    "The outputs of spec3 are `*_x1d.fits` and `*_c1d.fits` files. Here we do a quick look into some important parts of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a99265-6274-4f37-9fc3-24a94798a7c7",
   "metadata": {},
   "source": [
    "Each extension of the spec3 `*_x1d.fits` files contains the extracted, 1-D spectra for an individual dither for a single grism, filter, and extracted order combination. The specific filenames and extracted order can be verified with the `FILENAME` and `SPORDER` keywords in the header of each extension respectively. Within the extension, each of the extracted sources across all dithers are listed, with the values being empty if the particular dither did not contain data for that source. Also contained within each extension is information related to the extraction of a particular source, including the extents and starting size of the extraction box in the full reference frame. More information about the columns contained withing the `*_x1d.fits` files can be found in the [x1d filetype documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html#extracted-1-d-spectroscopic-data-x1d-and-x1dints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a list of the spec3 output x1d and c1d files\n",
    "spec3_x1ds = sorted(glob.glob(os.path.join(spec3_dir, \"*x1d.fits\")))\n",
    "print('spec3 x1d files:')\n",
    "for x1d_filename in spec3_x1ds:\n",
    "    print(f\"   {os.path.basename(x1d_filename)}\")\n",
    "\n",
    "spec3_c1ds = sorted(glob.glob(os.path.join(spec3_dir, \"*c1d.fits\")))\n",
    "print('spec3 c1d files:')\n",
    "for c1d_filename in spec3_c1ds:\n",
    "    print(f\"   {os.path.basename(c1d_filename)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aab189-ddea-41e5-b631-c9b1a52a9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the structure of the x1d files by reading in the first one\n",
    "if doviz:\n",
    "    sample_x1d = fits.open(spec3_x1ds[0])\n",
    "\n",
    "    print(\"***Format of the level 3 x1d file:\")\n",
    "    sample_x1d.info()\n",
    "\n",
    "    print(\"\\n***cal files used to create this level 3 x1d file:\")\n",
    "    for ext in range(len(sample_x1d))[1:-1]:\n",
    "        print(f\"Extension {ext}: {sample_x1d[ext].header['FILENAME']}, order {sample_x1d[ext].header['SPORDER']}\")\n",
    "\n",
    "    print(\"\\n***Columns contained in each extension of the level 3 x1d file:\")\n",
    "    print(sample_x1d[1].data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6a541-e1f7-41cd-a0d8-7db221fd9fb5",
   "metadata": {},
   "source": [
    "The `*_c1d.fits` files contain combined extensions of the same order in the spec3 `*_x1d.fits` files into a single file. The source numbers in the `*_c1d.fits` match those in the level 3 `*_x1d.fits` files. More information about the columns contained within the `*_c1d.fits` files can be found in the [c1d filetype documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html#combined-1-d-spectroscopic-data-c1d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e28edf-9c77-4773-88a6-2ba6cc7d1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about the structure of the c1d files by reading in the first one\n",
    "if doviz:\n",
    "    sample_c1d = fits.open(spec3_c1ds[0])\n",
    "\n",
    "    print(\"***Format of the c1d file:\")\n",
    "    sample_c1d.info()\n",
    "\n",
    "    print(\"\\n***Extracted orders contained in the c1d file:\")\n",
    "    for ext in range(len(sample_c1d))[1:-1]:\n",
    "        print(f\"Extension {ext}: order {sample_c1d[ext].header['SPORDER']}\")\n",
    "    \n",
    "    print(\"\\n***Columns contained in each extension of the c1d file:\")\n",
    "    print(sample_c1d[1].data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba332b3-8dd4-4225-bce0-8698b36055de",
   "metadata": {},
   "source": [
    "Digging a little bit further into the different source IDs and how those are handled, you can see that in each extension the source IDs are now identical, which is not always the case in the level 2 x1d files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad065fa-3399-480d-a981-4ec7a2a7f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    for ext in np.arange(len(sample_x1d))[1:-1]:\n",
    "        print(f\"Extension {ext}: {sample_x1d[ext].header['FILENAME']}, Order {sample_x1d[ext].header['SPORDER']}\")\n",
    "        print(\"  Sources:\\n\", sample_x1d[ext].data['SOURCE_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033df817-d2c3-4479-92ec-4c1a9df39ce1",
   "metadata": {},
   "source": [
    "If a source was not extracted for a given extension, the values will be filled in with a value of \"0\" or \"nan\". The column `N_ALONGDISP` is a useful tracer for finding sources that were not extracted as it represents the number of pixels in the trace along the dispersion direction, so if it is zero, no pixels were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e54974-13b6-4555-ba17-ab58ad02b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at extension 1 (first file) as an example of what a source looks like if it's not extracted\n",
    "ext = 1\n",
    "wh_no_source = np.where(sample_x1d[ext].data['N_ALONGDISP'] == 0)[0]\n",
    "if len(wh_no_source) > 0:\n",
    "    print(f\"{sample_x1d[ext].header['FILENAME']} does not extract the following sources:\")\n",
    "    print(f\"  {sample_x1d[ext].data['SOURCE_ID'][wh_no_source]}\")\n",
    "    print(\"Different column defaults when a source is not extracted:\")\n",
    "    for colname in sample_x1d[ext].data.names:\n",
    "        print(f\"  {colname} : {np.unique(sample_x1d[ext].data[colname][wh_no_source[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb6db0-c924-47e9-bbde-dbc162bbd166",
   "metadata": {},
   "source": [
    "### Visualize Spec3 Outputs\n",
    "\n",
    "To compare with the Spec2 output products above, we look at the same sources, plotting instead the final `*_c1d.fits` files for each grism. We again show the `*_i2d.fits` image for a specific source, followed by the level 3 `*_x1d.fits` individual spectra for each of the two grisms (if both were used--if one is not used that column will be blank), followed by the `*_c1d.fits` combined spectrum for each of the grisms if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0829d0-c072-436f-8500-04b105cfbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have run the cells defined convienence functions section: plot_i2d_plus_source & plot_spectrum\n",
    "# this cell looks at the i2d images, the level 3 x1d spectra, and the combined c1d spectra for both grisms for several sources\n",
    "if doviz:\n",
    "\n",
    "    # grab the c1d files to plot\n",
    "    spec3_c1ds = np.sort(glob.glob(os.path.join(spec3_dir, \"*c1d.fits\")))\n",
    "\n",
    "    # If there are multiple pupils (blocking filters) pick one for illustration\n",
    "    unique_pupils = np.unique([fits.getval(c1d, 'PUPIL') for c1d in spec3_c1ds])\n",
    "    pupil_c1ds = [c1d for c1d in spec3_c1ds if fits.getval(c1d, 'PUPIL') == unique_pupils[0]]\n",
    "\n",
    "    # find the sources to plot\n",
    "    if demo_mode:\n",
    "        # define some cool sources to look at if using the demo mode\n",
    "        sources = [417, 422, 505, 1296, 606]\n",
    "        nsources = len(sources)\n",
    "    else:\n",
    "        # or grab some sources from the first x1d file\n",
    "        nsources = 5 # 100 sources are extracted by default\n",
    "        source_offset = 10 # offsetting what nsources to plot to avoid extra bright sources\n",
    "        with fits.open(pupil_c1ds[0]) as temp_c1d:\n",
    "            sources = temp_c1d[1].data['SOURCE_ID'][source_offset:nsources+source_offset]        \n",
    "\n",
    "    # setting up the figure\n",
    "    cols = 4\n",
    "    rows = nsources\n",
    "    fig_c1d = plt.figure(figsize=(15, 4*(rows/2)))\n",
    "\n",
    "    # looping through the different sources to plot; one per row\n",
    "    for nsource, source_id in enumerate(sources):\n",
    "\n",
    "        # setting up the subplots for a single source\n",
    "        ypos = nsource\n",
    "        ax_i2d = plt.subplot2grid((rows, cols), (ypos, 0)) \n",
    "        ax_x1d_r = plt.subplot2grid((rows, cols), (ypos, 1))\n",
    "        ax_x1d_c = plt.subplot2grid((rows, cols), (ypos, 2))\n",
    "        ax_c1d = plt.subplot2grid((rows, cols), (ypos, 3))\n",
    "    \n",
    "        source_fluxes = [] # save the source flux to set the plot limits\n",
    "\n",
    "        # plot all of the 1-D combined spectra from the c1d files\n",
    "        for nfile, c1dfile in enumerate(pupil_c1ds):\n",
    "            \n",
    "            # plotting the c1d spectra\n",
    "            ax_c1d, catname, source_fluxes, grism = plot_spectrum(c1dfile, source_fluxes, ax_c1d, image3_dir)\n",
    "                \n",
    "            # plot the level 3 x1d files\n",
    "            x1dfile = c1dfile.replace('c1d', 'x1d')\n",
    "            with fits.open(x1dfile) as x1d:\n",
    "                for ext in range(len(x1d))[1:-1]:\n",
    "                    if grism == 'GR150R':\n",
    "                        ax_x1d_r, catname, source_fluxes, grism = plot_spectrum(x1dfile, source_fluxes, ax_x1d_r, image3_dir, ext=ext, legend=False)\n",
    "                    else:\n",
    "                        ax_x1d_c, catname, source_fluxes, grism = plot_spectrum(x1dfile, source_fluxes, ax_x1d_c, image3_dir, ext=ext, legend=False)\n",
    "            \n",
    "            # plot the direct image of the source based on the source number from the source catalog\n",
    "            if nfile == 0:\n",
    "                ax_i2d = plot_i2d_plus_source(catname, source_id, ax_i2d)\n",
    "\n",
    "        # plot labels and such\n",
    "        if len(source_fluxes):\n",
    "            # there may not have been data to extract if everything was saturated\n",
    "            ax_c1d.set_ylim(np.nanmin(source_fluxes), np.nanmax(source_fluxes))\n",
    "            \n",
    "        # Add labels to the subplots\n",
    "        if nsource == 0:\n",
    "            ax_i2d.set_title('Direct Image\\n(i2d)')\n",
    "            ax_x1d_r.set_title('Individual GR150R 1-D Spectrum\\n(level 3 x1d)')\n",
    "            ax_x1d_c.set_title('Individual GR150C 1-D Spectrum\\n(level 3 x1d)')\n",
    "            ax_c1d.set_title('Combined 1-D Spectrum\\n(c1d)')\n",
    "        ax_i2d.set_ylabel(f'Source\\n{source_id}', fontsize=15)\n",
    "\n",
    "        for ax in [ax_x1d_r, ax_x1d_c, ax_c1d]:\n",
    "            ax.set_xlabel('Wavelength (microns)')\n",
    "            ax.set_ylabel('F_nu')\n",
    "            ax.ticklabel_format(axis='y', style='sci', scilimits=(0, 0)) # forcing scientific notation for the spectra\n",
    "        \n",
    "    fig_c1d.tight_layout()\n",
    "    fig_c1d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca1f65",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_footer.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
