{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e21d550",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png' alt=\"stsci_logo\" width=\"1000px\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc24f5c",
   "metadata": {},
   "source": [
    "<!-- # TSO JWebbinar Notebook 1: Downloading and Calibrating `uncal` TSO Products -->\n",
    "# NIRISS/SOSS Notebook 1: Downloading and Calibrating 'uncal' TSO Products\n",
    "-----\n",
    "\n",
    "**Authors**:\n",
    "- **Tyler Baines** | Science Support Analyst | NIRISS Branch | tbaines@stsci.edu\n",
    "- **NÃ©stor Espinoza** | AURA Assistant Astronomer | Mission Scientist for Exoplanet Science | nespinoza@stsci.edu\n",
    "- **Aarynn Carter** | AURA Assistant Astronomer | NIRISS Branch | aacarter@stsci.edu\n",
    "\n",
    "**Date Published**: May 1st, 2024\n",
    "\n",
    "**Last Updated**: May 1st, 2024\n",
    "\n",
    "<!-- **Pipeline Version**: 1.12.5 -->\n",
    "\n",
    "## Table of contents\n",
    "1. [Introduction](#introduction)<br>\n",
    "      1.1 [Purpose of this Notebook](#purpose)<br>\n",
    "      1.2 [Data & Context of the Observations](#data)<br>\n",
    "2. [Imports](#imports)<br>\n",
    "3. [Downloading & Quick Looks at JWST TSO data](#download)<br>\n",
    "      3.1 [Downloading TSO data from MAST](#mast)<br>\n",
    "      3.2 [Quicklook, pt. I: Target Acquisition](#ta)<br>\n",
    "      3.3 [Quicklook, pt. II: `datamodels` & TSO Science Data Products](#science)<br>\n",
    "4. [A TSO tour through the `Detector1` stage](#detector1)<br>\n",
    "      4.1 [Checking data quality flags](#dqflags)<br>\n",
    "      4.2 [Identifying saturated pixels](#saturation)<br>\n",
    "      4.3 [Removing detector-level effects: the `superbias` and `refpix` steps](#refpix)<br>\n",
    "      4.4 [Linearity corrections](#linearity)<br>\n",
    "      4.5 [Removing the dark current](#dark-current)<br>\n",
    "      4.6 [Correcting 1/f noise](#one_over_f)<br>\n",
    "      4.7 [Detecting \"jumps\" on up-the-ramp sampling](#jump)<br>\n",
    "      4.8 [Fitting ramps with the `ramp_fit` step](#rampfit)<br>\n",
    "5. [Final words](#final-words)<br>\n",
    "\n",
    "\n",
    "CRDS Context used: jwst_1225.pmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a705b",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "### 1.1 Purpose of this Notebook<a class=\"anchor\" id=\"purpose\"></a>\n",
    "\n",
    "In this Notebook, we aim to perform an exploration of Time Series Observations (TSO) products, focusing in particular on products obtained by the [Transiting Exoplanet JWST Early Release Science (ERS) team](https://www.stsci.edu/jwst/science-execution/approved-programs/dd-ers/program-1366) --- a real science dataset that we will reduce starting from the most \"raw\" forms of data products that can be downloaded from MAST. We will learn how to download those products, as well as how to load them and make them interact with the JWST Calibration Pipeline to calibrate them. In a companion Notebook, we then perform spectroscopic analyses on this dataset.\n",
    "\n",
    "### 1.2 Data & Context of the Observations<a class=\"anchor\" id=\"data\"></a> \n",
    "\n",
    "The input data for this Notebook are observations from the ERS [Program 1366](https://www.stsci.edu/jwst/science-execution/program-information) where we will explore observations of the exoplanet WASP-39b obtained with the JWST [NIRISS/SOSS](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-observing-modes/niriss-single-object-slitless-spectroscopy) mode. This mode uses a unique grism ([GR700XD](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-gr700xd-grism)) to seperate the light of a source across three diffraction orders covering a broad wavelength range from 0.6-2.8 $\\mu m$ with a moderate spectral resolution (R $\\approx$ 700 at 1.4 $\\mu m$). \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Notes on the validity of this notebook</b>: It is important to realize that this notebook, as it is, is likely to be quickly outdated as new algorithms and fixes are implemented into the JWST Calibration pipeline, as well as new methodologies and studies update our knowledge of optimally calibrating JWST data products. An up-to-date list of known JWST pipeline issues (some of which we touch on this notebook) can be found on the <a href=\"https://jwst-docs.stsci.edu/jwst-calibration-pipeline-caveats/known-issues-with-jwst-data-products\">Known Issues with the JWST Data Products</a> JDox page. In doubt, or for any questions, please contact <a href=\"https://jwst-docs.stsci.edu/jwst-help-desk\">the JWST Helpdesk</a>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffeef35",
   "metadata": {},
   "source": [
    "## 2. Imports <a class=\"anchor\" id=\"imports\"></a>\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "For this demonstration we will need the following packages to be instraalled in your python environemnt:\n",
    "1. numpy \n",
    "2. scipy\n",
    "3. astropy\n",
    "4. matplotlib\n",
    "5. astroquery\n",
    "6. jwst\n",
    "7. pastasoss\n",
    "\n",
    "or you can create a conda environment using the provided requirements file following the step below:\n",
    "\n",
    "```markdown\n",
    "conda create -n jwst-soss-demo-py3.10 python=3.10 pip\n",
    "conda activate jwst-soss-demo-py3.10\n",
    "pip install -r requirements-soss.txt\n",
    "```\n",
    "With the working python environment setup lets begin importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccca95-97c0-4a4c-88e6-7c8d5d989ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ General Imports ------\n",
    "import os\n",
    "\n",
    "if \"CRDS_PATH\" not in os.environ:\n",
    "    %set_env CRDS_PATH $HOME/crds_cache\n",
    "    %set_env CRDS_SERVER_URL https://jwst-crds.stsci.edu\n",
    "else:\n",
    "    print(f\"CRDS_PATH: {os.environ['CRDS_PATH']}\")\n",
    "    print(f\"CRDS_SERVER_URL: {os.environ['CRDS_SERVER_URL']}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ------ Plotting/Stats Imports ------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------ Downloading MAST data products ------\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# ------ JWST Calibration Pipeline Imports ------\n",
    "import jwst\n",
    "from jwst import datamodels\n",
    "from jwst.pipeline import calwebb_detector1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2aa5e9",
   "metadata": {},
   "source": [
    "Lastly, lets configure some of the plotting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['image.origin'] = 'lower'\n",
    "plt.rcParams['image.aspect'] = 'auto'\n",
    "plt.rcParams['image.interpolation'] = None\n",
    "plt.rcParams['image.cmap'] = 'inferno'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3de65",
   "metadata": {},
   "source": [
    "## 3. Downloading JWST TSO data <a class=\"anchor\" id=\"download\"></a>\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "The very first step when it comes to analyzing a JWST dataset is to download that data and perform some quick looks so we know the data quality is acceptable to begin with. Here, we will download the `uncal` products, which are one of the \"raw\"-est forms of dataproducts users can download from MAST. We will perform our data download from MAST using `astroquery.mast` and then use the JWST Calibration pipeline to read and have quicklooks at this data. Let's begin!\n",
    "\n",
    "### 3.1 Downloading TSO data from MAST<a class=\"anchor\" id=\"mast\"></a> \n",
    "\n",
    "To download JWST data from MAST, we will use the `Observations` function from the `astroquery.mast` library. To do this, we need to indicate the properties of the dataset of interest. For this we need to figure out what instrument, filter, program ID _and_ target was obseved. Options for JWST TSO instruments are `NIRISS/SOSS`, `NIRSPEC/SLIT`, `NIRCAM/GRISM`, `MIRI/SLITLESS`, `NIRSPEC/SLIT`, etc. Here, we search for `NIRISS/SOSS`, and the `CLEAR;GR700XD` filter/grating combination, which corresponds to the dataset we want. We define the proposal ID for the ERS program (`1366`) and the name of the target, `WASP-39`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b211a40-1666-43c1-9c26-fe06bd458336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query MAST data\n",
    "query_results = Observations.query_criteria(\n",
    "    instrument_name='NIRISS/SOSS',\n",
    "    filters='CLEAR;GR700XD',\n",
    "    proposal_id='1366',\n",
    "    target_name='WASP-39'\n",
    ")\n",
    "\n",
    "# Define columns to display\n",
    "columns_to_display = [\n",
    "    'obs_collection', 'instrument_name', 'filters', 'target_name', 'obs_id',\n",
    "    's_ra', 's_dec', 't_exptime', 'proposal_id'\n",
    "]\n",
    "\n",
    "# Display results\n",
    "query_results[columns_to_display].show_in_notebook(display_length=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f10a82-9cef-4f56-b45d-516ee0a37018",
   "metadata": {},
   "source": [
    "This stores _all_ possible observations in the `query_results` variable. Then, we filter all the products to get only the `SCIENCE`, `UNCAL` data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d45ff-9bf9-41dd-96c2-d62e87f8d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data products:\n",
    "data_products = Observations.get_product_list(query_results)\n",
    "# data_products.show_in_notebook(display_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e21ae4-11d2-43c3-b40b-c6484420ccbe",
   "metadata": {},
   "source": [
    "Now we'll filter the results to obtain the uncalibrated data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncals = Observations.filter_products(\n",
    "    data_products, \n",
    "    productType='SCIENCE',\n",
    "    productSubGroupDescription='UNCAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3471c-584c-4c03-a6f4-7b947b4b8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncals[['obs_id', 'productSubGroupDescription', 'size']].show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ba179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of final data products {len(uncals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffcae5-05f1-440f-a43e-965ce0972196",
   "metadata": {},
   "source": [
    "Note how there are 9 data products. The ones with the lowest `size` values are the Target Aquisition exposures, used to align the telescope with the target of interest. These are _very_ useful to check the quality of the observations (and whether or not they were successful!). The _actual_ TSO data are all the products that follow. Note the latter data are segmented --- this is done in the ground to facilitate the processing of the data. \n",
    "\n",
    "Let's download all the data, including the Target Acquisition frames, which might be useful to diagnose the quality of observations (this might take some time ~5-7 mins):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeea47a-9e80-405a-8c2d-0474ae915fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the location where you want to download your data to\n",
    "download = True\n",
    "download_dir = \"data/\"\n",
    "\n",
    "# make sure the download directory exists; if not, write a new directory\n",
    "if not os.path.exists(download_dir):\n",
    "    os.mkdir(download_dir)\n",
    "\n",
    "if download:\n",
    "    Observations.download_products(uncals, download_dir=download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe27c6-6d56-451c-94f6-60c374a04969",
   "metadata": {},
   "source": [
    "Great, all downloads are complete! Let's perform some quick looks at those datasets, which we can do right away without the need to actually calibrate our data products.\n",
    "\n",
    "### 3.2<font color='white'> </font>Quicklook, pt. I: Target Acquisition<a class=\"anchor\" id=\"ta\"></a>\n",
    "\n",
    "The first set of data products we will have a look at are the Target Acquisition (TA) frames. These are frames that are used to precisely center objects in JWST, so as to correct from any JWST blind pointing errors. These frames are taken before the optical element that disperses the light for our TSO observations is put into place, and before doing any small slews to the actual science targets (which should be in the worst case scenario a few tens of arcseconds away from the science target).\n",
    "\n",
    "Note there are 4 TA frames. [This is expected](https://jwst-docs.stsci.edu/jwst-near-infrared-spectrograph/nirspec-operations/nirspec-target-acquisition/nirspec-wide-aperture-target-acquisition); the usual TA WATA procedure (which is used for TSOs) has one exposure that is used to correct for any pointing errors, and a post-correction TA, which is taken as a \"confirmation\" exposure. We can load those frames with the JWST `datamodels`, which as we will see below are extremely useful models to deal with JWST data, as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84335c83-763d-4add-a3e8-e5da91f02cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta1 = datamodels.RampModel(download_dir + '/mastDownload/JWST/jw01366001001_02101_00001-seg001_nis/jw01366001001_02101_00001-seg001_nis_uncal.fits')\n",
    "ta2 = datamodels.RampModel(download_dir + '/mastDownload/JWST/jw01366001001_02101_00002-seg001_nis/jw01366001001_02101_00002-seg001_nis_uncal.fits')\n",
    "ta3 = datamodels.RampModel(download_dir + '/mastDownload/JWST/jw01366001001_02101_00003-seg001_nis/jw01366001001_02101_00003-seg001_nis_uncal.fits')\n",
    "ta4 = datamodels.RampModel(download_dir + '/mastDownload/JWST/jw01366001001_02101_00004-seg001_nis/jw01366001001_02101_00004-seg001_nis_uncal.fits')\n",
    "\n",
    "TAs = [ta1, ta2, ta3, ta4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afe9c7-80e1-427b-a821-e5e62e13687d",
   "metadata": {},
   "source": [
    "The `data` attribute of those `datamodels` (e.g., `ta1.data`) stores the actual data from those frames. Let's check the dimensions of those first to familiarize ourselves with those data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77f679-a70c-4ac0-9e27-50e293aa3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ta in TAs:\n",
    "    print(f'Dimensions of the TA frame of {ta.meta.filename}: {ta.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05708b2-87f7-4c0d-bc31-d8ca3d9adda7",
   "metadata": {},
   "source": [
    "The dimensions come in the form `(integrations, groups, pixel, pixel)` --- so both are 1-integration exposures, of 13 groups each, on a 64x64 pixel frame. This actually is exactly what is expected from WATA TA frames!\n",
    "\n",
    "Let's take a look at the TA frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_index = 0\n",
    "group_index = -1\n",
    "\n",
    "vmin = 9000\n",
    "vmax = 16000\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "# Plot the TA images\n",
    "for i, ta in enumerate(TAs):\n",
    "    axes[0, i].imshow(ta.data[int_index, group_index, :, :], vmin=vmin, vmax=vmax)\n",
    "    axes[0, i].set_title(f'TA_{i+1}')\n",
    "\n",
    "# Plot the difference images\n",
    "for i in range(len(TAs)):\n",
    "    if i == 0:\n",
    "        diff = TAs[i].data[int_index, group_index, :, :] - TAs[-1].data[int_index, group_index, :, :]\n",
    "    else:\n",
    "        diff = TAs[i].data[int_index, group_index, :, :] - TAs[i-1].data[int_index, group_index, :, :]\n",
    "    axes[1, i].imshow(diff, interpolation=None)\n",
    "    axes[1, i].set_title(f'TA_{i+1} - TA_{i if i > 0 else 4}')\n",
    "\n",
    "# add vertical and horizontal crosshair. \n",
    "for ax in axes.ravel():\n",
    "    ax.axvline(64//2, ls='--', color='white', lw=0.75)\n",
    "    ax.axhline(64//2, ls='--', color='white', lw=0.75)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae5a66-6fb4-4626-8acd-8e4e86aead7b",
   "metadata": {},
   "source": [
    "Nice! There is a source, although it is rather faint, the difference between TA frames supresses the background variation enable the target source to become more apparent given by the sources positive and negative in the differnece frames. The target is reasonably placed around the center of the frame by the 4th exposure. \n",
    "\n",
    "We won't need the TA loaded in anymore so lets go ahead and release them from memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ta1, ta2, ta3, ta4, TAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3cfe5-009e-4088-a00b-ddf7b0cb212e",
   "metadata": {},
   "source": [
    "### 3.3 Quicklook, pt. II: `datamodels` & TSO Science Data Products<a class=\"anchor\" id=\"science\"></a> \n",
    "\n",
    "Next up, we will load the **TSO science** data products so we can interact with them. Once again, we open them through the JWST `datamodels` --- and store all segments of data on lists and explored the dimensions of the data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035daa26-b70a-46da-a8fc-3dd63cb41602",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '/mastDownload/JWST/jw01366001001_04101_00001-seg001_nis/jw01366001001_04101_00001-seg001_nis_uncal.fits',\n",
    "    '/mastDownload/JWST/jw01366001001_04101_00001-seg002_nis/jw01366001001_04101_00001-seg002_nis_uncal.fits',\n",
    "    '/mastDownload/JWST/jw01366001001_04101_00001-seg003_nis/jw01366001001_04101_00001-seg003_nis_uncal.fits',\n",
    "    '/mastDownload/JWST/jw01366001001_04101_00001-seg004_nis/jw01366001001_04101_00001-seg004_nis_uncal.fits'\n",
    "]\n",
    "\n",
    "uncal_nis = [datamodels.RampModel(download_dir + file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uncal in uncal_nis:\n",
    "    print(f'Dimensions of the TSO frame: {uncal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c91da3-d4bf-4e94-a176-6743bcfde91a",
   "metadata": {},
   "source": [
    "Note how we load each segment of data for each detector in simple python `list`s! This is the simplicity that these `datamodels` offer. We explore them a bit more below before continuing to the next Section.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Note on memory usage</b>: Loading data products in lists is very useful, but be aware that in particular for TSOs --- which typically involve large data volumes --- Random-Access Memory (RAM) might be severely impacted. The above loaded data products, for instance, take of order ~5 GB --- and this will only be larger as we run pipeline steps below, which convert data products from, e.g., <code>int</code>s to <code>float</code>s, taking even more space. For a typical TSO, when running the pipeline steps we'll run below, consider on the order of ~50 GB will be used. If you don't have 50GB of RAM they should consider alternatives such as a server, or run files individually</div>\n",
    "\n",
    "JWST `datamodels` simplify accessing vital information from `fits` files, such as instrument/mode, observation dates, and other header values, without needing to know the exact header keywords. For example, to find the observation date in NIS detector data, the datamodels `search` function quickly locates this information, demonstrating its practicality and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76809c-e8d7-40a2-b206-d67613ff8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0].search('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a034dd-e93e-41e4-a707-cc9e14762e0a",
   "metadata": {},
   "source": [
    "Note how we just added a word similar to what we were looking for, and then this function will take a look to find where similar words are located in the `datamodels` attribute tree. From the above, it seems this information is in `meta.date`. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f0b00-4589-4a15-adb8-5a956e1b23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0].meta.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4eb9b-d3b7-4ae2-8757-c26dbc4a8dd2",
   "metadata": {},
   "source": [
    "It works! Note `date_beg` is the one we would be typically interested in checking (which was when the observations happened). Let's try another one. Suppose we wanted to know the name of the PI of this program. Again, the key word here is `pi` so let's insert that one in the `search` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf6499-4621-4ad8-83c9-1f5e53cc110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0].search('pi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8c1d5-d797-443c-a126-8b1a8ba24a9d",
   "metadata": {},
   "source": [
    "So this exists under `meta.program.pi_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d6668-f2fe-4b4a-a5de-a8d4c82a4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0].meta.program.pi_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21230db-23d8-40cc-8cac-9dcd1c8bf041",
   "metadata": {},
   "source": [
    "As for the science frames, dimensions come in the form `(integrations, groups, pixel, pixel)`. So this is a 158-integration segment, with 9 groups per integration each, on a subarray of dimensions 256 x 2048 --- that sounds about right for NIRISS CLEAR/GR700XD exposures. Let's explore integration number 10 of the last group to get and idea of what NIRISS/SOSS data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614537d-88e5-4a82-8a2c-535979b1d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_index = 0\n",
    "int_index = 10\n",
    "group_index = -1\n",
    "\n",
    "data = uncal_nis[segment_index].data[int_index, group_index, :, :]\n",
    "\n",
    "vmin = data.min()\n",
    "vmax = data.mean() * 2.0\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('Uncal NIS CLEAR/GR700XD data; first segment, integration 10, last group')\n",
    "plt.imshow(data, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6118f3-2ad6-4ab0-a45c-382a6de63c41",
   "metadata": {},
   "source": [
    "The data looks great! The sweeping streaks across the dectector columns is the spectrum of WASP-39 dispersed over 3 spectral orders where the brightest spectrum corresponds to order 1 (primary science) which spans a wavelength range from about 0.8 to 2.8 $\\mu m $, the next brightest corresponds to order 2 (secondary science), and followed by order 3 has the lowest throughput of the three and may not be visible raw images. Some structure in the image is mostly dominated by detector-level effects that will be dealt with in the next section of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49194a6",
   "metadata": {},
   "source": [
    "## 4. A TSO tour through the `Detector1` stage <a class=\"anchor\" id=\"detector1\"></a>\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "The `uncal` data products we loaded above contain a series of detector systematic effects that we need to remove before our data is ready for science. Now, we will move to calibrating those TSO `uncal` data products, which will take care of most of those effects. \n",
    "\n",
    "To perform this calibration, here we will follow most of the steps outlined in the `calwebb_detector1` or \"Stage 1\" processing described in the [JWST Calibration pipeline documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html) --- in particular, the one defined for \"Near-IR\" instruments, such as NIRISS. This Stage 1 processing for Near-IR TSOs is defined by a series of steps, which in order are:\n",
    "\n",
    "1. `group_scale` (not relevant, there is no impact at all for SOSS data)\n",
    "2. `dq_init`\n",
    "3. `saturation`\n",
    "4. `superbias`\n",
    "5. `refpix`\n",
    "6. `linearity`\n",
    "7. `dark_current`\n",
    "8. `jump`\n",
    "9. `ramp_fitting`\n",
    "10. `gain_scale` (not relevant, there is no impact at all for SOSS data)\n",
    "\n",
    "We will slightly modify and/or add some steps to suit our TSO needs below --- let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76375",
   "metadata": {},
   "source": [
    "### 4.1<font color='white'>-</font>Checking data quality flags <a class=\"anchor\" id=\"dqflags\"></a>\n",
    "\n",
    "An important component of any TSO analysis is to flag bad pixels, pixels identified as cosmic rays and/or identify saturated pixels. Bad pixels are, in fact, curated by the instrument teams in what we colloquially refer to as a \"bad pixel mask\" --- a mask one can \"attach\" to the data products with the JWST Calibration pipeline. This is exactly what the first step in the pipeline, the Data Quality initialized (`dq_init`) step, does. \n",
    "\n",
    "#### 4.1.1 Running & understanding the `dq_init` step\n",
    "\n",
    "Let's run the `dq_init` step on the first segment of our NIS data products: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a184ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the DQ init step; first for the first segment of the NIS detector:\n",
    "print('Running dq_init on NIS:')\n",
    "nis_seg1_dqinit = calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nis[0], save_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7c514-6606-4c2d-89eb-e5c60ca96186",
   "metadata": {},
   "source": [
    "All right, data-quality flags have been attached to our uncalibrated data products. To figure out why these are so useful, let's take a look at this bad pixel mask that was attached to our data products; in particular, let's peek at the one attached to the NIS detector products. This mask lives in the `pixeldq` attribute of our products (e.g., `nis_seg1_dqinit.pixeldq`). To familiarize ourselves with this, let's print the dimensions of this mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37733464-8f27-4dad-bb7a-3f4c374338ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of resulting PixelDQ array: {nis_seg1_dqinit.pixeldq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c75f617-64df-4121-a450-fd2ac27a4a69",
   "metadata": {},
   "source": [
    "As expected, it has the same size as our subarray data. [As per the documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/references_general/references_general.html?highlight=data%20quality%20flags#data-quality-flags), most data-quality (DQ) flags should be zero in the subarray; let's plot this array to see how many of them get away from this value and where:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21cbde-850a-4ad5-9d9a-567a82b0d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('Non-zero data-quality values across the subarray')\n",
    "plt.imshow(nis_seg1_dqinit.pixeldq, vmin=-0.5, vmax=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cbb8f-3281-4bb5-aee2-d1d85bed82e4",
   "metadata": {},
   "source": [
    "All right --- so there are \"special\" pixels all over the place! But, what are the `pixeldq` values telling us? Let's print the pixel in the very corner of the subarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5e735-e12d-4cef-ba9a-e93fbb4b9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nis_seg1_dqinit.pixeldq[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b1487-041d-4e6c-b573-97f77f99c525",
   "metadata": {},
   "source": [
    "[According to the documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/references_general/references_general.html?highlight=data%20quality%20flags#data-quality-flags), this pixel is a **reference pixel**. This makes sense: for NIRISS in this CLEAR/GR700XD mode, the 4 pixel columns on the left-most end, the 4 pixel columns on the right-most end, and the 4 top-most rows are indeed, reference pixels.\n",
    "\n",
    "#### 4.1.2 Dynamically translating data-quality flags to human-readable form\n",
    "\n",
    "Looking back and forth from the documentation page the data-quality flag values we read from our data-products is a very tedious task. In addition, as we will see below, a pixel can have eventually several flags (e.g., saturated, has a cosmic-ray, etc.) which will, in turn, change some of its data-quality flags to account for this. \n",
    "\n",
    "A handy function to convert those data-quality flag numbers to \"human-readable\" form is actually inside the `datamodels` class --- the `datamodels.dqflags`. This simply takes in a data-quality value, and spits out a `set` with strings defining what this is telling us given a so-called \"mnemonic map\" --- one which is actually already loaded in the `datamodels.dqflags.pixel` dictionary.\n",
    "\n",
    "Let's try it out on the data-quality value we observed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc893c6d-d4f9-4a3e-af44-7895ba4c207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodels.dqflags.dqflags_to_mnemonics(2147483648, mnemonic_map=datamodels.dqflags.pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e09913-917e-4be6-ac14-384a0545696a",
   "metadata": {},
   "source": [
    "Indeed, we get back what we knew --- that is a reference pixel! With this handy-dandy function, we can write a simple snippet to figure out the total tally of all bad pixels as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e0bc0-b336-4a62-b608-6ac934eb0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that will save all types of bad pixels:\n",
    "bad_pixels = {}\n",
    "\n",
    "rows, columns = nis_seg1_dqinit.pixeldq.shape\n",
    "\n",
    "# Iterate through every row and column:\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(\n",
    "            nis_seg1_dqinit.pixeldq[row, column], \n",
    "            mnemonic_map=datamodels.dqflags.pixel\n",
    "            )\n",
    "\n",
    "        # Iterate through the possible flags (it can be more than one!):\n",
    "        for bp in bps:\n",
    "\n",
    "            # If already in the bad_pixels dict, simply add 1 to the counter. If not, create and instantiate to one:\n",
    "            if bp in bad_pixels.keys():\n",
    "\n",
    "                bad_pixels[bp] += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                bad_pixels[bp] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab06830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now print total tally:\n",
    "total_pixels = rows * columns\n",
    "\n",
    "# Table headers\n",
    "header = f\"| {'PIXEL DQ FLAG':<20} | {'# of Bad pixels':^19} | {'Fraction of Bad pixel (%)':^25} |\"\n",
    "\n",
    "# Table separator\n",
    "separator = \"+\" + \"-\" * 22 + \"+\" + \"-\" * 21 + \"+\" + \"-\" * 27 + \"+\"\n",
    "\n",
    "# Printing the table\n",
    "print(separator)\n",
    "print(header)\n",
    "print(separator)\n",
    "for bp in bad_pixels.keys():\n",
    "    val = bad_pixels[bp]\n",
    "    frac = 100*(val/float(total_pixels))\n",
    "    print(f\"| {bp:<20} | {val:^19} | {frac:^25.2f} |\")\n",
    "print(separator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23c07a",
   "metadata": {},
   "source": [
    "Based on our discussion above, we can see some number above make sense. For instance, 10208 `REFERENCE_PIXELS` makes sense as there are a total of 8 columns (4 columns to left and right, and 4 rows to the top of the subarray) --- given the subarray height is 256 pixels and width 2048 pixels (2040 pixels to avoid double counting pixel in the upper corners) given a total of 10208 reference pixels as expected. \n",
    "\n",
    "Let's go ahead now and attach this bad pixel mask to all the segments of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe5f40-e7f5-4d51-ab91-5b5c4130186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsegments = len(uncal_nis)\n",
    "for i in range(nsegments):\n",
    "    uncal_nis[i] = calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nis[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5516bd-43b5-4e0a-8223-3a4a31d633ff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note on saving data products with the JWST Calibration Pipeline</b>: Sometimes, one might find it useful to save data products after running each step into <code>.fits</code> files, so we can have \"intermediate steps\" stored in our system that we can check at a later time. This can be done when running any of the steps by adding the <code>save_results = True</code> flag to the step calls, e.g., <code>calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nis[i], save_results = True)</code>. An output directory can also be defined by using the <code>output_dir</code> parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd854cc-47b5-457d-858a-08264af5a9c7",
   "metadata": {},
   "source": [
    "### 4.2 Identifying saturated pixels <a class=\"anchor\" id=\"saturation\"></a>\n",
    "\n",
    "One very important detail in JWST data analysis involves checking which pixels are \"saturated\" or not. Saturation in the JWST context is an [instrument-by-instrument defined upper signal level](https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-tso-saturation), typically defined as a limit above which detector effects not modeled by the pipeline might start to kick-in. As such, identifying which pixels are above this limit is important. In particular, the pipeline tends to omit those pixels from most analyses, as they might introduce unwanted systematic effects.\n",
    "\n",
    "#### 4.2.1 Running and understanding the `saturation` step\n",
    "\n",
    "Through the analysis of calibration datasets, the JWST instrument teams have defined signal values for each pixel above which they are considered as \"saturated\". This identification is done through the `saturation` step --- the next step of the JWST pipeline for Detector 1. Let's run it for the very first segment of data for NIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00464d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run saturation step:\n",
    "saturation_results = calwebb_detector1.saturation_step.SaturationStep.call(uncal_nis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bfef9",
   "metadata": {},
   "source": [
    "The saturation step works by primarily comparing the observed count values with the saturation signal-levels defined for each pixel in a reference file. As can be seen above, that reference file is indicated by the line `stpipe.SaturationStep - INFO - Using SATURATION reference file [yourfile]`. In the case of our run at the time of writing, this was the `jwst_niriss_saturation_0015.fits` file --- but this might change as new analyses are made and the reference files get updated. \n",
    "\n",
    "In addition, at the time of writing, the `saturation` step in the JWST Calibration pipeline [by default flags not only pixels that exceed the signal limit defined by the instrument teams but also all `n_pix_grow_sat` pixels around it](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/jwst-operations-pipeline-build-information/jwst-operations-pipeline-build-8-0-release-notes#JWSTOperationsPipelineBuild8.0ReleaseNotes-charge_spilling); which at the time of writing is set to a default of `1`. That means that if a given pixel exceeds the signal limit, all 8 pixels around it will be marked as saturated as well. This is done because it has been observed that \"charge spilling\" can be an issue --- i.e., charge going from one pixel to another. While such migration of charge happens at a wide range of count levels, this is particularly dramatic when a pixel saturates --- reason by which this is set in the pipeline.\n",
    "\n",
    "We can check which pixels are saturated in a similar way as to how we checked the data-quality flags in [Section 3.1](#dqflags). The only difference with that analysis is that saturated pixels are integration and group-dependant, i.e., a property of a given pixel _in a given integration and group_. In other words, a pixel that is saturated in one integration and group might have \"recovered\" by the next integration and group.\n",
    "\n",
    "To figure out the data-quality for all integrations and all groups we look at the `groupdq` attribute of our data products instead of the `pixeldq` which we used above. To familiarize ourselves with this, let's print the dimensions of this array first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2ca2a-139f-4511-b354-d4ec9c66f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_results.groupdq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cca152-fdd6-4f57-97df-a770da5eb613",
   "metadata": {},
   "source": [
    "As expected, it has dimensions `(integrations, groups, row pixels, column pixels)`, just like the `data` array. The flags in the `groupdq` array follow the same structure as [all the data-quality flags described in the documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/references_general/references_general.html?highlight=data%20quality%20flags#data-quality-flags). \n",
    "\n",
    "#### 4.2.2 Exploring saturated pixels via the `groupdq` array\n",
    "\n",
    "To illustrate how to use the `groupdq`, let's pick the last group of integration 10 again and see if any pixels seem to be saturated --- we also count all of the saturated pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17085a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every row and column of integration number 10, last group:\n",
    "integration, group = 10, -1\n",
    "nsaturated = 0\n",
    "\n",
    "# indice location of where saturated pixel found\n",
    "row_idx = []\n",
    "column_idx = []\n",
    "\n",
    "verbose = False\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(\n",
    "            saturation_results.groupdq[integration, group, row, column], \n",
    "            mnemonic_map=datamodels.dqflags.pixel\n",
    "            )\n",
    "        \n",
    "        # Check if pixel is saturated; if it is...\n",
    "        if 'SATURATED' in bps:\n",
    "\n",
    "            # ...print which pixel it is, and...\n",
    "            if verbose:\n",
    "                print('Pixel ({0:},{1:}) is saturated in integration 10, last group'.format(row, column))\n",
    "\n",
    "            # ...count it:\n",
    "            nsaturated += 1\n",
    "\n",
    "            column_idx.append(column)\n",
    "            row_idx.append(row)\n",
    "\n",
    "print('\\nA total of {0:} out of {1:} pixels ({2:.2f}%) are saturated'.format(nsaturated, \n",
    "                                                                             rows*columns, \n",
    "                                                                             100 * nsaturated / float(rows * columns)\n",
    "                                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a4728",
   "metadata": {},
   "source": [
    "As can be seen, not many pixels are saturated on a given group. Let's see how the up-the-ramp samples look like for one of those pixels --- let's say, pixel `(176, 1503)`. Let's show in the same plot the group data-quality flags at each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd84b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel_row, pixel_column = row_idx[60], column_idx[60]\n",
    "pixel_row, pixel_column = 176, 1503\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.title(f'Saturated Pixel: ({pixel_row}, {pixel_column})')\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.data[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color='tomato')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.xlabel('Group number', fontsize=16)\n",
    "plt.ylabel('Counts', fontsize=16, color='tomato')\n",
    "\n",
    "plt.twinx()\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.groupdq[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color='cornflowerblue')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.ylabel('Group Data-quality', fontsize=16, color='cornflowerblue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c506c-e069-4f37-89da-315b8d1a6231",
   "metadata": {},
   "source": [
    "Very interesting plot! Note that all groups appear to be saturated after group ~6 in this example. Likely a cosmic-ray hit happened at this group which left the pixel at a very high count number from group 6 up to the end of the ramp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d8613",
   "metadata": {},
   "source": [
    "#### 4.2.3 Setting custom saturation limits with the `saturation` reference file\n",
    "\n",
    "TSOs often obtain data from bright stars that might quickly (i.e., first few groups) give rise to saturated pixels. As described in some early JWST results (see, e.g., [Rustamkulov et al., 2023](https://www.nature.com/articles/s41586-022-05677-y)), in some cases one might even want to be a bit more aggressive on the level of saturation allowed in a given dataset in order to improve on the reliability of the results. As such, understanding how to modify the level of saturation allowed in a given dataset might turn out to be an important skill on real TSO data analysis. \n",
    "\n",
    "The key file that sets the limits used to call a pixel \"saturated\"  is the reference file of the `saturation` step. \n",
    "\n",
    "As discussed above, this can be seen directly on the outputs of the `saturation` step while its running, but it's also saved in our data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_results.meta.ref_file.saturation.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e277e99",
   "metadata": {},
   "source": [
    "We can actually load this reference file using the `SaturationModel` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory where reference files are stored (this was defined in the Setup section above):\n",
    "base_ref_files = os.environ[\"CRDS_PATH\"]+\"/references/jwst/niriss/\"\n",
    "\n",
    "# Read it in:\n",
    "saturation_ref_file = datamodels.SaturationModel(base_ref_files+saturation_results.meta.ref_file.saturation.name[7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a8353-d769-42ec-90eb-002cf429ac24",
   "metadata": {},
   "source": [
    "More often than not, however, the saturation reference file might not match exactly the dimensions of our subarray. This is because the reference file might be padded to match several other subarrays, and thus we have to figure out how to \"cut\" it to match our data. This is, in fact, our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb58c8a-8645-4f6b-9dc7-497d459c1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_ref_file.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebe776-8197-47d5-86ed-bcb495c328b1",
   "metadata": {},
   "source": [
    "Luckily, the JWST calibration pipeline has a handy function to transform the dimensions between instruments --- this is the `jwst.lib.reffile_utils.get_subarray_model` function, which recieves an input data model (e.g., the one from our data) along with the reference file, and spits out the same reference file model but with the right dimensions. Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e8fda-366a-4003-bb78-31b2834e6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailored_saturation_ref_file = jwst.lib.reffile_utils.get_subarray_model(saturation_results, saturation_ref_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b6fe1-b134-4037-a9da-16ed439ae789",
   "metadata": {},
   "source": [
    "Indeed, now our \"tailored\" reference file matches our science data dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de053d-6d1b-4f65-9285-85983c68b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailored_saturation_ref_file.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b6402-c7a4-4375-b012-eb8ccce668c6",
   "metadata": {},
   "source": [
    "Let's see how the saturation map looks like for our subarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a6a56-1e8f-41f0-b215-e23b6dd3f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "plt.title('Saturation map for NIS (SUBSSTRIP256 subarray)')\n",
    "im = plt.imshow(tailored_saturation_ref_file.data)\n",
    "plt.colorbar(label='Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9c2eb-cde9-49ab-b73c-d816e129131c",
   "metadata": {},
   "source": [
    "There's clearly some structure, albeit is not exactly clear what values different pixels take. To visualize this, let's print the saturation limit for pixel `(176, 1503)`, the one we explored above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585feea1-aebc-43d3-80e0-c149c6abcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel in refernce to a saturate pixel.\n",
    "tailored_saturation_ref_file.data[pixel_row, pixel_column] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c5962-9876-414a-8f07-69775000a6b0",
   "metadata": {},
   "source": [
    "If the counts surpass this limit, the pixel will be considered saturated. To see if this was the case, let's repeat the plot above marking this signal limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df1a0a-24a8-4eeb-8237-29466736d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_row, pixel_column = row_idx[60], column_idx[60] \n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.title(f'Saturated Pixel: ({pixel_row}, {pixel_column})')\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.data[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color='tomato')\n",
    "\n",
    "plt.plot([1, saturation_results.data.shape[1]+1], \n",
    "         [tailored_saturation_ref_file.data[pixel_row, pixel_column], \n",
    "          tailored_saturation_ref_file.data[pixel_row, pixel_column]],\n",
    "         'r--', \n",
    "         label='Signal limit in reference file'\n",
    "        )\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.xlabel('Group number', fontsize=16)\n",
    "plt.ylabel('Counts', fontsize=16, color='tomato')\n",
    "plt.legend()\n",
    "\n",
    "plt.twinx()\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.groupdq[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color='cornflowerblue')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.ylabel('Group Data-quality', fontsize=16, color='cornflowerblue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a911e15-b2ae-499f-85f7-fd0e803a3834",
   "metadata": {},
   "source": [
    "Indeed, this is the case! Note that, as described above, by default for NIRISS not only this pixel gets marked as saturated, but all pixels around it. To see this, note for instance the same plot as above but for of the neighboring pixels lets use pixel (177,1502):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e572279-2d2e-4754-96f3-df4bc1896211",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_row, pixel_column = 175, 1502\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "plt.title(f'Same as above, but for neighboring pixel ({pixel_row},{pixel_column})')\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.data[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color='tomato'\n",
    "         )\n",
    "\n",
    "plt.plot([1, saturation_results.data.shape[1]+1], \n",
    "         [tailored_saturation_ref_file.data[pixel_row, pixel_column], \n",
    "          tailored_saturation_ref_file.data[pixel_row, pixel_column]],\n",
    "         'r--', \n",
    "         label='Signal limit in reference file'\n",
    "        )\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.xlabel('Group number', fontsize=16)\n",
    "plt.ylabel('Counts', fontsize=16, color='tomato')\n",
    "plt.legend()\n",
    "\n",
    "plt.twinx()\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.groupdq[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color='cornflowerblue')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.ylabel('Group Data-quality', fontsize=16, color='cornflowerblue')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# make sure to find pixel that is saturate and its neighbor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a75f3-3f5a-40c4-82a2-cf66cf6ea439",
   "metadata": {},
   "source": [
    "Note how the signal level has not gone above the limit in the reference file, but it is marked as saturated because pixel (176,1503) is. Again, this is to account for possible charge spilling to the pixel.\n",
    "\n",
    "Now, what if we wanted to mark as saturated all pixels, say, larger than 50\\% these saturation values? Well, we can directly modify the reference file and repeat the calculation pointing at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e149dc-27c9-4775-8cf3-6309bb34e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_ref_file.data = saturation_ref_file.data * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a625a17-3f8d-4387-87e9-2767783dfc79",
   "metadata": {},
   "source": [
    "To incorporate this new reference file, we simply use the `override_saturation` flag, passing this new `SaturationModel` along: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91279ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run saturation step:\n",
    "saturation_results2 = calwebb_detector1.saturation_step.SaturationStep.call(uncal_nis[0], \n",
    "                                                                            override_saturation=saturation_ref_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32975e",
   "metadata": {},
   "source": [
    "Let's see how many pixels are now counted as saturated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every row and column of integration number 10, last group:\n",
    "integration, group = 10, -1\n",
    "nsaturated = 0\n",
    "\n",
    "verbose = False\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(\n",
    "            saturation_results2.groupdq[integration, group, row, column], \n",
    "            mnemonic_map=datamodels.dqflags.pixel\n",
    "            )\n",
    "        \n",
    "        # Check if pixel is saturated; if it is...\n",
    "        if 'SATURATED' in bps:\n",
    "\n",
    "            # ...print which pixel it is, and...\n",
    "            if verbose:\n",
    "                print('Pixel ({0:},{1:}) is saturated in integration 10, last group'.format(row, column))\n",
    "\n",
    "            # ...count it:\n",
    "            nsaturated += 1\n",
    "\n",
    "print('\\nA total of {0:} out of {1:} pixels ({2:.2f}%) are saturated'.format(nsaturated, \n",
    "                                                                             rows*columns, \n",
    "                                                                             100 * nsaturated / float(rows * columns)\n",
    "                                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf46ae5",
   "metadata": {},
   "source": [
    "As expected, a much bigger portion! About 2.5\\% of the pixels in the subarray are now masked (against 0.01\\% from before) as saturated thanks to our higher threshold for flagging.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Note on manually setting the saturation limit</b>: Setting the saturation limit manually should be done with care, and we recommend trying different saturation levels to check whether TSO science is impacted by this choice. In particular, we suggest to <i>never</i> set limits that are above the thresholds defined by the instrument teams, as these are typically set to levels above which the non-linearity correction (see below) is not expected to work.</div>\n",
    "\n",
    "Before moving to the next step, let's run the saturation step on the other NIS segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99ccf3-53d7-4281-b119-c331fcaed288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(nsegments):\n",
    "    uncal_nis[i] = calwebb_detector1.saturation_step.SaturationStep.call(uncal_nis[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0355b1",
   "metadata": {},
   "source": [
    "### 4.3 Removing detector-level effects: the `superbias` and `refpix` steps <a class=\"anchor\" id=\"refpix\"></a>\n",
    "\n",
    "So far, we have focused on flagging pixels for various effects (e.g., bad pixels, saturation) but we haven't worked directly with the actual counts on our data. In this Section, we deal with various (non-astrophysical) detector-level effects present in our data through two steps in the JWST Calibration pipeline: the `superbias` and the `refpix` steps. \n",
    "\n",
    "#### 4.3.1 Removing the pedestal from the detector: the `superbias` step\n",
    "\n",
    "All detectors have mostly stable, factory-defined pedestal levels, which can be closely monitored with the right calibration exposures. Indeed, instrument teams closely monitor and refine this via what is called the \"super\" bias --- the spatial shape of this pedestal. The JWST Calibration pipeline substracts this pedestal from data via the `superbias` step.\n",
    "\n",
    "Applying this correction to the data is very simple to do; let's apply it once again to the first segment of data for the NIS detector, so we can check how our data changes after applying the step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "superbias_results = calwebb_detector1.superbias_step.SuperBiasStep.call(uncal_nis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3144d5-0b11-4984-9411-cbd4e5e87819",
   "metadata": {},
   "source": [
    "Once again, we can see that there is a particular reference file being used to remove the pedestal, `jwst_niriss_superbias_0200.fits`, which can be explored in a similar way as how we explored the reference file for the `saturation` step above. Let's see how our data changed after applying this pedestal removal --- let's again take the last group of integration 10 as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True)\n",
    "\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "# Plot before step\n",
    "ax1.set_title('Before the Superbias step:')\n",
    "ax1.set_ylabel('y [pixel]')\n",
    "im1 = ax1.imshow(uncal_nis[0].data[10, -1, :, :] / np.nanmedian(uncal_nis[0].data[10, -1, :, :]))\n",
    "im1.set_clim(-3, 2)\n",
    "fig.colorbar(im1, ax=ax1, label='Normalized (to median) fluence')\n",
    "\n",
    "# Plot after step\n",
    "ax2.set_title('Before the Superbias step:')\n",
    "ax2.set_xlabel('x [pixel]')\n",
    "ax2.set_ylabel('y [pixel]')\n",
    "im2 = ax2.imshow(superbias_results.data[10, -1, :, :] / np.nanmedian(superbias_results.data[10, -1, :, :]))\n",
    "im2.set_clim(-3, 2)\n",
    "fig.colorbar(im2, ax=ax2, label='Normalized (to median) fluence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e93a6",
   "metadata": {},
   "source": [
    "Wow! That's a huge change. Overall, this looks much better and the 3rd diffraction order that was previously being suppressed by detector effect is now visible. Let's plot the profiles of pixel column index 1500 to have a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85918a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_index = 1500\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "ax1.plot(uncal_nis[0].data[10, -1, :, column_index], label='Before the Superbias step')\n",
    "ax1.plot(superbias_results.data[10, -1, :, column_index], label='After the Superbias step')\n",
    "ax1.set_xlabel('Row pixel index', fontsize=14)\n",
    "ax1.set_ylabel('Counts', fontsize=14)\n",
    "ax1.set_title('Comparison before/after Superbias step', fontsize=14)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title('Same, but median-subtracted counts', fontsize=14)\n",
    "ax2.plot(uncal_nis[0].data[10, -1, :, column_index] - np.nanmedian(uncal_nis[0].data[0,-1, :, column_index]))\n",
    "ax2.plot(superbias_results.data[10, -1, :, column_index] - np.nanmedian(superbias_results.data[0, -1, :, column_index]))\n",
    "ax2.set_xlabel('Row pixel index', fontsize=14)\n",
    "ax2.set_ylabel('Counts - Median Counts', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ba157",
   "metadata": {},
   "source": [
    "As can be seen, a ton of structure has been removed. Also, all the pixels seem to be at the same background level. This is a good sign that the Superbias correction has worked, in principle, correctly. \n",
    "\n",
    "However, if we take a more detailed look at background pixels, we can note an interesting pattern. Let's plot a similar cut to the one above, but for column 250 --- which is far away from any illuminted pixels in the detector. Let's also plot the last superbias-corrected group and the second-to-last superbias-corrected group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a1979-9c92-4dac-ae8c-62fed86fa497",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "ax1.plot(superbias_results.data[10, -1, :, 250])\n",
    "ax1.plot([0, 32], [0, 0], 'k--')\n",
    "ax1.set_xlabel('Row pixel index', fontsize=14)\n",
    "ax1.set_ylabel('Counts', fontsize = 14)\n",
    "ax1.set_title('Superbias-corrected close-up, last group, integration 10', fontsize=14)\n",
    "ax1.set_ylim(-250, 250)\n",
    "ax1.set_xlim(0, 31)\n",
    "\n",
    "ax2.set_title('Superbias-corrected close-up, second-to-last group, integration 10', fontsize=14)\n",
    "ax2.plot(superbias_results.data[10, -2, :, 250])\n",
    "ax2.plot([0, 32], [0, 0], 'k--')\n",
    "ax2.set_ylim(-250,250)\n",
    "ax2.set_xlim(0,31)\n",
    "ax2.set_xlabel('Row pixel index', fontsize=14)\n",
    "ax2.set_ylabel('Counts', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0aeed7-e0f7-4ebb-bc20-6299be052a02",
   "metadata": {},
   "source": [
    "Note how the pedestal correction the `superbias` step has, first of all, **not** brought the background down to **exactly** zero. The answer to this behavior is that there are other, group-dependant detector effects that need to be removed. These are the ones the so-called \"reference pixels\" in the detector aim at correcting for, which is done in the JWST Calibration pipeline via the `refpix` step --- the step we cover next in this Notebook.\n",
    "\n",
    "#### 4.3.2 Removing group-dependant detector effects: the `refpix` step\n",
    "\n",
    "All the JWST detectors contain reference pixels, typically located in some (or all) of the edges of the detectors. These pixels are ones for which their \"sensitivity to light\" has been deactivated, and are thus useful for tracking detector-level effects happening at the time of our observations. While all detectors have those, **not all detector subarrays** contain reference pixels. Some, like in our case, contain reference pixels only in certain portions of the subarray.\n",
    "\n",
    "Let's visualize where those reference pixels are in our subarray by using the `pixeldq` flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf15880-a891-4de9-841d-1969b107db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that will save locations of reference pixels:\n",
    "reference_pixels = np.zeros([rows, columns])\n",
    "\n",
    "# Iterate through every row and column:\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(\n",
    "            superbias_results.pixeldq[row, column], \n",
    "            mnemonic_map=datamodels.dqflags.pixel)\n",
    "\n",
    "        if 'REFERENCE_PIXEL' in bps:\n",
    "\n",
    "            reference_pixels[row, column] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576ec37-fff3-4e65-aa8d-f03cfc10e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.title('Location of reference pixels in the subarray: 4 columns/rows')\n",
    "im = plt.imshow(reference_pixels)\n",
    "\n",
    "# Arrows to indicate edges:\n",
    "plt.text(1800-70, 32, 'Right Edge Ref Pixels', color='yellow')\n",
    "plt.arrow(1780, 16, 150, -1, widt=5, head_width=10, head_length=100, color='white')\n",
    "\n",
    "plt.text(30, 32, 'Left Edge Ref Pixels', color='yellow')\n",
    "plt.arrow(268, 16, -150, -1, width=5, head_width=10, head_length=100, color='white')\n",
    "\n",
    "# plot vertical error top reference pixels\n",
    "plt.text(1044, 170, 'Top Ref Pixels', color='yellow', rotation=90)\n",
    "plt.arrow(1024, 180, -0,150//4, width=5*2, head_width=10*2, head_length=100/2/2, color='white')\n",
    "\n",
    "im.set_clim(-0.5, 0.5)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183faf7-693b-4ca0-97da-321d9703012a",
   "metadata": {},
   "source": [
    "Note the white/yellow edges to the left and the right of the plot above, indicated by arrows, show the location of the reference pixels for our subarray. In other words, our subarray has reference pixels at the top of the frame andto the left and right-most sides, but not the bottom part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3e82f",
   "metadata": {},
   "source": [
    "Let's apply the `refpix` step to check how our data looks like after it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad093d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "refpix_results = calwebb_detector1.refpix_step.RefPixStep.call(superbias_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b29e8b-c0c9-4768-ae26-2ed24f92ab66",
   "metadata": {},
   "source": [
    "Let's plot the before and after applying this step:\n",
    "\n",
    "<!-- Let's plot once again the figures above. First, a vertical cut of the profile at pixel column index 250: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 9), sharex=True, sharey=True)\n",
    "\n",
    "ax1, ax2, ax3 = axes.ravel()\n",
    "\n",
    "# Plot before step\n",
    "ax1.set_title('Before the RefPix step:')\n",
    "ax1.set_ylabel('y [pixel]')\n",
    "im1 = ax1.imshow(superbias_results.data[0, -1, :, :] / np.nanmedian(superbias_results.data[0, -1, :, :]))\n",
    "im1.set_clim(-2, 2)\n",
    "fig.colorbar(im1, ax=ax1, label='Normalized (to median) fluence')\n",
    "\n",
    "# Plot after step\n",
    "ax2.set_title('Before the RefPix step:')\n",
    "ax2.set_xlabel('x [pixel]')\n",
    "ax2.set_ylabel('y [pixel]')\n",
    "im2 = ax2.imshow(refpix_results.data[0, -1, :, :] / np.nanmedian(refpix_results.data[0, -1, :, :]))\n",
    "im2.set_clim(-2, 2)\n",
    "fig.colorbar(im2, ax=ax2, label='Normalized (to median) fluence')\n",
    "\n",
    "# Plot difference\n",
    "ax3.set_title('Difference:')\n",
    "ax3.set_xlabel('x [pixel]')\n",
    "ax3.set_ylabel('y [pixel]')\n",
    "im3 = ax3.imshow(superbias_results.data[0, -1, :, :] - refpix_results.data[0, -1, :, :])\n",
    "fig.colorbar(im3, ax=ax3, label='Normalized (to median) fluence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33988dc",
   "metadata": {},
   "source": [
    "That looks much better. Several things have been removed, including some interesting high-frequency noise happening in the rows as show in the difference between the before and after. That's the so-called odd-even effect, that the refpix step takes care of efficiently thanks to reference pixels (pixels insenstive to light) in the detector.\n",
    "\n",
    "As can be seen, most of the detector structure is taken care of up to this point. The backgrounds are now nicely suited slightly above zero, as they should; most detector effects are gone and the group looks much cleaner. It is very instructive to do this kind of visual checks on real data, as they can significantly impact the final achieved S/N if not properly accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "ax1.plot(superbias_results.data[0, -1, :, column_index], label='Before the RefPix step')\n",
    "ax1.plot(refpix_results.data[0, -1, :, column_index], label='After the RefPix step')\n",
    "ax1.set_xlabel('Row pixel index', fontsize=14)\n",
    "ax1.set_ylabel('Counts', fontsize=14)\n",
    "ax1.set_title('Comparison before/after RefPix step', fontsize=14)\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.set_title('Same, but median-substracted counts', fontsize=14)\n",
    "ax2.plot(superbias_results.data[0, -1, :, column_index] - np.nanmedian(superbias_results.data[0, -1, :, column_index]))\n",
    "ax2.plot(refpix_results.data[0, -1, :, column_index] - np.nanmedian(refpix_results.data[0, -1, :, column_index]))\n",
    "ax2.set_xlabel('Row pixel index', fontsize=14)\n",
    "ax2.set_ylabel('Counts - Median Counts', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf297f-1f61-4c05-a6c6-d00909136f3c",
   "metadata": {},
   "source": [
    "We can see there's been a minor improvement in the results. \n",
    "\n",
    "It is interesting to note that the \"banding\" on the columns, as discussed above, has not dissapeared. This is more evident when plotting a series of groups from different integrations; let's plots the groups from integrations 10, 11 and 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e32b13-0592-4953-a516-8fefbf7975e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration 10, last group\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.title('Integration 10, last group')\n",
    "im = plt.imshow(refpix_results.data[10, -1, :, :])\n",
    "im.set_clim(-200, 400)\n",
    "plt.colorbar(label='Counts')\n",
    "\n",
    "# Integration 10, last group\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.title('Integration 11, last group')\n",
    "im = plt.imshow(refpix_results.data[11, -1, :, :])\n",
    "im.set_clim(-200, 400)\n",
    "plt.colorbar(label='Counts')\n",
    "\n",
    "# Integration 12, last group\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.title('Integration 12, last group')\n",
    "im = plt.imshow(refpix_results.data[12, -1, :, :])\n",
    "im.set_clim(-200, 400)\n",
    "plt.colorbar(label='Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837a984-6b6e-4583-9077-1a5456a480c3",
   "metadata": {},
   "source": [
    "This is, once again, expected as there are no reference pixels in the columns. We will explore how to correct this after going with the `linearity` correction/step, which we discuss next. Before moving on, we apply the superbias and reference pixel step to both detectors, all segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae102d5-d78c-42c8-bf43-dc54082cbafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(nsegments):\n",
    "    # Apply superbias and refpix to all NIS segments:\n",
    "    uncal_nis[i] = calwebb_detector1.superbias_step.SuperBiasStep.call(uncal_nis[i])\n",
    "    uncal_nis[i] = calwebb_detector1.refpix_step.RefPixStep.call(uncal_nis[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b2f7d",
   "metadata": {},
   "source": [
    "### 4.4 Linearity corrections <a class=\"anchor\" id=\"linearity\"></a>\n",
    "\n",
    "As a pixel accumulates charge, it becomes less and less efficient at generating charge-carriers and/or holding that charge in place. A consequence of this is that the raw -uncalibrated- up-the-ramp samples in JWST detectors are non-linear, with the pixels at lower fluences being almost linear and pixels near the saturation ranges deviating significantly from this behavior. This is the behavior that the `linearity` step in the JWST Calibration pipeline aims to fix.\n",
    "\n",
    "#### 4.4.1 Visualizing and correcting for non-linearity with the `linearity` step\n",
    "\n",
    "To visualize the non-linearity of the up-the-ramp samples, let's take a look at the samples of one of the brightest pixels in our subarray, pixel `(45, 1600)` --- say for integration number 10. Let's plot on top a line fitted to the first 10 pixels, which should be the most \"linear\" of all pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c49b7-f880-4ce9-8afd-a979ca0230bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngroups = uncal_nis[0].data.shape[1]\n",
    "group = np.arange(ngroups) + 1\n",
    "\n",
    "first_groups = 9\n",
    "i1, i2 = 45, 1600\n",
    "\n",
    "coeff = np.polyfit(group[:first_groups], uncal_nis[0].data[10, :first_groups, i1, i2], 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 6), constrained_layout=True)\n",
    "\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "# plot ramp samples\n",
    "ax1.set_title(f'Up-the-ramp sample, integration 10, pixel ({i1}, {i2})')\n",
    "ax1.plot(group, uncal_nis[0].data[10, :, i1, i2], 'o-', color='black', mfc='white', label='Up-the-ramp samples')\n",
    "ax1.plot(group, np.polyval(coeff, group), 'r--', label='Linear fit to first ' + str(first_groups) + ' groups')\n",
    "ax1.set_xlabel('Group number', fontsize=16)\n",
    "ax1.set_ylabel('Counts', fontsize=16)\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0.5, 9.5)\n",
    "\n",
    "# plot residuals\n",
    "ax2.plot(group, uncal_nis[0].data[10, :, i1, i2] - np.polyval(coeff, group), 'o-')\n",
    "ax2.set_xlabel('Group number', fontsize=16)\n",
    "ax2.set_ylabel(\"residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d261731-04de-466c-8f0e-e9dea55af661",
   "metadata": {},
   "source": [
    "Ah --- the ramp is _clearly_ non-linear! Let's apply the `linearity` step to the very first segment to see how well this gets corrected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099e9a1-4949-4eab-870b-3cb87415aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linearity step:\n",
    "linearity_results = calwebb_detector1.linearity_step.LinearityStep.call(uncal_nis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882aaf3-0f2d-492b-9a97-07e06ca44d72",
   "metadata": {},
   "source": [
    "Let's try the same plot as above, but with the linearity-corrected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa85745-3136-4b67-bc39-c727149c459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngroups = uncal_nis[0].data.shape[1]\n",
    "group = np.arange(ngroups) + 1\n",
    "\n",
    "first_groups = 9\n",
    "\n",
    "coeff = np.polyfit(group[:first_groups],  linearity_results.data[10, :first_groups, i1, i2], 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 6), constrained_layout=True)\n",
    "\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "# plot ramp samples\n",
    "ax1.set_title(f'Same as above, linearity-corrected')\n",
    "ax1.plot(group, linearity_results.data[10, :, i1, i2], 'o-', color='black', mfc='white', label='Up-the-ramp samples')\n",
    "ax1.plot(group, np.polyval(coeff, group), 'r--', label='Linear fit to first ' + str(first_groups) + ' groups')\n",
    "ax1.set_xlabel('Group number', fontsize=16)\n",
    "ax1.set_ylabel('Counts', fontsize=16)\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0.5, 9.5)\n",
    "\n",
    "# plot residuals\n",
    "ax2.plot(group, linearity_results.data[10, :, i1, i2] - np.polyval(coeff, group), 'o-')\n",
    "ax2.set_xlabel('Group number', fontsize=16)\n",
    "ax2.set_ylabel(\"residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8a4da-90ef-4581-a06e-f8a8395e334a",
   "metadata": {},
   "source": [
    "Ah, much better, notice how the residuals are have improve and are much closer to zero. \n",
    "\n",
    "#### 4.4.2 Testing the accuracy of the `linearity` step\n",
    "\n",
    "It is important to realize that the linearity corrections that the JWST Calibration pipeline applies through the `linearity` step are _not_ perfect. While this is difficult to see with a single integration, this can be studied with multiple integrations --- which helps us beat the noise embedded on single up-the-ramp samples. \n",
    "\n",
    "One trick to glance at how the linearity of the up-the-ramp samples evolves as one goes up-the-ramp is to note that if the detector is linear, it doesn't matter at which up-the-ramp sample one looks at, the **fluence level should change from group-to-group at _the same rate_ on average**. So one can quickly investigate if linearity is an issue (and if the pipeline is correctly correcting for it) by:\n",
    "\n",
    "1. Taking the difference in fluence between two subsequent groups (say, the last two).\n",
    "2. Taking the difference in fluence between two _other_ subsequent groups (say, the first two).\n",
    "3. Take the ratio between those differences.\n",
    "\n",
    "If the detector is linear, then all the pixels should fall around a ratio of 1. Do they? Let's try this experiment out. Let's first take the difference of the last two and first two groups for all the pixels of all the integrations of the **uncorrected** data --- then take the ratio of those. As we saw above, this should scream \"non-linearity\" all over!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a175321",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pair = uncal_nis[0].data[:, -1, :, :] - uncal_nis[0].data[:, -2, :, :]\n",
    "first_pair = uncal_nis[0].data[:, 1, :, :] - uncal_nis[0].data[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a060399",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = last_pair / first_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ec70a",
   "metadata": {},
   "source": [
    "Let's now flatten those arrays and plot them as a function of total fluence at the very last group. If linearity weren't an issue, all of these should line around 1 (this may take a few seconds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_ratio = ratio.flatten()\n",
    "flattened_fluences = uncal_nis[0].data[:, -1, :, :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53df40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "plt.plot(flattened_fluences, flattened_ratio, '.', alpha=0.01, color='black')\n",
    "plt.plot([0,35000], [1., 1.], 'r--')\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.xlim(0, 35000)\n",
    "plt.xlabel('Fluence at the last group (counts)', fontsize=14)\n",
    "plt.ylabel('(Last / First) Group differences', fontsize=14)\n",
    "plt.title('No linearity correction', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcd297",
   "metadata": {},
   "source": [
    "Indeed, the data does _not_ line up around 1. So linearity _is_ an issue the larger the flux received (as we already observed in the up-the ramp samples before)!\n",
    "\n",
    "All right, let's try the same experiment but now on the linearity-corrected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8002de",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_last_pair = linearity_results.data[:, -1, :, :] - linearity_results.data[:, -2, :, :]\n",
    "corrected_first_pair = linearity_results.data[:, 1, :, :] - linearity_results.data[:, 0, :, :]\n",
    "corrected_ratio = corrected_last_pair / corrected_first_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041b82e",
   "metadata": {},
   "source": [
    "Let's plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08942e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_corrected_ratio = corrected_ratio.flatten()\n",
    "flattened_corrected_fluences = linearity_results.data[:, -1, :, :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f30a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "plt.plot(flattened_corrected_fluences, flattened_corrected_ratio, '.', alpha=0.005, color='black')\n",
    "plt.plot([0, 35000], [1., 1.], 'r--')\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.xlim(0, 35000)\n",
    "plt.xlabel('Fluence at the last group (counts)', fontsize=14)\n",
    "plt.ylabel('(Last / First) Group differences', fontsize=14)\n",
    "plt.title('After linearity correction', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896c9fd-da47-4def-9882-62cde592a8bb",
   "metadata": {},
   "source": [
    "That looks **much** better. Note, however, that as discussed above the corrections are *not* perfect. In particular, below about 20,000 counts it seems the correction makes the last group difference to be slightly larger than the first group differences; this changes for the larger fluences, where the last group difference seems to have a _smaller_ flux than the first group differences. This is actually consistent with a _charge migration_ hypothesis, on which pixels that receive larger fluences _lose_ charge to neighboring pixels that receive them. Testing this hypothesis is, of course, outside of the present Notebook --- but this showcases that plots like the ones above are fundamental to make sense of data and the overall accuracy and precision of non-linearity corrections.\n",
    "\n",
    "Before moving to the next step, we apply the `linearity` step to all our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95974a7f-25d9-41fe-8a3d-b02cb8a58ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(nsegments):\n",
    "    # Apply the linearity step to NIS segments:\n",
    "    uncal_nis[i] = calwebb_detector1.linearity_step.LinearityStep.call(uncal_nis[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91161c62",
   "metadata": {},
   "source": [
    "### 4.5 Removing the Dark Current <a class=\"anchor\" id=\"dark-current\"></a>\n",
    "\n",
    "One of the last steps before the most computationally expensive steps in the pipeline is the `dark_current` step. This step grabs a reference file that calculates the dark current at each group, and applies the same correction to every integration in the same way. \n",
    "\n",
    "It is unclear if this step is helpful at all for TSOs, where signals are typically high (and thus, the dark current is but a very small addition to the total current gathered in a TSO), but we go ahead and apply this step nonetheless in our data. First, to check what changes this step does in our data, we apply it on the first NIS segment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41eade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the darkcurrent step:\n",
    "darkcurrent_results = calwebb_detector1.dark_current_step.DarkCurrentStep.call(uncal_nis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efd9f",
   "metadata": {},
   "source": [
    "Let's see its impact on products before the dark current correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e46eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them:\n",
    "plt.figure(figsize=(10, 3))\n",
    "im = plt.imshow(uncal_nis[0].data[10, -1, :, :] / np.nanmedian(uncal_nis[0].data[10, -1, :, :]))\n",
    "im.set_clim(-3, 2)\n",
    "plt.colorbar(label='Normalized (to median) fluence')\n",
    "plt.title('Before the DarkCurrent step:')\n",
    "\n",
    "# Plot them:\n",
    "plt.figure(figsize=(10, 3))\n",
    "im = plt.imshow(darkcurrent_results.data[10, -1, :, :] / np.nanmedian(darkcurrent_results.data[10, -1, :, :]))\n",
    "im.set_clim(-3, 2)\n",
    "plt.title('After the DarkCurrent step:')\n",
    "plt.colorbar(label='Normalized (to median) fluence')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f349d",
   "metadata": {},
   "source": [
    "Difficult to see the impact from those simple plots. \n",
    "\n",
    "Let's quantify \"how much\" dark current there is by simply calculating the average (accross integrations) percentage of dark current on the last group. The reason for doing this in the last group is that this is the group that accumulates _the most_ dark current --- as dark current grows as a function of the number of groups. \n",
    "\n",
    "To do this, we consider that for a non-dark current corrected signal $S_{DC}$, if we substract the dark-current corrected signal $S_{DC, corrected}$ we get the dark current signal back, i.e., $S_{DC} - S_{DC, corrected} = DC$; dividing this by the dark-current corrected signal gives us the percentage of dark current signal on each pixel. Let's calculate a map of this for all integrations and take the median of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc5d9d-e417-4fc3-a483-4b0fe89e3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (average) percent change of the signal accross all integrations --- this is (Dark Signal) / (\"Real\" signal):\n",
    "percent = ((linearity_results.data[:, -1, :, :] - darkcurrent_results.data[:, -1, :, :]) / \n",
    "           darkcurrent_results.data[:, -1, :, :]) * 100\n",
    "\n",
    "percent = np.nanmedian(percent, axis=0)\n",
    "\n",
    "# Plot --- minimum and maximum are bounded to about 20:\n",
    "plt.figure(figsize=(10, 3))\n",
    "im = plt.imshow(percent)\n",
    "im.set_clim(0, 25)\n",
    "plt.colorbar(label='% of Dark Signal')\n",
    "plt.title('Median impact of dark signal on the last group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab230130-64be-4014-9ec9-7e9e92e04622",
   "metadata": {},
   "source": [
    "All right, so there _is_ an impact on the order of ~3-7% for the last group, at least on the left-hand side of the detector where there is not a lot of signal --- i.e., left-most of pixel column 500. Right-most of this, it seems the impact is very low, of order ~5\\% in background pixels (i.e., close to the upper and lower edges) and even lower in the location of the spectra itself --- less than ~0.03\\% at the peak signal level.\n",
    "\n",
    "How much the above impacts a given TSO must be defined on a target-by-target basis. In the worst-case scenarios, this impact might not be purely aesthetical --- this dark current can give rise to transit depth dilutions in transiting exoplanet science, for instance; just as any non-accounted background signal.\n",
    "\n",
    "In the case of this notebook, we apply it nonetheless to all the detector-level data in all segments, but we leave as an excercise to the reader to perform a full re-reduction with and without dark-current to see the impact of this step on this particular dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d1694-ccfe-478e-a7b4-a8861ad18028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(nsegments):\n",
    "    # Apply the dark_current step to NIS segments:\n",
    "    uncal_nis[i] = calwebb_detector1.dark_current_step.DarkCurrentStep.call(uncal_nis[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc98259-ac61-4985-956d-7de273357d79",
   "metadata": {},
   "source": [
    "### 4.6 Correcting 1/f noise <a class=\"anchor\" id=\"one_over_f\"></a>\n",
    "\n",
    "For TSOs, there is some discussion in the literature about whether attempting to remove 1/f noise at the group-level, at the rate-level (i.e., after all the steps in <code>detector1</code>) or both is the way to go --- and whether simplistic algorithms provide a quick means of removed this source of noise. The reality is that, at the time of writing, the jury is still out on the final answer. We thus encourage readers to try different methodologies and find the one that works best for their scientific use-case. As a start, an interesting reader might, e.g., skip the above 1/f removal algorithm and simply try to remove it at the rate-level --- or perform no removal at all, and see differences in the final lightcurve precision.\n",
    "\n",
    "<!-- <div class=\"alert alert-block alert-info\"> <b>Note on 1/f correction methods</b>: For TSOs, there is some discussion in the literature about whether attempting to remove 1/f noise at the group-level, at the rate-level (i.e., after all the steps in <code>detector1</code>) or both is the way to go --- and whether simplistic algorithms provide a quick means of removed this source of noise. The reality is that, at the time of writing, the jury is still out on the final answer. We thus encourage readers to try different methodologies and find the one that works best for their scientific use-case. As a start, an interesting reader might, e.g., skip the above 1/f removal algorithm and simply try to remove it at the rate-level --- or perform no removal at all, and see differences in the final lightcurve precision. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cafb8",
   "metadata": {},
   "source": [
    "### 4.7 Detecting \"jumps\" in up-the-ramp samples <a class=\"anchor\" id=\"jump\"></a>\n",
    "\n",
    "When a cosmic-ray hits JWST detectors, this impacts the up-the-ramp samples by making them \"[jump](https://www.youtube.com/watch?v=SwYN7mTi6HM)\" from one group to another. We already noted this happening above \n",
    "[when we discussed saturation](#saturation) --- a pixel was suddenly pushed above the saturation limit and the `saturation` step flagged the pixel. However, some other jumps are not as dramatic, and the data after the jump might actually be as usable as data before the jump.\n",
    "\n",
    "\n",
    "#### 4.7.1 Understanding jumps and the `jump` step\n",
    "\n",
    "To exemplify the behavior of the jumps in up-the-ramp samples, let's look at an example. Consider the behavior of pixel index `(12,1000)` in integration `67`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jump_results = calwebb_detector1.jump_step.JumpStep.call(refpix_results, maximum_cores='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(uncal_nis[0].data[67, -1] - uncal_nis[0].data[68, -1], vmin=-100, vmax=100)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bf4de-99d0-470c-a1b7-d21fabac9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# possible examples\n",
    "# column_index = 998\n",
    "# row_index = 8\n",
    "\n",
    "column_index = 213\n",
    "row_index = 182\n",
    "\n",
    "plt.title(f'Pixel index ({row_index}, {column_index})')\n",
    "\n",
    "group = np.arange(uncal_nis[0].data.shape[1])\n",
    "plt.plot(group+1, uncal_nis[0].data[67, :, row_index, column_index], 'o-', \n",
    "         color='black', mfc='white', label='Integration 67')\n",
    "plt.plot(group+1, uncal_nis[0].data[66, :, row_index, column_index], 'o-', \n",
    "         color='tomato', mfc='white', label='Integration 66', alpha=0.5)\n",
    "plt.plot(group+1, uncal_nis[0].data[68, :, row_index, column_index], 'o-', \n",
    "         color='cornflowerblue', mfc='white', label='Integration 68', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Group number', fontsize=16)\n",
    "plt.ylabel('Counts', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bde0f8-c2ad-4e3c-ac64-7805f8083ac6",
   "metadata": {},
   "source": [
    "While the intercept of the different up-the-ramp samples is slightly different, the _slope_ (i.e., the count-rate) of it is fairly similar for integrations 66, 67 and 68. However, integration 67 shows a clear jump at group 4, likely from a cosmic ray. Let's take a look at what happened in this integration and group in the 2D spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddc0da-b2af-45b7-9974-fa3272201053",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_group = 3\n",
    "i_integration = 66\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "im = plt.imshow(uncal_nis[0].data[i_integration, i_group, :, :])\n",
    "im.set_clim(-100, 1000)\n",
    "plt.xlim(column_index-5, column_index+5)\n",
    "plt.ylim(row_index-5, row_index+5)\n",
    "plt.title('Integration 66, group 15')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "im = plt.imshow(uncal_nis[0].data[i_integration+1, i_group, :, :])\n",
    "im.set_clim(-100, 1000)\n",
    "plt.xlim(column_index-5, column_index+5)\n",
    "plt.ylim(row_index-5, row_index+5)\n",
    "plt.title('Integration 67, group 15')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "im = plt.imshow(uncal_nis[0].data[i_integration+2, i_group, :, :])\n",
    "im.set_clim(-100, 1000)\n",
    "plt.xlim(column_index-5, column_index+5)\n",
    "plt.ylim(row_index-5, row_index+5)\n",
    "plt.title('Integration 68, group 15')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824c640-e5a7-4292-a5c3-d53bb1ec0ff1",
   "metadata": {},
   "source": [
    "Ah! Clearly some cosmic ray hitting around pixel `(182, 213)`, with an area of about a pixel --- including pixel `(182, 213)`. Note that the `groupdq` doesn't show anything unusual so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0].groupdq[67, -1, row_index, column_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b1c74",
   "metadata": {},
   "source": [
    "The JWST Calibration pipeline has an algorithm that aims to detect those jumps --- and is appropriately named the `jump` step. An important consideration when running the `jump` step is that one can use multiprocessing to run the step. This can offer dramatic speed improvements when running the step, in particular on large subarrays of data. The number of cores to use can be defined by the `maximum_cores` parameter, which can be an integer number or `all`, which will use all available cores. \n",
    "\n",
    "Let's run the step using all cores (this step does take some time ~4 mins):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eea42c-1165-42dd-a46f-104fe3a40ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nsegments):\n",
    "    uncal_nis[i] = calwebb_detector1.jump_step.JumpStep.call(uncal_nis[i], maximum_cores='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d989a3-62f1-4ab2-ba3d-73146bea5396",
   "metadata": {},
   "source": [
    "It's not too obvious from the messages in the pipeline what happened, but the algorithm was used to _detect_ jumps, and these are added as new data-quality flags in the `groupdq`. Let's see what happened with the pixel identified by eye above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0].groupdq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341559a9-3240-4f14-ab87-1d7e72427971",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0].groupdq[67, -1, row_index, column_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64bf86",
   "metadata": {},
   "source": [
    "Aha! It changed. What does this mean? Let's repeat the trick we learned with the `saturation` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodels.dqflags.dqflags_to_mnemonics(\n",
    "    uncal_nis[0].groupdq[67, -1, row_index, column_index], \n",
    "    mnemonic_map=datamodels.dqflags.pixel\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a487a84",
   "metadata": {},
   "source": [
    "Nice! We now have a flag that identifies when a jump detection happened. \n",
    "\n",
    "#### 4.7.2 Jump rates per integration\n",
    "\n",
    "For fun, let's use the `groupdq` changes to figure out how many jumps happened per integration on this first segment of data by simple differencing with the products from the previous step, the `dark_current` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc19d0-c707-42c0-9873-97cb724dfca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that will store the number of jumps per integration:\n",
    "njumps = np.zeros(uncal_nis[0].groupdq.shape[0])\n",
    "\n",
    "# Iterate through integrations counting how many pixels changed in all groups:\n",
    "for integration in range(uncal_nis[0].groupdq.shape[0]):\n",
    "\n",
    "    groupdq_difference = uncal_nis[0].groupdq[integration, :, :, :] - darkcurrent_results.groupdq[integration, :, :, :]\n",
    "    wherejumps = np.where(groupdq_difference != 0.)\n",
    "    njumps[integration] = len(wherejumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b238f93-18ae-4453-9e8b-9962e87cb075",
   "metadata": {},
   "source": [
    "Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrations = np.arange(uncal_nis[0].groupdq.shape[0]) + 1\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.title('Number of jumps on the first segment of data for NIS')\n",
    "plt.plot(integrations, njumps, 'o-', color='black', mfc='white' )\n",
    "plt.xlabel('Integration', fontsize=16)\n",
    "plt.ylabel('Number of jumps', fontsize=16)\n",
    "plt.xlim(0.5, uncal_nis[0].groupdq.shape[0] + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b104d5-0b46-4ddb-a0ec-afa10717004f",
   "metadata": {},
   "source": [
    "Very interesting! Per integration, it seems on the order of ~3,500 average jumps are detected. Each integration has (ngroups) x (number of pixels) =  70 x 32 x 2048 = 4587520 opportunities for jumps to appear, so this means an average rate of (detected events) / (total opportunities) = 0.07% per integration for this particular segment, detector and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a6cf0-3433-40dc-90b7-b579ba115f10",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note on the effectiveness of the <code>jump</code> detection step</b>: The <code>jump</code> detection step uses, by default, <a href=\"https://jwst-pipeline.readthedocs.io/en/latest/jwst/jump/description.html#multiprocessing\">a two-point difference method</a> that relies on appropriate knowledge of the read-noise of the detector. In some cases, this might be significantly off (or <code>detector1</code> corrections might not be optimal as to leave significant detector effects) such that the algorithm might be shown to be too aggressive. Similarly, the algorithm relies on a decent amount of groups in the integration to work properly (larger than about 5). It is, thus, important to try different parameters to identify jumps in a given dataset and study their impact on the final products. One of the most important parameters is the <code>rejection_threshold</code>. The default value is <code>4</code>, but TSO studies in the literature have sometimes opted for more conservative values (typically larger than 10). For this particular dataset, which has a large number of groups (70), the default value works well, but it might not be optimal nor be the best for other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afc925-f0b2-49dc-80af-ec9ef301f5a8",
   "metadata": {},
   "source": [
    "Before moving to the next step, we showcase one additional function from the `datamodels` which allows to save products to files --- the `save` function. This step is optional, if you want to use the `jump` step products for later use, uncomment the lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2785e5-b7fc-4301-9985-d7fd67153495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this line to run \n",
    "# if not os.path.exists('nis_jumpstep_seg001.fits'):\n",
    "#     nsegments = 3\n",
    "#     for i in range(nsegments):\n",
    "#         # Save jump step is products:\n",
    "#         uncal_nis[i].save('nis_jumpstep_seg00' + str(i+1) + '.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea1fad",
   "metadata": {},
   "source": [
    "### 4.8 Fitting ramps with the `ramp_fit` step <a class=\"anchor\" id=\"rampfit\"></a>\n",
    "\n",
    "The last step of `detector1` is the `ramp_fit` step. This step does something that might _appear_ to be quite simple, but that in reality it's not as trivial as it seems to be: fit a line and get the associated uncertainties to the up-the-ramp samples. The reason why this is not straightforward to do is because samples up-the-ramp are correlated. That is, because signal is accumulated up-the-ramp, group number 2 has a non-zero covariance with group number 1, and so on. \n",
    "\n",
    "In addition, we will save the results to this step to a desired `output_dir` location and used in the next notebook where we'll generate some Lightcurvs.\n",
    "\n",
    "#### 4.8.1 Applying the `ramp_fit` step to JWST data\n",
    "\n",
    "The JWST Calibration pipeline algorithm performs a sensible weighting of each group in order to account for that correlation when fitting a slope on the samples. Let's run this step, and save the products in files as we go, so we can use them for the next notebook. Note that as in the `jump` step, we can also run this step via multi-processing --- and we do just that below (if not ran already):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d70ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the desired out put directory\n",
    "output_dir = \"data/calibrated\"\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "for i in range(nsegments):\n",
    "    uncal_nis[i] = calwebb_detector1.ramp_fit_step.RampFitStep.call(\n",
    "        uncal_nis[i], \n",
    "        maximum_cores='all', \n",
    "        save_results=True, \n",
    "        output_dir=output_dir, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96724249",
   "metadata": {},
   "source": [
    "All right, note the products of this step for TSO's are actually a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uncal_nis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fe870",
   "metadata": {},
   "source": [
    "The data associated with the zeroth element of this list (`ramps_nis1[0][0].data`) has dimensions equal to the size of the frames (rows and columns). The first element (`ramps_nis1[0][1].data`), has three dimensions, the same as the zeroth but for each integration. We usually refer to this latter product as the `rateints` product --- i.e., the rates per integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0][0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0][1].data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae6b9d",
   "metadata": {},
   "source": [
    "To familiarize ourselves with these products, let's plot the rates of the 10th integration for NIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('NIS data; rates for integration 10')\n",
    "im = plt.imshow(uncal_nis[0][1].data[10, :, :])\n",
    "im.set_clim(-1, 10)\n",
    "plt.colorbar(label='Counts/s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7fcc50-a114-4c2d-a487-a9f5e493b912",
   "metadata": {},
   "source": [
    "In case you were unsure of the units in the colorbar, you can double-check them through the `datamodels` themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e8653-3c4c-41ad-9602-fdb034f0db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nis[0][1].search('unit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998289d",
   "metadata": {},
   "source": [
    "These rates look very pretty, lets check the first element results for the 10th inegration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "plt.title('NIS data; rates for integration 10')\n",
    "im = plt.imshow(uncal_nis[0][1].data[10, :, :])\n",
    "im.set_clim(-1, 30)\n",
    "plt.colorbar(label='Counts/s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22644d",
   "metadata": {},
   "source": [
    "These rates look _very_ good as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c6b40",
   "metadata": {},
   "source": [
    "## 5. Final words <a class=\"anchor\" id=\"final-words\"></a>\n",
    "\n",
    "This completes this notebook where we have reduced and calibrated NIRISS/SOSS data of WASP-39b from program 1366 using STAGE1 of the JWST pipeline. In the next notebook, `02_niriss_soss_spec2_generate_lightcurves`, we will use the calibrated data products to extract the spectra of WASP-39b and generate some lightcurves performing similiar steps to what is done in STAGE 2 of the JWST pipeline. I would like to thank the JWST NIRISS team, especially NÃ©stor Espinoza and Aarynn Carter for their feedback and support For this particular effort of writing these NIRISS/SOSS demonstration notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
