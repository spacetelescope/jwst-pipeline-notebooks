{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095a295c",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393e357-9d9d-4516-b28d-4d335fad33a0",
   "metadata": {},
   "source": [
    "# NIRISS SOSS Pipeline Notebook\n",
    "\n",
    "**Authors**: R. Cooper, A. Carter, N. Espinoza, T. Baines<br>\n",
    "**Last Updated**: November 7, 2025<br>\n",
    "**Pipeline Version**: 1.20.2 (Build 12.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da2029f",
   "metadata": {},
   "source": [
    "**Purpose**:<br>\n",
    "This notebook provides a framework for processing Near-Infrared\n",
    "Imager and Slitless Spectrograph (NIRISS) Single Object Slitless Spectrograph (SOSS) data through all\n",
    "three James Webb Space Telescope (JWST) pipeline stages.  Data is assumed\n",
    "to be located in one observation folder according to paths set up below.\n",
    "It should not be necessary to edit any cells other than in the\n",
    "[Configuration](#1.-Configuration) section unless modifying the standard\n",
    "pipeline processing options.\n",
    "\n",
    "**Data**:<br>\n",
    "This notebook uses an example dataset from Early Release Observation (ER0) [Program\n",
    "2734](https://www.stsci.edu/jwst/science-execution/program-information) (PI: K. Pontoppidan).\n",
    "This program consists of time series observations (TSO) of confirmed exoplanets HAT-P-18b and WASP-96b,\n",
    "intended to demonstrate the power and precision of the JWST TSO modes. In this notebook, we will reduce the\n",
    "NIRISS SOSS observations of transiting exoplanet WASP-96b.\n",
    "\n",
    "Example input data to use will be downloaded automatically unless\n",
    "disabled (i.e., to use local files instead).\n",
    "\n",
    "**JWST pipeline version and CRDS context**:<br>\n",
    "This notebook was written for the above-specified pipeline version and associated\n",
    "build context for this version of the JWST Calibration Pipeline. Information about\n",
    "this and other contexts can be found in the JWST Calibration Reference Data System\n",
    "(CRDS [server](https://jwst-crds.stsci.edu/)). If you use different pipeline versions,\n",
    "please refer to the table [here](https://jwst-crds.stsci.edu/display_build_contexts/)\n",
    "to determine what context to use. To learn more about the differences for the pipeline,\n",
    "read the relevant \n",
    "[documentation](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/jwst-operations-pipeline-build-information#references).<br>\n",
    "\n",
    "Please note that pipeline software development is a continuous process, so results\n",
    "in some cases may be slightly different if a subsequent version is used. **For optimal\n",
    "results, users are strongly encouraged to reprocess their data using the most recent\n",
    "pipeline version and\n",
    "[associated CRDS context](https://jwst-crds.stsci.edu/display_build_contexts/),\n",
    "taking advantage of bug fixes and algorithm improvements.**\n",
    "Any [known issues](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/niriss-known-issues/niriss-soss-known-issues) for this build are noted in the notebook.<BR>\n",
    "\n",
    "**Visualization**:<br>\n",
    "    \n",
    "This notebook uses the `batman' package (Pypi batman-package) for analysis of the SOSS data.\n",
    "Some versions of this package may be incompatible with certain python versions and CPU architectures.\n",
    "If issues are encountered with this package it can be disabled in the 'Package Imports' section below at\n",
    "the cost of being unable to view a few of the final data product visualization plots.\n",
    "    \n",
    "**Updates**:<br>\n",
    "This notebook is regularly updated as improvements are made to the\n",
    "pipeline. Find the most up to date version of this notebook at:\n",
    "https://github.com/spacetelescope/jwst-pipeline-notebooks/\n",
    "\n",
    "**Recent Changes**:<br>\n",
    "November 07, 2025: original notebook released<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453945c7",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291c2f1",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "1. [Configuration](#1.-Configuration) \n",
    "2. [Package Imports](#2.-Package-Imports)\n",
    "3. [Demo Mode Setup (ignore if not using demo data)](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "4. [Directory Setup](#4.-Directory-Setup)\n",
    "5. [Detector 1 Pipeline](#5.-Detector1-Pipeline)\n",
    "6. [Spec2 Pipeline](#6.-Spec2-Pipeline)\n",
    "7. [TSO3 Pipeline](#7.-TSO3-Pipeline)\n",
    "8. [Visualize the data](#9.-Visualize-the-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a88584",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd07d7",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a649d-a0d2-4e3f-ab6d-cf8c26e6ce1d",
   "metadata": {},
   "source": [
    "#### Install dependencies and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6c02c-a7cd-4999-96a0-87ce398d7e9a",
   "metadata": {},
   "source": [
    "To make sure that the pipeline version is compatabile with the steps\n",
    "discussed below and the required dependencies and packages are installed,\n",
    "you can create a fresh conda environment and install the provided\n",
    "`requirements.txt` file:\n",
    "```\n",
    "conda create -n niriss_soss_pipeline python=3.13\n",
    "conda activate niriss_soss_pipeline\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Set the basic parameters to use with this notebook. These will affect\n",
    "what data is used, where data is located (if already in disk), and\n",
    "pipeline modules run in this data. The list of parameters are:\n",
    "\n",
    "* demo_mode\n",
    "* directories with data\n",
    "* pipeline modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import necessary for configuration\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb14e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Note that <code>demo_mode</code> must be set appropriately below.\n",
    "</div>\n",
    "\n",
    "Set <code>demo_mode = True </code> to run in demonstration mode. In this\n",
    "mode this notebook will download example data from the Barbara A.\n",
    "Mikulski Archive for Space Telescopes ([MAST](https://archive.stsci.edu/)) \n",
    "and process it through the\n",
    "pipeline. This will all happen in a local directory unless modified\n",
    "in [Section 3](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "below.\n",
    "\n",
    "Set <code>demo_mode = False</code> if you want to process your own data\n",
    "that has already been downloaded and provide the location of the data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for demo_mode and processing steps.\n",
    "\n",
    "# -----------------------------Demo Mode---------------------------------\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode using online example data!')\n",
    "\n",
    "# --------------------------User Mode Directories------------------------\n",
    "# If demo_mode = False, look for user data in these paths\n",
    "if not demo_mode:\n",
    "    # Set directory paths for processing specific data; these will need\n",
    "    # to be changed to your local directory setup (below are given as\n",
    "    # examples)\n",
    "    basedir = os.path.join(os.getcwd(), '')\n",
    "\n",
    "    # Point to location of science observation data.\n",
    "    # Assumes uncalibrated data in sci_dir/uncal/ and results in stage1,\n",
    "    # stage2, stage3 directories\n",
    "    sci_dir = os.path.join(basedir, 'JWSTData/PID_2734/')\n",
    "\n",
    "# --------------------------Set Processing Steps--------------------------\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# Science processing\n",
    "dodet1 = True  # calwebb_detector1\n",
    "dospec2 = True  # calwebb_spec2\n",
    "dotso3 = True  # calwebb_tso3\n",
    "doviz = True  # Visualize calwebb_tso3 output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070079a",
   "metadata": {},
   "source": [
    "### Set CRDS context and server\n",
    "Before importing <code>CRDS</code> and <code>JWST</code> modules, we need\n",
    "to configure our environment. This includes defining a CRDS cache\n",
    "directory in which to keep the reference files that will be used by the\n",
    "calibration pipeline.\n",
    "\n",
    "If the root directory for the local CRDS cache directory has not been set\n",
    "already, it will be set to create one in the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Set CRDS context and paths----------------------\n",
    "# Each version of the calibration pipeline is associated with a specific CRDS\n",
    "# context file. The pipeline will select the appropriate context file behind\n",
    "# the scenes while running. However, if you wish to override the default context\n",
    "# file and run the pipeline with a different context, you can set that using\n",
    "# the CRDS_CONTEXT environment variable. Here we show how this is done,\n",
    "# although we leave the line commented out in order to use the default context.\n",
    "# If you wish to specify a different context, uncomment the line below.\n",
    "#os.environ['CRDS_CONTEXT'] = 'jwst_1464.pmap'  # CRDS context for 1.20.2\n",
    "\n",
    "# Check whether the local CRDS cache directory has been set.\n",
    "# If not, set it to the user home directory\n",
    "if (os.getenv('CRDS_PATH') is None):\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds')\n",
    "    \n",
    "# Check whether the CRDS server URL has been set.  If not, set it.\n",
    "if (os.getenv('CRDS_SERVER_URL') is None):\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Echo CRDS path in use\n",
    "print(f\"CRDS local filepath: {os.environ['CRDS_PATH']}\")\n",
    "print(f\"CRDS file server: {os.environ['CRDS_SERVER_URL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165cb98d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575588c8",
   "metadata": {},
   "source": [
    "## 2. Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire available screen width for this notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f7f54-ff98-428b-9fa9-b39c50210c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "# ----------------------General Imports------------------------------------\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Numpy for calculations\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for loading data into tables\n",
    "import pandas as pd\n",
    "\n",
    "# Astroquery for downloading demo files\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# For visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import (ManualInterval, LogStretch,\n",
    "                                   ImageNormalize, simple_norm)\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.time import Time\n",
    "import batman # Transit modeling\n",
    "\n",
    "# For file manipulation\n",
    "from astropy.io import fits\n",
    "\n",
    "# For JWST calibration pipeline\n",
    "import jwst\n",
    "import crds\n",
    "\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Tso3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels\n",
    "from jwst.associations import asn_from_list  # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base  # Definition of a Lvl3 association file\n",
    "\n",
    "# Echo pipeline version and CRDS context in use\n",
    "print(f\"JWST Calibration Pipeline Version: {jwst.__version__}\")\n",
    "print(f\"Using CRDS Context: {crds.get_context_name('jwst')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1079aad",
   "metadata": {},
   "source": [
    "### Define convenience functions\n",
    "\n",
    "These functions are used within the notebook and assist with selecting certain kinds of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = []\n",
    "blah.append('test')\n",
    "blah.append('test2')\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a305694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort files into types TA, spectrum, and F277W\n",
    "def sort_files(files):\n",
    "    tafiles = []\n",
    "    scifiles = []\n",
    "    f277wfiles = []\n",
    "    for file in files:\n",
    "        model = datamodels.open(file)\n",
    "        exptype = model.meta.exposure.type\n",
    "        filt = model.meta.instrument.filter\n",
    "        if ((exptype == 'NIS_TACQ') | (exptype == 'NIS_TACONFIRM')):\n",
    "            tafiles.append(file)\n",
    "        if ((exptype == 'NIS_SOSS') & (filt == 'CLEAR')):\n",
    "            scifiles.append(file)\n",
    "        if ((exptype == 'NIS_SOSS') & (filt == 'F277W')):\n",
    "            f277wfiles.append(file)\n",
    "        \n",
    "    return tafiles, scifiles, f277wfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844d476-f8b3-4c24-9c79-091d6016314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d3f75",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8050f",
   "metadata": {},
   "source": [
    "## 3. Demo Mode Setup (ignore if not using demo data)\n",
    "------------------\n",
    "If running in demonstration mode, set up the program information to\n",
    "retrieve the uncalibrated data automatically from MAST using\n",
    "[astroquery](https://astroquery.readthedocs.io/en/latest/mast/mast.html).\n",
    "MAST has a dedicated service for JWST data retrieval, so the archive can\n",
    "be searched by instrument keywords rather than just filenames or proposal IDs.<br>\n",
    "\n",
    "The list of searchable keywords for filtered JWST MAST queries \n",
    "is [here](https://mast.stsci.edu/api/v0/_jwst_inst_keywd.html).<br>\n",
    "\n",
    "For this notebook, we will examine a single TSO of the target, which uses the GR700XD/CLEAR grating/filter combination. \n",
    "Note that the TSO data are typically split into multiple files to faciliate data processing; for more information see the documentation about [Segmented Products](\n",
    "https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html#segmented-files).\n",
    "\n",
    "We will start with the uncalibrated data products. The files we are interested in are named\n",
    "`jw02734002001_04101_00001-segNNN_nis_uncal.fits`, where *NNN* refers to the\n",
    "segment number.\n",
    "\n",
    "More information about the JWST file naming conventions can be found at:\n",
    "https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d595f1e-2590-4f01-bb92-13bb43a22b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the program information and paths for demo program\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode and will download example data from MAST!')\n",
    "    \n",
    "    # --------------Program and observation information--------------\n",
    "    program = '02734'\n",
    "    instr = 'NIRISS/SOSS'\n",
    "    filt_pupil = 'CLEAR;GR700XD'\n",
    "    targname = 'WASP-96'\n",
    "\n",
    "    # --------------Program and observation directories--------------\n",
    "    data_dir = os.path.join('.', 'nis_soss_demo_data')\n",
    "    sci_dir = os.path.join(data_dir, 'PID_2734')\n",
    "    uncal_dir = os.path.join(sci_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "\n",
    "    if not os.path.exists(uncal_dir):\n",
    "        os.makedirs(uncal_dir)\n",
    "\n",
    "    # Create directory if it does not exist\n",
    "    if not os.path.isdir(data_dir):\n",
    "        os.mkdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674a8b8",
   "metadata": {},
   "source": [
    "Identify list of science (SCI) uncalibrated files associated with visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b22375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of observation IDs for the specified demo program\n",
    "if demo_mode:\n",
    "    # Science data\n",
    "    sci_obs_id_table = Observations.query_criteria(instrument_name=[instr],\n",
    "                                                   proposal_id=[program],\n",
    "                                                   filters=[filt_pupil],  # Data for specific filter/pupil\n",
    "                                                   obs_id=['jw' + program + '*'],\n",
    "                                                   target_name=targname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666350e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Turn the list of visits into a list of uncalibrated data files\n",
    "if demo_mode:\n",
    "    # Define types of files to select\n",
    "    file_dict = {'uncal': {'product_type': 'SCIENCE',\n",
    "                           'productSubGroupDescription': 'UNCAL',\n",
    "                           'calib_level': [1]}}\n",
    "\n",
    "    # Science files\n",
    "    sci_files = []\n",
    "\n",
    "    # Loop over visits identifying uncalibrated files that are associated\n",
    "    # with them\n",
    "    for exposure in (sci_obs_id_table):\n",
    "        products = Observations.get_product_list(exposure)\n",
    "        for filetype, query_dict in file_dict.items():\n",
    "            filtered_products = Observations.filter_products(products, productType=query_dict['product_type'],\n",
    "                                                             productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                             calib_level=query_dict['calib_level'])\n",
    "            sci_files.extend(filtered_products['dataURI'])\n",
    "\n",
    "    print(f\"Science files selected for downloading: {len(sci_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a54be",
   "metadata": {},
   "source": [
    "Download all the uncal files and place them into the appropriate\n",
    "directories.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Warning: If this notebook is halted during this step the downloaded file\n",
    "may be incomplete, and cause crashes later on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764fa682",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "if demo_mode:\n",
    "    for filename in sci_files:\n",
    "        sci_manifest = Observations.download_file(filename,\n",
    "                                                  local_path=os.path.join(uncal_dir, Path(filename).name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14864b",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c51254-6295-4f98-bc25-d07300e0d8f4",
   "metadata": {},
   "source": [
    "## 4. Directory Setup\n",
    "---------------------\n",
    "Set up detailed paths to input/output stages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbedc3-fbe3-4c56-ad18-85b5d0c7d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output subdirectories to keep science data products organized\n",
    "# -----------------------------Science Directories------------------------------\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal')  # Uncalibrated pipeline inputs should be here\n",
    "det1_dir = os.path.join(sci_dir, 'stage1')  # calwebb_detector1 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(sci_dir, 'stage2')  # calwebb_spec2 pipeline outputs will go here\n",
    "tso3_dir = os.path.join(sci_dir, 'stage3')  # calwebb_tso3 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "# Ensure filepaths for input data exist\n",
    "if not os.path.exists(uncal_dir):\n",
    "    os.makedirs(uncal_dir)\n",
    "\n",
    "if not os.path.exists(det1_dir):\n",
    "    os.makedirs(det1_dir)\n",
    "if not os.path.exists(spec2_dir):\n",
    "    os.makedirs(spec2_dir)\n",
    "if not os.path.exists(tso3_dir):\n",
    "    os.makedirs(tso3_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd8e6a7-433d-4e5b-a832-46f0bc96a0fe",
   "metadata": {},
   "source": [
    "Print the exposure parameters of all potential input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d78244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uncal_files = sorted(glob.glob(os.path.join(uncal_dir, '*_uncal.fits')))\n",
    "\n",
    "for file in uncal_files:\n",
    "    model = datamodels.open(file)\n",
    "    # print file name\n",
    "    print(model.meta.filename)\n",
    "    # Print out exposure info\n",
    "    print(f\"Instrument: {model.meta.instrument.name}\")\n",
    "    print(f\"Filter: {model.meta.instrument.filter}\")\n",
    "    print(f\"Pupil: {model.meta.instrument.pupil}\")\n",
    "    print(f\"Exposure type: {model.meta.exposure.type}\")\n",
    "    print(f\"Total number of integrations: {model.meta.exposure.nints}\")\n",
    "    if model.meta.exposure.nints != 1:\n",
    "        print(f\"Integration range: {model.meta.exposure.integration_start}-{model.meta.exposure.integration_end}\")\n",
    "    print(f\"Exposure start time (UTC): {Time(model.meta.exposure.start_time, format='mjd').fits}\")\n",
    "    print(f\"Number of groups: {model.meta.exposure.ngroups}\")\n",
    "    print(f\"Readout pattern: {model.meta.exposure.readpatt}\")\n",
    "    print(\"\\n\")\n",
    "    model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469a648-9bb1-40f9-9f8b-fb3a29bc5c7f",
   "metadata": {},
   "source": [
    "Since this is a NIRISS SOSS observation, the first four files are target aquisition (TA) exposures. \n",
    "Target acquisition is performed in a 64x64 pixel subarray before the target is moved\n",
    "to its position in the science subarray. The TA exposures have exposure type `NIS_TACQ` or `NIS_TACONFIRM` and use the F480M filter.\n",
    "These exposures, particularly the final confirmation image, can be helpful for diagnosing potential problems with the data.\n",
    "For more information about the SOSS TA procedure, see the [NIRISS TA documentation](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-operations/niriss-target-acquisition).\n",
    "\n",
    "The following three exposures are our time series observation, split into three segments: `seg001` through `seg003` in the filenames. These exposures use the CLEAR/GR700XD filter/pupil combination and consist of 280 integrations in total, each composed of 14 groups up the ramp, corresponding to a total exposure time of 6.41 hours.\n",
    "Each exposure uses the [`NISRAPID` readout pattern](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-detector-overview/niriss-detector-readout-patterns). \n",
    "\n",
    "The final exposure uses the F277W filter and was obtained because it is useful for masking order-zero sources and to isolate the first spectral order in the $2.4 \\ \\mu m-2.8 \\ \\mu m$\n",
    "wavelength range, where they overlap significantly in the CLEAR exposures. We will process the F277W exposure through stage 1 in this notebook, but the background subtraction step in the second stage does not currently work with these exposures.\n",
    "\n",
    "For more information about how JWST exposures are defined by up-the-ramp sampling, see the\n",
    "[Understanding Exposure Times JDox article](https://jwst-docs.stsci.edu/understanding-exposure-times)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a042c01-ad45-4bc7-81eb-115f83534c67",
   "metadata": {},
   "source": [
    "In this notebook, we will focus on processing the CLEAR/GR700XD exposures (though we will also process the single F277W exposure through Stage 1), so we can update the list of uncalibrated files to remove the TA exposures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843ff7f-58b6-4294-bfaa-606c29abc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97528f9b",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e28e0-a63f-4790-b8a0-3ad4bff5a167",
   "metadata": {},
   "source": [
    "## 5. Detector1 Pipeline\n",
    "Run the data through the\n",
    "[Detector1](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_detector1)\n",
    "stage of the pipeline to apply detector level calibrations and create a\n",
    "countrate data product where slopes are fitted to the integration ramps.\n",
    "These `_rateints.fits` products are 3D (nintegrations x nrows x ncols)\n",
    "and contain the fitted ramp slopes for each integration.\n",
    "2D countrate data products (`_rate.fits`) are also\n",
    "created (nrows x ncols) which have been averaged over all\n",
    "integrations.\n",
    "\n",
    "The following Detector1 steps are available for NIRISS SOSS:\n",
    "1. `group_scale`\n",
    "2. `dq_init`\n",
    "3. `saturation`\n",
    "4. `superbias`\n",
    "5. `refpix`\n",
    "6. `linearity`\n",
    "7. `dark_current`\n",
    "8. `jump`\n",
    "9. `clean_flicker_noise`\n",
    "10. `ramp_fit`\n",
    "11. `gain_scale`\n",
    "\n",
    "By default, these Detector1 steps are currently skipped for NIRISS SOSS exposures: `group_scale`, `clean_flicker_noise`, and `gain_scale`.\n",
    "\n",
    "Each observing mode of JWST has different requirements when it comes to correcting for detector effects.\n",
    "The `clean_flicker_noise` step was designed to remove 1/f noise from calibrated ramp images, but SOSS \n",
    "users have found its performance insufficient due to the lack of non-illuminated\n",
    "background pixels in the SOSS subarrays. A more rigorous group-level subtraction is likely needed, and is currently in development by the SOSS team.\n",
    "By default, this step is currently skipped.\n",
    "\n",
    "It is also unclear whether TSO science benefits from the dark current step in its current implementation. \n",
    "In the following example we leave the step on, but it can easily be turned off as shown. \n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation on [ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592ee58-b424-4bd6-973b-88fd081b81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Detector1 pipeline should be configured\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "det1dict = defaultdict(dict)\n",
    "\n",
    "# Step names are copied here for reference\n",
    "det1_steps = ['group_scale', 'dq_init', 'saturation', 'superbias', 'refpix',\n",
    "              'linearity', 'dark_current', 'jump', 'clean_flicker_noise',\n",
    "              'ramp_fit', 'gain_scale']\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped\n",
    "# Optionally, skip the dark step\n",
    "# det1dict['dark_current']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#det1dict['dq_init']['override_mask'] = 'myfile.fits'  # Bad pixel mask\n",
    "#det1dict['saturation']['override_saturation'] = 'myfile.fits'  # Saturation\n",
    "#det1dict['linearity']['override_linearity'] = 'myfile.fits'  # Linearity\n",
    "#det1dict['dark_current']['override_dark'] = 'myfile.fits'  # Dark current subtraction\n",
    "#det1dict['jump']['override_gain'] = 'myfile.fits'  # Gain used by jump step\n",
    "#det1dict['ramp_fit']['override_gain'] = 'myfile.fits'  # Gain used by ramp fitting step\n",
    "#det1dict['jump']['override_readnoise'] = 'myfile.fits'  # Read noise used by jump step\n",
    "#det1dict['ramp_fit']['override_readnoise'] = 'myfile.fits'  # Read noise used by ramp fitting step\n",
    "\n",
    "# Turn on multi-core processing (off by default). Choose what fraction of cores to use (quarter, half, or all)\n",
    "det1dict['jump']['maximum_cores'] = 'half'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13db72-1310-4ef8-acf9-f326a0767566",
   "metadata": {},
   "source": [
    "Run Detector1 stage of pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f107b-5f80-4674-bc34-2d88791e4409",
   "metadata": {
    "scrolled": true,
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run Detector1 stage of pipeline, specifying:\n",
    "# output directory to save *_rateints.fits files\n",
    "# save_results flag set to True so the *rateints.fits files are saved\n",
    "\n",
    "if dodet1:\n",
    "    for uncal in uncal_files:\n",
    "        rate_result = Detector1Pipeline.call(uncal,\n",
    "                                             output_dir=det1_dir,\n",
    "                                             steps=det1dict,\n",
    "                                             save_results=True)\n",
    "else:\n",
    "    print('Skipping Detector1 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc2b48-7ec7-4b59-b3ee-2dfdf0782c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime for Detector1: {time1 - time0:0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5520a-6097-482b-b3e0-a6bf94cf7af3",
   "metadata": {},
   "source": [
    "### Exploring the data\n",
    "\n",
    "Identify `*_rateints.fits` files and verify which pipeline steps were run and\n",
    "which calibration reference files were applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c122766-a545-4042-93e8-f68c18f62fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dodet1:\n",
    "    # find rateints files\n",
    "    rateints_files = sorted(glob.glob(os.path.join(det1_dir, '*_rateints.fits')))\n",
    "    # Restrict to selected filter if applicable\n",
    "    #rateints_files = select_filter_files(rateints_files, use_filter)\n",
    "    \n",
    "    # Read in the first file as datamodel as an example\n",
    "    rateints = datamodels.open(rateints_files[0])\n",
    "    \n",
    "    # Check which steps were run\n",
    "    for step, status in rateints.meta.cal_step.instance.items():\n",
    "        print(f\"{step}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2396cc-939f-44a6-878c-b33955796da8",
   "metadata": {},
   "source": [
    "Check which CRDS version and reference files were used to calibrate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525e133-d875-4ea7-9552-d7f0df05747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dodet1:\n",
    "    for key, val in rateints.meta.ref_file.instance.items():\n",
    "        print(f\"{key}:\")\n",
    "        for param in rateints.meta.ref_file.instance[key]:\n",
    "            print(f\"\\t{rateints.meta.ref_file.instance[key][param]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06450da",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350808ee-9579-4cb5-8f02-a74904c91449",
   "metadata": {},
   "source": [
    "## 6. Spec2 Pipeline \n",
    "\n",
    "In the [Spec2 stage of the pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html),\n",
    "each exposure is corrected with further instrumental calibrations, and then a 1D spectrum is extracted. The steps applied to NIRISS SOSS exposures are, in order:\n",
    "1. `assign_wcs`\n",
    "2. `background`\n",
    "3. `srctype`\n",
    "4. `straylight`\n",
    "5. `flat_field`\n",
    "6. `pathloss`\n",
    "7. `extract_1d`\n",
    "8. `photom`\n",
    "\n",
    "Note that while most JWST spectroscopic modes perform the photometric calibration (`photom`) followed by spectral extraction (`extract_1d`),\n",
    "the order of these steps is switched for SOSS so that the overlapping spectral orders can be disentangled before they are converted to flux units, as required for the [Algorithm to Treat Order ContAmination (ATOCA)](https://ui.adsabs.harvard.edu/abs/2022PASP..134i4502D/abstract).\n",
    "\n",
    "The Spec2 Pipeline can produce `*_x1d.fits` or `*_x1dints.fits` files, depending on whether the input files are\n",
    "`*_rate.fits` or `*_rateints.fits`. These products are the 1D extracted spectra which will be used as input to the following pipeline stage. In this case, we are interested in the time series of observations, so we want to preserve the multiple integrations by using the `*_rateints.fits` files as input. \n",
    "\n",
    "The Spec2 Pipeline can also save `*_cal.fits` or `*_calints.fits`, which are 2D or 3D fully calibrated images, again depending on the dimensions of the input. \n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation on [ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a091817-7a3c-4a60-a914-f9ac54d36e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00fda2-a7c0-445a-bf23-0d9b4f050419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec2 pipeline should be configured.\n",
    "\n",
    "# Boilerplate dictionary setup\n",
    "spec2dict = defaultdict(dict)\n",
    "\n",
    "# Step names are copied here for reference\n",
    "spec2steps = ['assign_wcs', \n",
    "              'bkg_subtract',\n",
    "              'srctype',\n",
    "              'straylight',\n",
    "              'flat_field',\n",
    "              'pathloss',\n",
    "              'extract_1d',\n",
    "              'photom']\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "# spec2dict['bkg_subtract']['skip'] = True\n",
    "\n",
    "spec2dict['bkg_subtract']['soss_source_percentile'] = 50.0\n",
    "\n",
    "# Overrides for various reference files\n",
    "# Files should be in the base local directory or provide full path\n",
    "#spec2dict['flat_field']['override_flat'] = 'myfile.fits'  # Pixel flatfield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247170cd-a3ee-4d9d-b8b3-f930727f8abc",
   "metadata": {},
   "source": [
    "Find and sort the input files, ensuring use of absolute paths. At this time we will discard the last exposure, the F277W spectrum, from the list of files, as it cannot currently be processed through Stage 2. We will also remove the TA exposures from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19cd4e6-cc38-4e26-8e8b-7fb143856940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use files from the detector1 output folder\n",
    "rateints_files = sorted(glob.glob(os.path.join(det1_dir, 'jw*rateints.fits')))\n",
    "\n",
    "for ii in range(len(rateints_files)):\n",
    "    rateints_files[ii] = os.path.abspath(rateints_files[ii])\n",
    "\n",
    "# Discard any TA exposures and the final F277W exposure from the list\n",
    "tafiles, scifiles, f277wfiles = sort_files(rateints_files)\n",
    "rateints_files = scifiles\n",
    "\n",
    "print(f\"Found {str(len(rateints_files))} science files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d916e5-dd5d-472d-9aff-b2b4c86decf8",
   "metadata": {
    "scrolled": true,
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run Spec2 stage of pipeline, specifying:\n",
    "# output directory to save files\n",
    "\n",
    "if dospec2:\n",
    "    for rateints in rateints_files:\n",
    "        calints_result = Spec2Pipeline.call(rateints,\n",
    "                                            output_dir=spec2_dir,\n",
    "                                            steps=spec2dict,\n",
    "                                            )\n",
    "else:\n",
    "    print(\"Skipping Spec2 processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58b94d-e1d7-4b73-b598-756c99d81174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.0f} seconds\")\n",
    "print(f\"Runtime for Spec2: {time1 - time_spec2:0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57b7d0-95ef-4227-b730-6f54a3eaaa16",
   "metadata": {},
   "source": [
    "Verify which pipeline steps were run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27390bbd-ae2d-49e7-977a-59d3765c0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec2:\n",
    "    # Identify *_calints.fits files\n",
    "    calints_files = sorted(glob.glob(os.path.join(spec2_dir, '*_calints.fits')))\n",
    "    # Restrict to selected filter if applicable\n",
    "\n",
    "    # Read in the first file as datamodel as an example\n",
    "    calints = datamodels.open(calints_files[0])\n",
    "    \n",
    "    # Check which steps were run\n",
    "    for step, status in calints.meta.cal_step.instance.items():\n",
    "        print(f\"{step}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77293ab-4eaa-4ebd-9f9b-bef8f6a2300f",
   "metadata": {},
   "source": [
    "Check which reference files were used to calibrate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544744b-804e-4583-8311-e47b5e16c7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dospec2:\n",
    "    for key, val in calints.meta.ref_file.instance.items():\n",
    "        print(f\"{key}:\")\n",
    "        for param in calints.meta.ref_file.instance[key]:\n",
    "            print(f\"\\t{calints.meta.ref_file.instance[key][param]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9155e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbb244-7af9-4cbf-90d9-08598378c16a",
   "metadata": {},
   "source": [
    "## 7. TSO3 Pipeline\n",
    "\n",
    "In the [TSO3 stage of the pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_tso3.html), an association of calibrated TSO exposures is used to produce calibrated time-series spectra of the target. By default his stage consists of three steps for SOSS TSOs:\n",
    "1. `outlier_detection`\n",
    "2. `extract_1d`\n",
    "3. `white_light`\n",
    "\n",
    "This stage also includes an optional [pixel replacement step](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pixel_replace/main.html) (`pixel_replace`) that is off by default for SOSS, but can be enabled to reduce the noise introduced by bad pixels in the spectrum. To run it, uncomment the relevant line in the tso3dict below.\n",
    "\n",
    "In order to run the TSO3 stage, an [Association](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/overview.html) file\n",
    "must first be created to instruct the pipeline to process the segments of the time series together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e81dc9-78dd-4bba-879c-9dfa2a4da69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tso3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159e63d-3c89-44ef-9a43-caaccd1abb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the TSO3 pipeline should be configured\n",
    "# Boilerplate dictionary setup\n",
    "tso3dict = defaultdict(dict)\n",
    "\n",
    "# Options for each step (example)\n",
    "#tso3dict['outlier_detection']['snr'] = '5.0 4.0' # Signal-to-noise thresholds for bad pixel identification\n",
    "#tso3dict['extract_1d']['soss_atoca'] = False  # Turn off the ATOCA algorithm for order contamination (default=True)\n",
    "#tso3dict['whitelight']['min_wavelength'] = 0.8 # minimum wavelength from which to sum the flux array\n",
    "tso3dict['extract_1d']['save_results'] = True\n",
    "tso3dict['white_light']['save_results'] = True\n",
    "# tso3dict['pixel_replace']['skip'] = False # Run the pixel replacement step\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example)\n",
    "#tso3dict['white_light']['skip'] = True\n",
    "\n",
    "# Overrides for various reference files (example)\n",
    "# Files should be in the base local directory or provide full path\n",
    "#tso3dict['extract_1d']['override_extract1d'] = 'myx1dfile.fits'  # override spectral extraction parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10da872-21ae-46c2-a3b1-f9cdefae3b36",
   "metadata": {},
   "source": [
    "Find and sort all of the input files, ensuring use of absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b5b47-62e9-4593-abc4-7ec630492e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSO3 takes the calints.fits files output by Spec2\n",
    "\n",
    "calints_files = sorted(glob.glob(os.path.join(spec2_dir, 'jw*calints.fits')))\n",
    "calints_files = [os.path.abspath(calints) for calints in calints_files]\n",
    "\n",
    "print(f'Found {str(len(calints_files))} science files to process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cb6f5-44a1-4f28-95dc-b23a97451465",
   "metadata": {},
   "source": [
    "### Create Association Files\n",
    "\n",
    "An association file lists the exposures to calibrate together in `Stage 3`\n",
    "of the pipeline. Note that an association file is available for download\n",
    "from MAST, with a filename of `*_asn.json`, though it may require additional manipulation.\n",
    "\n",
    "Here we create a [Level 3 association](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/level3_asn_technical.html) file that will tell the pipeline to process the segments of the time series together. Since we only have a single time series, we only need one association file.\n",
    "\n",
    "Note that the final output products will have a rootname that is specified by the `product_name`\n",
    "in the association file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31f80f-8d7b-45ae-8537-15d0208637df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Level 3 Association\n",
    "if dotso3:\n",
    "    # Get the program, target name from the header of one file\n",
    "    hdr = fits.getheader(calints_files[0])\n",
    "    program = hdr['PROGRAM']\n",
    "    name = hdr['TARGNAME']\n",
    "    # Create and save the asn file to the TSO3 directory\n",
    "    asnfile = os.path.join(tso3_dir, f'level3_{program}_asn.json')\n",
    "    asn = asn_from_list.asn_from_list(calints_files, rule=DMS_Level3_Base, product_name=name)\n",
    "    asn.data['asn_type'] = 'tso3'\n",
    "    asn.data['program'] = program\n",
    "\n",
    "    with open(asnfile, 'w') as f:\n",
    "        f.write(asn.dump()[1])\n",
    "    if os.path.exists(asnfile):\n",
    "        print(rf\"Level 3 association successfully created and saved to: {asnfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4ea2c-0fc3-4ee0-820f-a85e18e4fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the ASN file.\n",
    "if dotso3:\n",
    "    with open(asnfile, 'r') as f_obj:\n",
    "        asnfile_data = json.load(f_obj)\n",
    "    expanded_json = json.dumps(asnfile_data, indent=2)\n",
    "    print(expanded_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629cad5b-e5a7-473e-8583-ad12cda55fa4",
   "metadata": {},
   "source": [
    "### Run TSO3 stage of the pipeline\n",
    "\n",
    "From the four exposures listed in the association file, the\n",
    "`TSO3` stage of the pipeline will produce:\n",
    "* `*_cfrints.fits` files from the `outlier_detection` step containing cosmic-ray-flagged images with updated DQ arrays for each input `calints` file\n",
    "* `*_x1dints.fits` file from the `extract_1d` step containing extracted spectra for all integrations in the input exposures\n",
    "* `*_whtlt.ecsv` file from the `white_light` step containing an ASCII catalog of wavelength-integrated white-light flux as a function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cebfd8-a650-41a3-8ff0-a8fd43e079d6",
   "metadata": {
    "scrolled": true,
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run Stage 3\n",
    "if dotso3:\n",
    "    tso3_result = Tso3Pipeline.call(asnfile,\n",
    "                                    save_results=True,\n",
    "                                    output_dir=tso3_dir,\n",
    "                                    steps=tso3dict)\n",
    "else:\n",
    "    print('Skipping TSO3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d2c65-4614-4592-aa67-daa2de985013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.0f} seconds\")\n",
    "print(f\"Runtime for TSO3: {time1 - time_tso3:0.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb6c40-fbb8-4226-b620-64b0c02c8ceb",
   "metadata": {},
   "source": [
    "### Verify which pipeline steps were run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754530a-d4e2-493b-bf8b-bf8d1ad08770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dotso3:\n",
    "    # Identify *x1dints.fits file and open as datamodel\n",
    "    x1dints = glob.glob(os.path.join(tso3_dir, \"*_x1dints.fits\"))[0]\n",
    "\n",
    "    with datamodels.open(x1dints) as x1d_model:\n",
    "        # Check which steps were run\n",
    "        for step, status in x1d_model.meta.cal_step.instance.items():\n",
    "            print(f\"{step}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c5280-69a5-4796-b78f-724078a94d2b",
   "metadata": {},
   "source": [
    "Check which reference files were used to calibrate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb0842-cc95-42d5-8852-593b85032c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dotso3:\n",
    "    for key, val in x1d_model.meta.ref_file.instance.items():\n",
    "        print(f\"{key}:\")\n",
    "        for param in x1d_model.meta.ref_file.instance[key]:\n",
    "            print(f\"\\t{x1d_model.meta.ref_file.instance[key][param]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624c896-6d2a-4578-a68c-de598a00e81a",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569d0b2-5682-4f48-aefa-95f091c8f75b",
   "metadata": {},
   "source": [
    "## 8. Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e56276-f13b-412e-b4a2-52b51d7eb20f",
   "metadata": {},
   "source": [
    "### Define plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aef88c-0c1c-4364-b3c4-5a04a7fb2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spectra(filelist, order=[1, 2]):\n",
    "    \"\"\"\n",
    "    Join segmented 1D spectra and their corresponding timestamps\n",
    "    and wavelengths to form single arrays. If only one file given, \n",
    "    just extract quantities and return them in suitable shape for plotting.\n",
    "    Handles x1dints products from either spec2 or tso3.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The contents of the returned dictionary are structured as follows:\n",
    "        specdict = {\n",
    "                    \"ORDER1\" : {\"FLUX\" : arr(nints, npix),\n",
    "                                \"FLUX_ERROR\" : arr(nints, npix),\n",
    "                                \"WAVELENGTH\" : arr(nints, npix),\n",
    "                                \"INT_TIMES\" : arr(nints),\n",
    "                                }\n",
    "                    \"ORDER2\" : ...\n",
    "                    }\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filelist: str, list of str\n",
    "        Name(s) of x1dints file(s) to join. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    specdict: dict\n",
    "        Nested dictionary containing dict of spectral data for each order. See Notes for contents\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check inputs\n",
    "    if isinstance(filelist, str):\n",
    "        filelist = [filelist]\n",
    "    if isinstance(order, int):\n",
    "        order = [order]\n",
    "    # Prepare a dictionary for outputs\n",
    "    specdict = defaultdict(dict)\n",
    "    # Find out how many orders are contained in the dataproducts\n",
    "    ordlst = []\n",
    "    with fits.open(filelist[0]) as hdulist:\n",
    "        for ext in hdulist:\n",
    "            try:\n",
    "                ordlst.append(hdulist[ext].header[\"SPORDER\"])\n",
    "            except KeyError:\n",
    "                continue\n",
    "    ordersinfile = max(ordlst)\n",
    "    # Loop over the *requested* orders\n",
    "    for nord in np.arange(order[0] - 1, order[-1]):\n",
    "        first = True\n",
    "        # Loop over input files\n",
    "        for seg in filelist:\n",
    "            with datamodels.open(seg) as model:\n",
    "                specidxlist = np.arange(len(model.spec))\n",
    "                idx = specidxlist[nord::ordersinfile]\n",
    "                for i in idx:\n",
    "                    segflux = model.spec[i].spec_table.FLUX\n",
    "                    segfluxerr = model.spec[i].spec_table.FLUX_ERROR\n",
    "                    seginttimes = model.spec[i].spec_table['TDB-MID']\n",
    "                    # wavelength array for each integration is the same, but we include them all for completeness\n",
    "                    segwavel = model.spec[i].spec_table.WAVELENGTH\n",
    "                    if first:\n",
    "                        fullflux = segflux\n",
    "                        fullfluxerr = segfluxerr\n",
    "                        fulltimes = seginttimes\n",
    "                        fullwavels = segwavel\n",
    "                        first = False\n",
    "                    else:\n",
    "                        fullflux = np.concatenate((fullflux, segflux), axis=0)\n",
    "                        fullfluxerr = np.concatenate((fullfluxerr, segfluxerr), axis=0)\n",
    "                        fullwavels = np.concatenate((fullwavels, segwavel), axis=0)\n",
    "                        fulltimes = np.concatenate((fulltimes, seginttimes), axis=0)\n",
    "                        \n",
    "        # Populate the dictionary\n",
    "        specdict[f'ORDER{nord+1}']['FLUX'] = fullflux\n",
    "        specdict[f'ORDER{nord+1}']['FLUX_ERROR'] = fullfluxerr\n",
    "        specdict[f'ORDER{nord+1}']['WAVELENGTH'] = fullwavels\n",
    "        specdict[f'ORDER{nord+1}']['INT_TIMES'] = fulltimes\n",
    "        \n",
    "        print(f\"Order {nord+1} dimensions: {fullflux.shape[0]} integrations, {fullflux.shape[1]} wavelengths\")\n",
    "    return specdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d02849-f437-4457-92f1-85045a40d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_soss_im(filename, intnum=0, groupnum=-1, lognormalize=True):\n",
    "    \"\"\"\n",
    "    Display a SOSS image.\n",
    "\n",
    "    Can display 2D (e.g. rate), 3D (e.g. calints), 4D (e.g. uncal) images.\n",
    "    Defaults to show the first integration of 3D files; first integration/\n",
    "    last group of 4D files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Name of file to display\n",
    "    intnum: int\n",
    "        Integration number in each file to display. Default is 0.\n",
    "    groupnum: int\n",
    "        Group number to display, if relevant. Default is -1 (last group)\n",
    "    \"\"\"\n",
    "\n",
    "    # Open file as a datamodel\n",
    "    with datamodels.open(filename) as model:\n",
    "        # Read the data\n",
    "        data = model.data\n",
    "        # Get some other useful info\n",
    "        units = model.meta.bunit_data\n",
    "        filt = model.meta.instrument.filter\n",
    "        pupil = model.meta.instrument.pupil\n",
    "        targname = model.meta.target.catalog_name\n",
    "        \n",
    "        if len(data.shape) == 2:\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "        # Check inputs\n",
    "        if isinstance(intnum, int) and -1 <= intnum < data.shape[0]: # if intnum is between -1 and the first dimension\n",
    "            data = data[intnum] # now 2d \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid integration number '{intnum}' for data with shape {data.shape}\")\n",
    "        if len(data.shape) == 3:\n",
    "            if isinstance(groupnum, int) and -1 <= groupnum < data.shape[0]:\n",
    "                data = data[groupnum]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid group number '{groupnum}' for data with shape {data.shape}\")\n",
    "    # Make the figure\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 5))\n",
    "    if lognormalize is True:\n",
    "        # sigma_clipped_data = sigma_clip(data)\n",
    "        # vmin = np.nanmin(sigma_clipped_data)\n",
    "        # vmax = np.nanmax(sigma_clipped_data)\n",
    "        vmin = np.nanmin(data)\n",
    "        vmax = np.nanmax(data)\n",
    "        # norm = simple_norm(data, vmin=vmin, vmax=vmax)\n",
    "        norm = ImageNormalize(data,\n",
    "                              interval=ManualInterval(vmin=vmin-(vmin), vmax=vmax+(vmin)),\n",
    "                              stretch=LogStretch())\n",
    "        im = ax.imshow(data, origin='lower', norm=norm)\n",
    "    else:\n",
    "        im = ax.imshow(data, origin='lower')\n",
    "    titlestring = f\"{os.path.basename(filename)}: {targname} {pupil}/{filt}\"\n",
    "    ax.set_title(titlestring)\n",
    "    ax.set_xlabel('X pixel')\n",
    "    ax.set_ylabel('Y pixel')\n",
    "    fig.colorbar(im, orientation='horizontal', label=units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30b2aa-88b9-4865-941c-55b0962b85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_spectrum(filename, intnum=0, order=1):\n",
    "    \"\"\"\n",
    "    Display a SOSS spectrum.\n",
    "\n",
    "    This function take either a single file or a list of files,\n",
    "    in which case it will assume they are a single TSO and concatenate \n",
    "    the segments to produce a single set of extracted measurements.\n",
    "    Can display an averaged spectrum ('x1d') or a single integration of a \n",
    "    multi-integration spectrum ('x1dints').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str or list of str\n",
    "        Name of file to display\n",
    "    intnum: int\n",
    "        Integration number in each file to display. Default is 0.\n",
    "    order: int\n",
    "        Which spectral order to plot. Options are 1, 2, and 3.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check inputs\n",
    "    if isinstance(order, int):\n",
    "        order = [order]\n",
    "    # If multiple segments, join the spectra\n",
    "    # We assume they are in the correct order already...\n",
    "    specdict = prepare_spectra(filename, order=order)\n",
    "    \n",
    "    # Set up the plot.\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "    \n",
    "    for ordr in order:\n",
    "        orderdict = specdict[f\"ORDER{ordr}\"]\n",
    "        flux = orderdict['FLUX']\n",
    "        fluxerr = orderdict['FLUX_ERROR']\n",
    "        wavel = orderdict['WAVELENGTH']\n",
    "        ax.errorbar(wavel[intnum], flux[intnum], yerr=fluxerr[intnum], label=f\"Order {ordr}\")\n",
    "    plt.legend()\n",
    "    ax.set_xlabel(r'Wavelength [$\\mu$m]')\n",
    "    ax.set_ylabel('Flux [MJy]')\n",
    "    ax.set_title(os.path.basename(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88d7ed-aa53-4832-a20c-2837477dae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrum_timeseries(specfiles, normalize=True):\n",
    "    \"\"\"\n",
    "    Display a SOSS spectrum timeseries.\n",
    "\n",
    "    This function creates a plot of flux as a function of both wavelength and time,\n",
    "    to visualize how the flux changes over the duration of the time series. The flux\n",
    "    at each wavelength is normalized by default to emphasize the transit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    specfiles: str or list of str\n",
    "        Name of file(s) containing a time-series of spectra\n",
    "    normalize: bool\n",
    "        If True, normalize the flux at each wavelength by an approximate out-of-transit flux\n",
    "        Default=True\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get data \n",
    "    specdict = prepare_spectra(specfiles)\n",
    "    \n",
    "    flux = specdict[\"ORDER1\"][\"FLUX\"]\n",
    "    wavel = specdict[\"ORDER1\"][\"WAVELENGTH\"][0] \n",
    "    times = specdict[\"ORDER1\"][\"INT_TIMES\"]\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(7, 7))\n",
    "\n",
    "    if normalize:\n",
    "\n",
    "        # Average flux at each wavelength for first 100 integrations (assumed out-of-transit)\n",
    "        first100 = np.nanmean(flux[:100, :], axis=0)\n",
    "        # Normalize the flux to emphasize the transit\n",
    "        flux = flux/first100\n",
    "        sigma_clipped_data = sigma_clip(flux)\n",
    "        vmin = np.nanmin(sigma_clipped_data)\n",
    "        vmax = np.nanmax(sigma_clipped_data)\n",
    "        norm = simple_norm(sigma_clipped_data, vmin=vmin, vmax=vmax)\n",
    "    # Trim last row/column for mesh plotting\n",
    "    fluxtrim = flux[:-1, :-1].T\n",
    "\n",
    "    # Map times in MJD to time from midtransit\n",
    "    # Transit ephemera from Carter et al. 2024 (doi:10.1038/s41550-024-02292-x)\n",
    "    T_c = 2459787.5567843 - 2400000.5\n",
    "    \n",
    "    reltime = (times - T_c) * 24\n",
    "    if normalize:\n",
    "        mesh = ax.pcolormesh(reltime, wavel, fluxtrim, norm=norm)\n",
    "    else:\n",
    "        mesh = ax.pcolormesh(reltime, wavel, fluxtrim)\n",
    "    fig.colorbar(mesh, label='Relative Flux')\n",
    "    ax.set_xlabel('Time from Mid-Transit [hr]')\n",
    "    ax.set_ylabel(r'Wavelength [$\\mu$m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849ecb1-80ce-43fd-adb9-f7ba53c4433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lightcurve(whtlt_file, order=[1, 2], modelparams=None):\n",
    "    \"\"\"\n",
    "    Display a SOSS whitelight curve.\n",
    "\n",
    "    This function displays the total flux over all wavelengths for \n",
    "    each integration in the time series, for each order. Optionally, \n",
    "    we can pass `batman` model parameters to overplot a transit model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    whtlt_file: str\n",
    "        Name of file(s) containing a time-series of spectra\n",
    "    order: int\n",
    "        Which spectral order to plot. Options are 1, 2, and 3.\n",
    "    modelparams: batman.TransitParams()\n",
    "        Transit model parameters to plot with the data\n",
    "    \"\"\"\n",
    "    # Read the file into a pandas dataframe\n",
    "    whitelight_df = pd.read_csv(whtlt_file, on_bad_lines='skip', comment='#', sep=r'\\s+')\n",
    "    # Check inputs\n",
    "    if isinstance(order, int):\n",
    "        order = [order]\n",
    "    # Set up the figure\n",
    "    fig, axs = plt.subplots(len(order), 1, figsize=(7, 5), sharex=True)\n",
    "    if not isinstance(axs, np.ndarray):\n",
    "        axs = [axs]\n",
    "    \n",
    "    times = np.array(whitelight_df['BJD_TDB'])\n",
    "    if modelparams is not None:\n",
    "        # Plot a simple transit model using the best-fit parameters from the Carter et al. 2024 paper\n",
    "        transitmodel = batman.TransitModel(modelparams, times)    # initializes model\n",
    "        modelflux = transitmodel.light_curve(modelparams)         # calculates light curve\n",
    "        reltime = (times - modelparams.t0)*24\n",
    "    \n",
    "    for ii, ordr in enumerate(order):\n",
    "        flux = whitelight_df[f'whitelight_flux_order_{ordr}']\n",
    "        # Normalize to have an out-of-transit flux of ~1\n",
    "        fluxnorm = flux/np.nanmean(flux[:100])\n",
    "        \n",
    "        axs[ii].set_ylabel('Relative Flux')\n",
    "        axs[ii].set_title(f'Order {ordr}')\n",
    "        if modelparams is not None:\n",
    "            axs[ii].plot(reltime, fluxnorm, label='Data')\n",
    "            axs[ii].plot(reltime, modelflux, 'k-', label='Model')\n",
    "            axs[-1].set_xlabel('Time to Mid-Transit [hr]')\n",
    "        else:\n",
    "            axs[ii].plot(times, fluxnorm, label='Data')\n",
    "            axs[-1].set_xlabel('BJD [TDB]')\n",
    "    \n",
    "    plt.suptitle(os.path.basename(whtlt_file))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce7c5e-d3a9-49c2-a564-cec82d087b4f",
   "metadata": {},
   "source": [
    "### Gather the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb1109-ef2f-41eb-874d-a6463642df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    # Get all the filenames\n",
    "    rateintsfiles = sorted(glob.glob(os.path.join(det1_dir, '*rateints.fits')))\n",
    "    tafiles, scifiles, f277wfiles = sort_files(rateintsfiles)\n",
    "    rateintsfiles = scifiles\n",
    "    calintsfiles = sorted(glob.glob(os.path.join(spec2_dir, '*calints.fits')))\n",
    "    x1d_spec2files = sorted(glob.glob(os.path.join(spec2_dir, '*x1dints.fits')))\n",
    "    x1d_tso3file = sorted(glob.glob(os.path.join(tso3_dir, '*x1dints.fits')))[0]\n",
    "    whtltfile = sorted(glob.glob(os.path.join(tso3_dir, '*whtlt.ecsv')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9487bf5-8c14-42ea-a905-fc5dba7e84dd",
   "metadata": {},
   "source": [
    "### Examine Target Acquisition images\n",
    "\n",
    "We will not do any analysis of the TA images in this instance, but we will check them briefly to confirm that the source appears at the center of the final TACONFIRM image. These 4D uncal files contain a single integration composed of 13 groups up the ramp, so we will look at the last group of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15305b-5bfc-4078-b9dc-14a8379f0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (doviz & (len(tafiles) > 0)):\n",
    "    # Prepare the figure\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    for ii, ta_fn in enumerate(tafiles):\n",
    "        # Plot the data\n",
    "        with datamodels.open(ta_fn) as ta_dm:\n",
    "            ta_im = ta_dm.data\n",
    "            # Exclude outlier pixels\n",
    "            sigma_clipped_data = sigma_clip(ta_im[0])\n",
    "            imshape = ta_im[0].shape\n",
    "            vmin = np.nanmin(sigma_clipped_data)\n",
    "            vmax = np.nanmax(sigma_clipped_data)\n",
    "            # normalize, adjusting the limits a bit\n",
    "            norm = simple_norm(sigma_clipped_data, vmin=vmin - (0.5*abs(vmin)), vmax=vmax + (0.5*abs(vmax)))\n",
    "\n",
    "            im = axs[ii].imshow(ta_im[0], origin='lower', norm=norm)\n",
    "            units = ta_dm.meta.bunit_data\n",
    "            axs[ii].set_title(ta_dm.meta.exposure.type)\n",
    "            axs[ii].scatter(imshape[0]/2, imshape[1]/2, marker='x', color='k')\n",
    "\n",
    "            axs[ii].set_axis_off()\n",
    "\n",
    "    fig.subplots_adjust(bottom=.2)\n",
    "    cbar_ax = fig.add_axes([0.12, 0.0, 0.78, 0.12])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.set_label(units, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a01317-b133-4233-8ded-7c4f6ba11724",
   "metadata": {},
   "source": [
    "We can see that the source is being dithered in the first three images, and is nicely centered in the final image, as indicated by the black 'x' at the center of the array. The white pixels are those replaced by NaNs by the Detector1 pipeline due to saturation, outliers, or other data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7afe2-6a75-40c4-8307-e72166aa3baf",
   "metadata": {},
   "source": [
    "### Examine Detector1 products\n",
    "\n",
    "We will begin by looking at the output of the first pipeline stage, the `rateints` files. As the 2D spectra over the duration of the time series will look very similar, we will only plot the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494d591-ea23-4721-b5fd-7749c299433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    display_soss_im(rateintsfiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedffb44-e03a-4050-b616-2a6df7cbaf6a",
   "metadata": {},
   "source": [
    "We are displaying these images on a logarithmic scale to be able to see the features of both the cross-dispersed spectral traces and the background. To see them on a linear scale, pass the argument `lognormalize=False`. \n",
    "\n",
    "Orders 1, 2, and 3 are clearly visible in this image, along with some faint background sources. Order 1 extends across the full subarray and covers wavelengths from 0.9 $\\mu m$ to 2.8 $\\mu m$. The Order 2 trace, covering 0.6$ \\mu m$ to 1.4$ \\mu m$, overlaps with Order 1 at the longer-wavelength end of the traces. Order 3 peaks at ~0.6 $ \\mu m$, though it is very faint.\n",
    "\n",
    "For bright targets that would saturate the detector with the longer readout times of the SUBSTRIP256 subarray, users can elect to use the SUBSTRIP96 array. The 96-pixel vertical dimension of this subarray covers only the Order 1. For more information about the optics and spectral traces, see the [GR700X grism documentation](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-gr700xd-grism).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf72b67-d3af-48a9-ac22-750ca0314873",
   "metadata": {},
   "source": [
    "Although we did not extract a spectrum from it, it is interesting to look at the F277W spectral image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0250e40-d228-4445-8a10-16c53af78257",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (doviz & (len(f277wfiles) > 0)):\n",
    "    display_soss_im(f277wfiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d126af-4d6e-4307-8d16-f0ea93a54c10",
   "metadata": {},
   "source": [
    "Compared to the CLEAR image above, we can clearly see that the Order 0 trace extends over only a small portion of the subarray, and Orders 2 and 3 are entirely absent. This is expected, because the left side of the subarray (in DMS orientation) corresponds to the longer wavelengths that SOSS is sensitive to, and the F277W filter covers only 2.413 $\\mu m$ to 3.143 $\\mu m$. This allows users to isolate the first order spectrum in the range where the first and second order overlap considerably. \n",
    "\n",
    "We can also see several potentially contaminating zeroth-order sources in the image (the Order 0 for our target does not fall on the detector). Characterizing these sources in the F277W image can allow users to model and remove them in the full wavelength coverage spectra, although this capability is not currently included in the pipeline. Best practices for obtaining an F277W exposure are included in the [SOSS Recommended Strategies](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-observing-strategies/niriss-soss-recommended-strategies#NIRISSSOSSRecommendedStrategies-f277wAddinganoptionalF277Wexposuretoyourobservingprogram).\n",
    "\n",
    "The 1/f noise (vertical striping) is also more apparent in this exposure because of the lower signal relative to the background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcafb33-bdb6-493d-98ca-c7827b18b604",
   "metadata": {},
   "source": [
    "### Examine Spec2 products\n",
    "\n",
    "Next we will look at the outputs of the second pipeline stage: the calibrated `calints` images and the preliminary `x1dints` extracted spectra. For more information about the structure and contents of these data products, see the documentation about [x1d/x1dints](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html#extracted-1-d-spectroscopic-data-x1d-and-x1dints) and [calints](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html#calibrated-data-cal-and-calints) files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa822c66-56a3-4dbc-8853-cf7f3f25b5b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    display_soss_im(calintsfiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d2dbf-ae2c-47db-bd13-cefbec773ecf",
   "metadata": {},
   "source": [
    "The SOSS sky background is characterized by a smooth rising gradient towards longer wavelengths, with a sharp discontinuity at around X=700 (corresponding to 2.1 $\\mu m$ in Order 1) caused by the edges of the pick-off mirror (POM). The background varies with sky position and relative pointing due to the contribution of zodiacal light. Because of the variation in relative intensity on either side of the discontinuity, an empirical model cannot be simply linearly scaled to match a given observation. However, it is important to remove the background for most SOSS science use cases, so in the absence of a contemporaneous background exposure, the Spec2 `bkg_subtract` step finds the best match for each exposure from a library of empirical models scaled independently on either side of the discontinuity. \n",
    "\n",
    "We can see that the background is considerably lower in the background-subtracted `calints` image compared to the previous `rateints` image, and the discontinuity is not as prominent, though some structure remains. The NIRISS team intends to continue improving the background subtraction algorithm in the future.\n",
    "\n",
    "Now, let's look at one of the extracted spectra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f50339-bbef-44d4-90c3-0f21e1995416",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    display_spectrum(x1d_spec2files[0], order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821bf8c-3988-4460-ad57-cddecc6a0d1f",
   "metadata": {},
   "source": [
    "Note that not all of the \"features\" of this spectrum are absorption lines; many are due to bad pixels in the spectrum. As mentioned in the [TSO3 Pipeline](#7.-TSO3-Pipeline) section, there is an optional pixel replacement step that may reduce this type of noise.\n",
    "\n",
    "This is the first integration of the time series, but we are especially interested in how the spectrum changes over the duration of the observations. Let's look at the spectrum as a function of both wavelength and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb9ba4-c074-4211-8245-b518aaf18168",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    spectrum_timeseries(x1d_spec2files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc2bdd-e9bb-43b3-af02-bbefa31b88f8",
   "metadata": {},
   "source": [
    "In this plot, the flux at each wavelength is normalized so that the decrease in flux caused by the transiting exoplanet is emphasized. \n",
    "\n",
    "If we pass the argument `normalize=False` to the plotting function, the transit is much less obvious but we can see some interesting spectral features, as well as how the flux decreases at longer wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec94487-c429-43b1-9f87-b04bcfd3cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    spectrum_timeseries(x1d_spec2files, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e7873-ab3d-42db-9420-be9f8398c148",
   "metadata": {},
   "source": [
    "### Examine TSO3 products\n",
    "\n",
    "Lastly, we will examine the products of the third and final pipeline stage: the final `x1dints` spectra and the white light (`whtlt`) curve for orders 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbb74d-00dd-4e0d-bec1-c643e151643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    display_spectrum(x1d_tso3file, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf4d5b-bf35-4580-8e17-7a1b0cdfca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    display_spectrum(x1d_tso3file, order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59246e-6809-4334-b6df-4f4a743f5e5c",
   "metadata": {},
   "source": [
    "By eye, there is not much difference apparent between the Stage 2 x1dints files and the Stage 3 x1dints files; the main difference is that the outlier detection step has now been run. However, it is worth noting that in Order 2, measurements beyond ~1 $\\mu m$ are not reliable due to contamination from Order 1.\n",
    "\n",
    "We can also make the 2d transit plot again, using the final extracted spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45f370-fcbf-49a0-be2e-698edf6b79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    spectrum_timeseries(x1d_tso3file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b642ed-59c2-439e-bd65-25b1c4c12d56",
   "metadata": {},
   "source": [
    "Lastly, let's look at the white light curve. In this file, the flux at each wavelength has been summed to give us a single value at each timestamp. \n",
    "\n",
    "We will plot the white light curve for orders 1 and 2, along with a simple transit model using the `batman` package ([Kreidberg 2015](https://ui.adsabs.harvard.edu/abs/2015PASP..127.1161K/abstract)). We adopt all of our planetary parameters from [McGruder et al. 2023](https://ui.adsabs.harvard.edu/abs/2023ApJ...944L..56M/abstract), except for the ratio of the planet radius to the stellar radius ($R_{p}/R_{*}$), which is from [Kokori et al. 2023](https://ui.adsabs.harvard.edu/abs/2023ApJS..265....4K/abstract)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fe959-143a-41c7-8ecc-27cede53addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if doviz:\n",
    "    # Set up the model parameters\n",
    "    params = batman.TransitParams()\n",
    "    params.t0 = 2456258.06272 - 2400000.5       # time of inferior conjunction\n",
    "    params.per = 3.4252567                        # orbital period\n",
    "    params.rp = 0.1175                            # planet radius (in units of stellar radii)\n",
    "    params.a = 9.13                               # semi-major axis (in units of stellar radii)\n",
    "    params.inc = 85.45                            # orbital inclination (in degrees)\n",
    "    params.ecc = 0.                               # eccentricity\n",
    "    params.w = 90.                                # longitude of periastron (in degrees)\n",
    "    params.u = [0.16, 0.26]                       # limb darkening coefficients [u1, u2]\n",
    "    params.limb_dark = \"quadratic\"                # limb darkening model\n",
    "\n",
    "    # Display the light curves and models\n",
    "    display_lightcurve(whtltfile, order=[1, 2], modelparams=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaba93-b553-4dc4-8103-b5c2fcbae7ef",
   "metadata": {},
   "source": [
    "Since the model parameters were derived from a different reduction that used the same data, it makes sense that this model looks like a pretty good fit to the data by eye. Further model fitting is beyond the scope of this notebook, but can also be done using `batman` or a variety of other open-source software packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee8944",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303c0e7-13af-4e4c-bed7-8b85cc8e3645",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_footer.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
