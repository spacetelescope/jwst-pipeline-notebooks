{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3778dea5-4311-4285-af93-a4e2eb5fb192",
   "metadata": {},
   "source": [
    "# NIRSpec BOTS Pipeline Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77365d40-4256-42ce-a35e-bcc9e8e22b35",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764eb72-d523-4653-8471-d3c9c155b41c",
   "metadata": {},
   "source": [
    "**Authors**: Nikolay Nikolov (AURA Associate Scientist, nnikolov@stsci.edu); Kayli Glidic (kglidic@stsci.edu); NIRSpec branch</br>\n",
    "**Last Updated**: November 7, 2025</br>\n",
    "**Pipeline Version**: 1.20.2 (Build 12.1.1, Context jwst_1464.pmap)\n",
    "\n",
    "**Purpose**:<br>\n",
    "End-to-end calibration with the James Webb Space Telescope (JWST) pipeline is divided into three main processing stages. This notebook provides a framework for processing generic Near-Infrared Spectrograph (NIRSpec) Bright Object Time-Series (BOTS) data through [stages 1-3 of the JWST pipeline](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing#gsc.tab=0), including how to use associations for multi-exposure observations and how to interact and work with JWST datamodels. In most cases, editing cells outside the [Configuration](#1.-Configuration) section is unnecessary unless the standard pipeline processing options or plot parameters need to be modified.\n",
    "\n",
    "**[Data](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))**:<BR>\n",
    "This notebook is set up to use transit observations of WASP-39b with the G395H grism, obtained by Proposal ID (PID) 1366, Observation 3. The demo data will automatically download unless disabled (i.e., to use local files instead).\n",
    " \n",
    "**[JWST pipeline version and CRDS context](#Set-CRDS-Context-and-Server)**:<BR>\n",
    "This notebook was written for the above-specified pipeline version and associated build context for this version of the JWST Calibration Pipeline. Information about this and other contexts can be found in the JWST Calibration Reference Data System (CRDS [server](https://jwst-crds.stsci.edu/)). If you use different pipeline versions, please refer to the table [here](https://jwst-crds.stsci.edu/display_build_contexts/) to determine what context to use. To learn more about the differences for the pipeline, read the relevant [documentation](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/jwst-operations-pipeline-build-information#references).\n",
    "\n",
    "Please note that pipeline software development is a continuous process, so results in some cases may be slightly different if a subsequent version is used. **For optimal results, users are strongly encouraged to reprocess their data using the most recent pipeline version and [associated CRDS context](https://jwst-crds.stsci.edu/display_build_contexts/), taking advantage of bug fixes and algorithm improvements.** Any [known issues](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/nirspec-known-issues/nirspec-bots-known-issues#gsc.tab=0) for this build are noted in the notebook.\n",
    "\n",
    "**Updates**:<BR>\n",
    "This notebook is regularly updated to incorporate the latest pipeline improvements. Find the most up-to-date version of this notebook [here](https://github.com/spacetelescope/jwst-pipeline-notebooks/). \n",
    "\n",
    "**Recent Changes**:</br>\n",
    "* October 15, 2024: Converted notebook to follow standard template ([original](https://github.com/exonik/JWST-NIRSpec-STScI-pipeline-rerun/blob/main/BOTS_pipeline_rerun.ipynb)).</br>\n",
    "* November 6, 2024: Notebook updated to JWST pipeline version 1.16.0 (Build 11.1).\n",
    "* February 3, 2025: Notebook updated to JWST pipeline version 1.17.1 (Build 11.2)\n",
    "* April 18, 2025: Notebook updated to JWST pipeline version 1.18.0 (Build 11.3) added curved trace extraction.\n",
    "* July 16, 2025: Updated to JWST pipeline version 1.19.1 (update plotting to work with new spectral data table format).\n",
    "* November 7, 2025: Updated to JWST pipeline version 1.20.2 (no significant changes in pipeline).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503d59d-a856-4e41-90f9-6e9745fb78b6",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Configuration](#1.-Configuration)\n",
    "* [2. Package Imports](#2.-Package-Imports)\n",
    "* [3. Demo Mode Setup](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "* [4. Directory Setup](#4.-Directory-Setup)\n",
    "* [5. Stage 1: `Detector1Pipeline` (`calwebb_detector1`)](#5.-Stage-1:-Detector1Pipeline-(calwebb_detector1))\n",
    "    * [5.1 Configure `Detector1Pipeline`](#5.1-Configure-Detector1Pipeline)\n",
    "    * [5.2 Run `Detector1Pipeline`](#5.2-Run-Detector1Pipeline)\n",
    "* [6. Stage 2: `Spec2Pipeline` (`calwebb_spec2`)](#5.-Stage-2:-Spec2Pipeline-(calwebb_spec2))\n",
    "    * [6.1 Configure `Spec2Pipeline`](#6.1-Configure-Spec2Pipeline)\n",
    "    * [6.2 Create `Spec2Pipeline` ASN Files](#6.2-Create-Spec2Pipeline-ASN-Files)\n",
    "    * [6.3 Run `Spec2Pipeline`](#6.3-Run-Spec2Pipeline)\n",
    "* [7. Stage 3: `Tso3Pipeline` (`calwebb_tso3`)](#5.-Stage-3:-Tso3Pipeline-(calwebb_tso3))\n",
    "    * [7.1 Configure `Tso3Pipeline`](#7.1-Configure-Tso3Pipeline)\n",
    "    * [7.2 Create `Tso3Pipeline` ASN Files](#7.2-Create-Tso3Pipeline-ASN-Files)\n",
    "    * [7.3 Run `Tso3Pipeline`](#7.3-Run-Tso3Pipeline)\n",
    "* [8. Visualizing the Data](#8.-Vusualizing-the-Data)\n",
    "    * [8.1 Display `Detector1Pipeline` Products](#8.1-Display-Detector1Pipeline-Products)\n",
    "    * [8.2 Display `Spec2Pipeline` Products](#8.2-Display-Spec2Pipeline-Products)\n",
    "    * [8.3 Display `Tso3Pipeline` Products](#8.3-Display-Tso3Pipeline-Products)\n",
    "* [9. Modifying the EXTRACT1D Reference File (as needed)](#9.-Modifying-the-EXTRACT1D-Reference-File-(as-needed))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5df04a-b989-4b78-9bab-0b02d32b5924",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "### Install dependencies and parameters\n",
    "\n",
    "To make sure that the pipeline version is compatible with the steps discussed below and that the required dependencies and packages get installed, you can create a fresh Conda environment and install the provided requirements.txt file before starting this notebook:\n",
    "\n",
    "    conda create -n nirspec_bots_pipeline python=3.12\n",
    "    conda activate nirspec_bots_pipeline\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "Set the basic parameters to configure the notebook. These parameters determine what data gets used and where the data is located (if already on disk). The list of parameters includes:\n",
    "\n",
    "* `demo_mode`:\n",
    "    * `True`: Downloads example data from the [Barbara A. Mikulski Archive for Space Telescopes (MAST)](https://archive.stsci.edu/) and processes it through the pipeline. All processing will occur in a local directory unless modified in [Section 3](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data)) below.\n",
    "    * `False`: Process your own downloaded data; provide its location.\n",
    "* **Directories with data**:\n",
    "    * `sci_dir`: Directory where science observation data is stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cfffc-39f7-49dc-891f-ea41e873cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import necessary for configuration.\n",
    "# Uncomment logging to hide log information.\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "#import logging\n",
    "\n",
    "# Control logging level: INFO, WARNING, ERROR\n",
    "# Run command logging.disable if want to hide logging\n",
    "# ERROR messages.\n",
    "#logging.disable(logging.ERROR)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b445518-8dee-488c-982a-4306fb784fd4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Note that `demo_mode` must be set appropriately below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e548674-591e-4cfa-bdee-7a3951cc45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for demo_mode, data mode directories, and processing steps.\n",
    "\n",
    "# -------------------------------DEMO MODE-----------------------------------\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode using online example data!')\n",
    "\n",
    "# ----------------------------User Mode Directories--------------------------\n",
    "else:  # If demo_mode = False, look for user data in these paths.\n",
    "\n",
    "    # Set directory paths for processing specific data; adjust to your local\n",
    "    # directory setup (examples provided below).\n",
    "    basedir = os.path.abspath(os.path.join(os.getcwd(), ''))\n",
    "\n",
    "    # Directory to science observation data; expects uncalibrated data in\n",
    "    # sci_dir/uncal/ and results in stage1, stage2, and stage3 directories.\n",
    "    sci_dir = os.path.join(basedir, 'bots_data_01366/Obs003', '')\n",
    "\n",
    "# ---------------------------Set Processing Steps----------------------------\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# Science processing.\n",
    "dodet1 = True  # calwebb_detector1\n",
    "dospec2 = True  # calwebb_spec2\n",
    "dotso3 = True  # calwebb_tso3\n",
    "doviz = True  # Visualize calwebb outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e0c7d8-67d8-45a7-8b0d-14f19bbc48ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Set CRDS Context and Server\n",
    "\n",
    "Before importing `CRDS` and `JWST` modules, we need to configure our environment. This includes defining a CRDS cache directory in which to keep the reference files that will be used by the calibration pipeline. If the local CRDS cache directory has not been set, it will automatically be created in the home directory.\n",
    "\n",
    "[Build Context Table](https://jwst-crds.stsci.edu/display_build_contexts/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d430a0-81ab-4ddd-a570-0d8c038b2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Set CRDS context and paths------------------------\n",
    "# Each version of the calibration pipeline is associated with a specific CRDS\n",
    "# context file. The pipeline will select the appropriate context file behind\n",
    "# the scenes while running. However, if you wish to override the default context\n",
    "# file and run the pipeline with a different context, you can set that using\n",
    "# the CRDS_CONTEXT environment variable. Here we show how this is done,\n",
    "# although we leave the line commented out in order to use the default context.\n",
    "# If you wish to specify a different context, uncomment the line below.\n",
    "#os.environ['CRDS_CONTEXT'] = 'jwst_1413.pmap'  # CRDS context for 1.19.1\n",
    "\n",
    "# Set CRDS cache directory to user home if not already set.\n",
    "if os.getenv('CRDS_PATH') is None:\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds_cache')\n",
    "\n",
    "# Check whether the CRDS server URL has been set. If not, set it.\n",
    "if os.getenv('CRDS_SERVER_URL') is None:\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Output the current CRDS path and server URL in use.\n",
    "print('CRDS local filepath:', os.environ['CRDS_PATH'])\n",
    "print('CRDS file server:', os.environ['CRDS_SERVER_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d6145-a051-4ee0-929b-03a801642852",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Package Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e0b5f-7d49-4d2c-99f3-531322338a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire available screen width for this notebook.\n",
    "from IPython.display import display, HTML, JSON\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb1544-c55e-4b35-aef1-910a47f8ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------General Imports----------------------\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Astroquery Imports ----------------------\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# ----------------------Astropy Imports----------------------\n",
    "# Astropy utilities for opening FITS files, downloading demo files, etc.\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.visualization import ImageNormalize, ManualInterval, LogStretch\n",
    "from astropy.visualization import LinearStretch, AsinhStretch, simple_norm\n",
    "\n",
    "# ----------------------Plotting Imports---------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from jwst.extract_1d.extract import location_from_wcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd224efc-b5e2-4b6a-891d-3ac39177885c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Installation instructions for the JWST pipeline found here: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview) • \n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io) • \n",
    "[Github](https://github.com/spacetelescope/jwst)\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797f264-6dfe-46b3-ac07-cdec10656b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------JWST Calibration Pipeline Imports----------------------\n",
    "import jwst  # Import the base JWST and CRDS packages.\n",
    "import crds\n",
    "from crds.client import api\n",
    "from stpipe import crds_client\n",
    "\n",
    "# JWST pipelines (each encompassing many steps).\n",
    "from jwst.pipeline import Detector1Pipeline  # calwebb_detector1\n",
    "from jwst.pipeline import Spec2Pipeline  # calwebb_spec2\n",
    "from jwst.pipeline import Tso3Pipeline  # calwebb_tso3\n",
    "from jwst.extract_1d import Extract1dStep  # Extract1D Step\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels  # JWST pipeline utilities: datamodels.\n",
    "from jwst.associations import asn_from_list as afl  # Tools for creating association files.\n",
    "from jwst.associations.lib.rules_level2b import Asn_Lv2SpecTSO\n",
    "from jwst.associations.lib.rules_level3 import DMS_Level3_Base\n",
    "\n",
    "# Check the default context for the Pipeline version\n",
    "default_context = crds.get_default_context('jwst', state='build')\n",
    "print(\"JWST Calibration Pipeline Version = {}\".format(jwst.__version__))\n",
    "print(f\"Default CRDS Context for JWST Version {jwst.__version__}: {default_context}\")\n",
    "print(f\"Using CRDS Context: {os.environ.get('CRDS_CONTEXT', default_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7124de5c",
   "metadata": {},
   "source": [
    "---\n",
    "### Define Convenience Functions\n",
    "\n",
    "Define a function that filters files based on detector, filter, grating, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching(files, detector, filt, grating, fxd_slit, exp_type):\n",
    "    \"\"\"\n",
    "    Filters a list of FITS files to find those with matching \n",
    "    detector, filter, and grating for a specified exposure type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list of str\n",
    "        Paths to FITS files to check.\n",
    "    detector : str\n",
    "        Expected value of the DETECTOR keyword.\n",
    "    filt : str\n",
    "        Expected value of the FILTER keyword.\n",
    "    grating : str\n",
    "        Expected value of the GRATING keyword.\n",
    "    fxd_slit : str\n",
    "        Fixed slit name.\n",
    "    exp_type : str, optional\n",
    "        The exposure type to match.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    files_regular : list of str\n",
    "        Files with matching configuration and IS_IMPRT == False or missing.\n",
    "    files_imprint : list of str)\n",
    "        Files with matching configuration and IS_IMPRT == True.\n",
    "    \"\"\"\n",
    "    files_regular, files_imprint = [], []\n",
    "    for file in files:\n",
    "        # Skip if EXP_TYPE doesn't match the provided one.\n",
    "        if fits.getval(file, 'EXP_TYPE') != exp_type:\n",
    "            files_regular.append(file)\n",
    "            continue\n",
    "        # Check if DETECTOR, FILTER, GRATING, and SLIT match\n",
    "        detector_match = fits.getval(file, 'DETECTOR') == detector\n",
    "        filter_match = fits.getval(file, 'FILTER') == filt\n",
    "        grating_match = fits.getval(file, 'GRATING') == grating\n",
    "        slit_match = fits.getval(file, 'FXD_SLIT') == fxd_slit\n",
    "        if detector_match and filter_match and grating_match and slit_match:\n",
    "            # Only IFU and MOS observations have imprint exposures.\n",
    "            try:\n",
    "                is_imprt = fits.getval(file, 'IS_IMPRT')\n",
    "            except KeyError:\n",
    "                is_imprt = None\n",
    "            (files_imprint if is_imprt else files_regular).append(file)\n",
    "    return files_regular, files_imprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime.\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dacda3c-253a-4d99-8f34-3744c02eccea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Demo Mode Setup (ignore if not using demo data)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The data in this notebook is public and does not require a token. For other data sets, you may need to provide a token. For more infomation visit the [astroquery](https://astroquery.readthedocs.io/en/latest/index.html) documentation.\n",
    "</div> \n",
    "\n",
    "If running in demonstration mode, set up the program information to retrieve the uncalibrated data (`_uncal.fits`) automatically from MAST using `astroquery`. MAST provides flexibility by allowing searches based on proposal ID and observation ID, rather than relying solely on filenames. More information about the JWST file naming conventions can be found [here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html).\n",
    "\n",
    "The BOTS demo data in this notebook is from the [JWST Early Release Science (ERS) program 1366](https://www.stsci.edu/jwst/science-execution/approved-programs/dd-ers/program-1366) and features transit observations of WASP-39b using the G395H grism. The program setup is briefly summarized in the table below.\n",
    "\n",
    "| Demo Target: WASP-39b |       |   | \n",
    "|:-----------:|:-------:|:---:|\n",
    "| PROGRAM | 01366 |  Program number | \n",
    "| OBSERVTN | 003 | Observation number | \n",
    "| [GRATING/FILTER](https://jwst-docs.stsci.edu/jwst-near-infrared-spectrograph/nirspec-observing-modes/nirspec-bright-object-time-series-spectroscopy#gsc.tab=0) | G395H/F290LP | λ: 2.87–5.14 μm (a high resolution, R ~ 2700) |\n",
    "| SUBARRAY | SUB2048 | Subarray used (2048x32 pixels per integration, per group) | \n",
    "| NINTS | 465 |  Number of integrations in exposure  |           \n",
    "| NGROUPS | 70 | Number of groups in integration  |\n",
    "| DURATION | 29789.053 [s] | Total duration of one exposure |   \n",
    "| READPATT | NRSRAPID | Readout pattern |  \n",
    "| PATTTYPE | NONE | Primary dither pattern type |  \n",
    "| NUMDTHPT | 1 |  Total number of points in pattern |                           \n",
    "| SRCTYAPT | UNKNOWN | Source type selected in APT | \n",
    "\n",
    "> **Note:** The presence of a physical gap between detectors affects high-resolution BOTS observations because the spectra are long enough to span both NIRSpec detectors. [More Info ...](https://jwst-docs.stsci.edu/jwst-near-infrared-spectrograph/nirspec-operations/nirspec-bots-operations/nirspec-bots-wavelength-ranges-and-gaps#gsc.tab=0)\n",
    ">\n",
    "\n",
    "Many TSO exposures may contain a sufficiently large number of integrations (NINTS) so as to make their individual exposure products too large (in terms of file size on disk) to be able to handle conveniently. In these cases, the uncalibrated raw data for a given exposure are split into multiple “segmented” products, each of which is identified with a segment number (see segmented products). \n",
    "\n",
    "Information about existing and planned JWST TSO programs for transiting exoplanets, including with the NIRSpec BOTS mode, can be obtained from [TrExoLiSTS](https://www.stsci.edu/~nnikolov/TrExoLiSTS/JWST/trexolists.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839c792-4bd3-47ff-84a9-c597f5512d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the program information for demo mode.\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode. '\n",
    "          'Example data will be downloaded from MAST!')\n",
    "\n",
    "    # NOTE:\n",
    "    # The data in this notebook is public and does not require a token.\n",
    "    # For other data sets, you may need to provide a token.\n",
    "    # Observations.login(token=None)\n",
    "\n",
    "    # --------------Program and observation information--------------\n",
    "    program = \"01366\"\n",
    "    sci_observtn = \"003\"\n",
    "    bg_observtn = None\n",
    "    filters = [\"F290LP;G395H\"]\n",
    "\n",
    "    # ----------Define the base and observation directories----------\n",
    "    basedir = os.path.abspath(os.path.join(os.getcwd(), ''))\n",
    "    sci_dir = os.path.join(basedir, f'Obs{sci_observtn}')\n",
    "    uncal_dir = os.path.join(sci_dir, 'uncal/')\n",
    "\n",
    "    os.makedirs(uncal_dir, exist_ok=True)\n",
    "else:\n",
    "    print('Running with user provided data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab021e-231f-4541-8c6e-4f6edd45370c",
   "metadata": {},
   "source": [
    "<br>Click on the following links to learn more about querying and downloading data:<br>\n",
    "• [Downloading data](https://astroquery.readthedocs.io/en/latest/mast/mast_obsquery.html#downloading-data)<br>\n",
    "• [Observations Class](https://astroquery.readthedocs.io/en/latest/api/astroquery.mast.ObservationsClass.html)<br>\n",
    "• [Products Field Descriptions](https://mast.stsci.edu/api/v0/_productsfields.html)<br><br>\n",
    "\n",
    "Compile a table of files from MAST associated with the science (SCI) observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd0d0d-787a-4ed4-9e31-e77702a407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of observation IDs for the specified demo program.\n",
    "\n",
    "if demo_mode:\n",
    "    # --------------------SCIENCE Observation--------------------\n",
    "    sci_obs_id_table = Observations.query_criteria(instrument_name=['NIRSPEC/SLIT'],\n",
    "                                                   provenance_name=[\"CALJWST\"],\n",
    "                                                   obs_id=[f'*{program}*{sci_observtn}*'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427eb0-838f-4b01-b4db-1a1c64354cae",
   "metadata": {},
   "source": [
    "Filter these tables to identify uncalibrated data and their association files for download from MAST.\n",
    "\n",
    "The demo dataset consists of six segments of `_uncal.fits` files, each approximately 1.42 GB in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f04313-0047-4479-8bcc-0e8402135e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert visits into a list of uncalibrated data and ASN files.\n",
    "\n",
    "if demo_mode:\n",
    "    file_criteria = {'filters': filters, 'calib_level': [1],\n",
    "                     'productSubGroupDescription': 'UNCAL'}\n",
    "\n",
    "    # Initialize lists for science, background, and ASN files.\n",
    "    sci_downloads = []\n",
    "\n",
    "    pfilter = Observations.filter_products  # Alias for filter_products method.\n",
    "\n",
    "    # ----------Identify uncalibrated SCIENCE files associated with each visit----------\n",
    "    for exposure in sci_obs_id_table:\n",
    "        sci_products = Observations.get_product_list(exposure)\n",
    "\n",
    "        # Filter for full-size science files (exclude smaller confirmation images).\n",
    "        avg_sci_size = np.nanmean(sci_products['size'])\n",
    "        sci_products = sci_products[sci_products['size'] > avg_sci_size]\n",
    "        sci_downloads.extend(pfilter(sci_products, **file_criteria)['dataURI'])\n",
    "\n",
    "    # Filter out other observations and remove duplicates.\n",
    "    sci_downloads = {f for f in sci_downloads if f\"jw{program}{sci_observtn}\" in f}\n",
    "\n",
    "    print(f\"Science files selected for downloading: {len(sci_downloads)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a8934",
   "metadata": {},
   "source": [
    "Downoload the data\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "**Warning:** If this notebook is halted during this step, the downloaded file may be incomplete, and cause crashes later on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d230c8-49d6-4ddf-8a7e-fb804d9ce98f",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Download data and place them into the appropriate directories.\n",
    "if demo_mode:\n",
    "    for file in sci_downloads:\n",
    "        sci_manifest = Observations.download_file(file, local_path=uncal_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e43f9",
   "metadata": {},
   "source": [
    "## 4. Directory Setup\n",
    "\n",
    "Set up detailed paths to input/output stages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37d892-a0d6-4ddd-9f24-fde118c48ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define/create output subdirectories to keep data products organized.\n",
    "\n",
    "# -----------------------------Science Directories------------------------------\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal/')  # Uncalibrated pipeline inputs.\n",
    "det1_dir = os.path.join(sci_dir, 'stage1/')  # calwebb_detector1 pipeline outputs.\n",
    "spec2_dir = os.path.join(sci_dir, 'stage2/')  # calwebb_spec2 pipeline outputs.\n",
    "tso3_dir = os.path.join(sci_dir, 'stage3/')  # calwebb_tso3 pipeline outputs.\n",
    "asn_dir = os.path.join(sci_dir, 'asn/')  # Association directory.\n",
    "\n",
    "os.makedirs(det1_dir, exist_ok=True)\n",
    "os.makedirs(spec2_dir, exist_ok=True)\n",
    "os.makedirs(tso3_dir, exist_ok=True)\n",
    "os.makedirs(asn_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark.\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time1 - time0) / 60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c91e18-74f9-4074-81da-36057bbc03e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Stage 1: `Detector1Pipeline` (`calwebb_detector1`)\n",
    "\n",
    "In this section, we process the data through the `calwebb_detector1` pipeline to create Stage 1 [data products](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html).\n",
    "\n",
    "* **Input**: Raw exposure (`_uncal.fits`) containing original data from all detector readouts (ncols x nrows x ngroups x nintegrations).\n",
    "* **Output**: Uncalibrated countrate (slope) image in units of DN/s:\n",
    "    * `_rate.fits`: A single countrate image averaged over multiple integrations (if available).\n",
    "    * `_rateints.fits`: Countrate images for each integration, saved in multiple extensions.\n",
    "\n",
    "The `Detector1Pipeline` applies basic detector-level corrections on a group-by-group basis, followed by ramp fitting for all exposure types, commonly referred to as \"ramps-to-slopes\" processing. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7482e-1f7c-47bb-88d7-d67e80da39eb",
   "metadata": {},
   "source": [
    "### 5.1 Configure `Detector1Pipeline`\n",
    "\n",
    "The `Detector1Pipeline` has the following steps available for NIRSpec BOTS:\n",
    "\n",
    "> * `group_scale` : Rescales pixel values to correct for improper onboard frame averaging.\n",
    "> * `dq_init` : Initializes the data quality (DQ) flags for the input data.\n",
    "> * `saturation` : Flags pixels at or below the A/D floor or above the saturation threshold.\n",
    "> * `superbias` : Subtracts the superbias reference file from the input data.\n",
    "> * `refpix` : Use reference pixels to correct bias drifts.\n",
    "> * `linearity` : Applies a correction for non-linear detector response.   \n",
    "> * `dark_current` : Subtracts the dark current reference file from the input data.\n",
    "> * `jump` : Performs CR/jump detection on each ramp integration within an exposure.\n",
    "> * `ramp_fit` : Determines the mean count rate (counts per second) for each pixel by performing a linear fit to the input data.\n",
    "> * `gain_scale` : Corrects pixel values for non-standard gain settings, primarily in NIRSpec subarray data.\n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_detector1) •\n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/stable/jwst/pipeline/calwebb_detector1.html)\n",
    "\n",
    "Below, we set up a dictionary that defines how the `Detector1Pipeline` should be configured for BOTS data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c53bf-e46c-4699-8860-7919497a83f4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  To override specific steps and reference files, use the examples below.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67672cfa-fd6c-4023-8ec2-7c4ecf579226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Detector1 pipeline should be configured.\n",
    "\n",
    "# -------------------------Boilerplate dictionary setup-------------------------\n",
    "det1dict = {}\n",
    "det1dict['group_scale'], det1dict['dq_init'], det1dict['saturation'] = {}, {}, {}\n",
    "det1dict['superbias'], det1dict['refpix'], det1dict['linearity'] = {}, {}, {}\n",
    "det1dict['dark_current'], det1dict['jump'], det1dict['clean_flicker_noise'] = {}, {}, {}\n",
    "det1dict['ramp_fit'], det1dict['gain_scale'] = {}, {}\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "# Overrides for various reference files (example).\n",
    "# Files should be in the base local directory or provide full path.\n",
    "#det1dict['dq_init']['override_mask'] = 'myfile.fits' # Bad pixel mask\n",
    "#det1dict['superbias']['override_superbias'] = 'myfile.fits' # Bias subtraction\n",
    "#det1dict['dark_current']['override_dark'] = 'myfile.fits' # Dark current subtraction\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "# Overrides for whether or not certain steps should be skipped (example).\n",
    "#det1dict['linearity']['skip'] = True  # This is the default.\n",
    "\n",
    "# Turn on multi-core processing (off by default).\n",
    "# Choose what fraction of cores to use (quarter, half, or all).\n",
    "det1dict['jump']['maximum_cores'] = 'quarter'\n",
    "#det1dict['ramp_fit']['maximum_cores'] = 'half'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfa440",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "Many exposures are affected by artifacts known as [snowballs](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/shower-and-snowball-artifacts#gsc.tab=0) caused by large cosmic ray events. These artifacts are particularly significant in deep exposures with long integration times, with an estimated rate of one snowball per detector (FULL FRAME) per 20 seconds. To expand the number of pixels flagged as jumps around large cosmic ray events, set `expand_large_events` to True. An `expand_factor` of 3 works well for NIRSpec observations to cover most snowballs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on detection of cosmic ray snowballs (on by default).\n",
    "det1dict['jump']['expand_large_events'] = True\n",
    "det1dict['jump']['expand_factor'] = 3  # (default 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eaaa2e-c9d1-49e8-91ec-c05266314ff5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "JWST detector readout electronics (a.k.a. SIDECAR ASICs) generate significant 1/f noise during detector operations and signal digitization. This noise manifests as faint banding along the detector's slow axis and varies from column to column. If not handled properly, the 1/f noise can introduce systematic errors and extra scatter in BOTS light curves. For more information, please visit [JWST Time-Series Observations Noise Sources](https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-noise-sources#JWSTTimeSeriesObservationsNoiseSources-1/fnoise&gsc.tab=0).\n",
    "\n",
    "For NIRSpec data, the primary pipeline algorithm to address 1/f noise is `nsclean` in the `Spec2Pipeline` (Rauscher 2023) and is off by default. However, we turned on in Stage 2. An additional 1/f noise-cleaning algorithm, `clean_flicker_noise`, has been implemented at the group stage in the `Detector1Pipeline`. This step is also off by default.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on 1/f noise correction in Stage 1? (off by default).\n",
    "#det1dict['clean_flicker_noise']['skip'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b534d44-079b-4e3b-a0df-05f6ea9bdb5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5.2 Run `Detector1Pipeline`\n",
    "\n",
    "Run the science files through the `calwebb_detector1` pipeline using the `.call()` method. \n",
    "\n",
    "We use `.call()` instead of `.run()` to ensure that the latest default parameters from CRDS are applied ([ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/stpipe/call_via_run.html)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01bdf1-1fec-4f08-92e7-02c6692ad00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final list of UNCAL files ready for Stage 1 processing.\n",
    "uncal_sci = sorted(glob.glob(uncal_dir + '*uncal.fits'))\n",
    "\n",
    "print(f\"Science UNCAL Files:\\n{'-' * 20}\\n\" + \"\\n\".join(uncal_sci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34ac42-1b12-4c54-b77f-b426e9425548",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run Stage 1 pipeline using the custom det1dict dictionary.\n",
    "\n",
    "if dodet1:\n",
    "    # ---------------------Science UNCAL files---------------------\n",
    "    for uncal_file in sorted(glob.glob(uncal_dir + '*uncal.fits')):\n",
    "        print(f\"Applying Stage 1 Corrections & Calibrations to: \"\n",
    "              f\"{os.path.basename(uncal_file)}\")\n",
    "\n",
    "        det1_result = Detector1Pipeline.call(uncal_file,\n",
    "                                             save_results=True,\n",
    "                                             steps=det1dict,\n",
    "                                             # To save calibrated ramps set to True.\n",
    "                                             save_calibrated_ramp=False,  # Default True.\n",
    "                                             output_dir=det1_dir)\n",
    "\n",
    "        print(\"... Stage 1 has been completed!\\n\")\n",
    "else:\n",
    "    print('Skipping Detector1 processing for SCI data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68097f8-2405-4fab-b1c9-f8d44baaea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final list of RATE[INTS] files ready for Stage 2 processing.\n",
    "rate_sci = sorted(glob.glob(det1_dir + '*_rateints*.fits'))\n",
    "print(f\"SCIENCE | RATE[INTS] Files:\\n{'-' * 20}\\n\" + \"\\n\".join(rate_sci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark.\n",
    "time2 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time2 - time0) / 60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6dc82-5a19-4e3f-9b34-8f7b59770e68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Stage 2: `Spec2Pipeline` (`calwebb_spec2`)\n",
    "\n",
    "In this section, we process our countrate (slope) image products from Stage 1 (`calwebb_detector1`) through the Spec2 (`calwebb_spec2`) pipeline to create Stage 2 [data products](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html).\n",
    "\n",
    "* **Input**: A single countrate (slope) image (`_rate[ints].fits`) or an association file listing multiple inputs.\n",
    "* **Output**: Calibrated products (rectified and unrectified) and 1D spectra.\n",
    "\t* `_cal[ints].fits`: Calibrated 2D (unrectified) spectra (ncols x nrows).\n",
    "\t* `_x1d[ints].fits`: Extracted 1D spectroscopic data (wavelength vs. flux).\n",
    "      \n",
    "The `Spec2Pipeline` applies additional instrumental corrections and calibrations (e.g., slit loss, path loss, etc.,) to countrate products that result in a fully calibrated individual exposure (per segment). The `Spec2Pipeline` also converts countrate products from units of DN/s to flux (Jy) for point sources and surface brightness (MJy/sr) for extended sources.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112e1ee-0bf2-4451-b9c2-4d08312b7511",
   "metadata": {},
   "source": [
    "### 6.1 Configure `Spec2Pipeline`\n",
    "\n",
    "The `Spec2Pipeline` has the following steps available for NIRSpec BOTS:\n",
    "\n",
    "* `assign_wcs`: Assigns wavelength solution for spectra.\n",
    "* `badpix_selfcal`: Flags bad pixels in the input data using a self-calibration technique based on median filtering along the spectral axis.\n",
    "* `nsclean`: Cleans 1/f noise.\n",
    "* `extract_2d` : Extracts 2D arrays from spectral images.\n",
    "* `srctype`: Determines whether a spectroscopic source should be classified as a point or extended object.\n",
    "* `flat_field`: Applies flat-field corrections to the input science dataset.\n",
    "* `photom`: Applies photometric calibrations to convert data from countrate to surface brightness or flux density.\n",
    "* `pixel_replace`: Interpolates and estimates flux values for pixels flagged as DO_NOT_USE in 2D extracted spectra.\n",
    "* `extract_1d`: Extracts a 1D signal from 2D or 3D datasets.\n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_spec2) •\n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html)\n",
    "\n",
    "Below, we set up a dictionary that defines how the `Spec2Pipeline` should be configured for BOTS data.\n",
    "\n",
    "We opt to skip the `flat_field` and `photom` steps, as they can introduce scatter in the final light curves. This scatter arises from uncertainties in the limited number of flat field frames used to generate the flat field reference file, as well as uncertainties in the throughput used for converting the extracted spectra from relative to absolute units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4732aa-8e32-4a4e-b9c2-bbc6d1eedd06",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "To override specific steps and reference files, use the examples below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029da5ac-ec25-4540-a1e8-01a71adabe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec2 pipeline should be configured.\n",
    "\n",
    "# -------------------------Boilerplate dictionary setup-------------------------\n",
    "spec2dict = {}\n",
    "spec2dict['assign_wcs'], spec2dict['badpix_selfcal'] = {}, {}\n",
    "spec2dict['nsclean'] = {}\n",
    "spec2dict['extract_2d'], spec2dict['srctype'] = {}, {}\n",
    "spec2dict['flat_field'], spec2dict['photom'] = {}, {}\n",
    "spec2dict['pixel_replace'], spec2dict['extract_1d'] = {}, {}\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "# Overrides for various reference files (example).\n",
    "# Files should be in the base local directory or provide full path.\n",
    "# spec2dict['extract_1d']['override_extract1d'] = 'myfile.json'\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "# Overrides for whether or not certain steps should be skipped.\n",
    "# Skip this step, because it can increase the light curve scatter.\n",
    "spec2dict['flat_field']['skip'] = True\n",
    "spec2dict['photom']['skip'] = True  # Skip this; BOTS observations are relative.\n",
    "\n",
    "# For Brown dwarf observation uncomment the following.\n",
    "#spec2dict['flat_field']['skip'] = False\n",
    "#spec2dict['photom']['skip'] = False\n",
    "\n",
    "# Run pixel replacement code to extrapolate values for otherwise bad pixels.\n",
    "# This can help mitigate 5-10% negative dips in spectra of bright sources.\n",
    "# Use the 'fit_profile' algorithm.\n",
    "spec2dict['pixel_replace']['skip'] = False\n",
    "spec2dict['pixel_replace']['n_adjacent_cols'] = 5\n",
    "spec2dict['pixel_replace']['algorithm'] = 'fit_profile'\n",
    "\n",
    "# Run nsclean for 1/f noise.\n",
    "spec2dict['nsclean']['skip'] = False\n",
    "spec2dict['nsclean']['n_sigma'] = 0.1\n",
    "spec2dict['nsclean']['save_mask'] = True\n",
    "spec2dict['nsclean']['save_results'] = True\n",
    "\n",
    "# Build a sigma-clipped mask not based on WCS.\n",
    "spec2dict['nsclean']['mask_spectral_regions'] = False\n",
    "spec2dict['nsclean']['save_noise'] = True  # Save the 1/f noise removed.\n",
    "\n",
    "spec2dict['extract_1d']['apply_apcorr'] = False  # Turn off aperture correction.\n",
    "\n",
    "# Turn on bad pixel self-calibration, where all exposures on a given detector \n",
    "# are used to find and flag bad pixels that may have been missed by the bad pixel mask.\n",
    "# This step is experimental, and works best when dedicated background observations are included.\n",
    "#spec2dict['badpix_selfcal']['skip'] = False\n",
    "#spec2dict['badpix_selfcal']['flagfrac_upper'] = 0.005  # Fraction of pixels to flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c30071-e404-44a2-b55b-079c70a721b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "To correct for 1/f noise with `nsclean` in Stage 2, see the demo notebook for BOTS data [here](https://github.com/spacetelescope/jdat_notebooks/tree/main/notebooks/NIRSpec/NIRSpec_NSClean).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0c00a-4428-41e9-ac93-457575b1b25f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6.2 Create `Spec2Pipeline` ASN Files\n",
    "\n",
    "[Association (ASN) files](https://jwst-pipeline.readthedocs.io/en/stable/jwst/associations/overview.html) define the relationships between multiple exposures, allowing them to get processed as a set rather than individually. Processing an ASN file enables the exposures to be calibrated, archived, retrieved, and reprocessed as a set rather than as individual objects.\n",
    "\n",
    "[Stage 2 ASN files](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/level2_asn_technical.html) for BOTS data will typically only include `science` (not `background`) asn `selfacal` exposure types.\n",
    "\n",
    "This notebook creates the Stage 2 ASN files using the files created in Stage 1.\n",
    "\n",
    "Define a function that creates a Level 2 ASN file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel2asn(onescifile, allscifiles, asnfile, product_name, exp_type):\n",
    "    \"\"\"\n",
    "    Create a Level 2 association file for each science exposure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    onescifile : str\n",
    "        Path to the primary science exposure file.\n",
    "    allscifiles : list of str\n",
    "        List of all science exposure files.\n",
    "    asnfile : str\n",
    "        Path to write the output association file.\n",
    "    product_name : str\n",
    "        Name of the product for the association.\n",
    "    exp_type : str, optional\n",
    "        Exposure type to match against.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    True if the association was written successfully, and False otherwise \n",
    "    \"\"\"\n",
    "    # Define a basic association with the science file.\n",
    "    # Wrap in array since input was single exposure.\n",
    "    asn = afl.asn_from_list([onescifile], rule=Asn_Lv2SpecTSO, product_name=product_name)\n",
    "    program = fits.getval(onescifile, 'PROGRAM')\n",
    "    asn.data['program'] = program\n",
    "\n",
    "    # Grab header information from the science file.\n",
    "    exp_type = fits.getval(onescifile, 'EXP_TYPE')\n",
    "    if (exp_type == exp_type):\n",
    "        detector = fits.getval(onescifile, 'DETECTOR')\n",
    "        grating = fits.getval(onescifile, 'GRATING')\n",
    "        filt = fits.getval(onescifile, 'FILTER')\n",
    "        fxd_slit = fits.getval(onescifile, 'FXD_SLIT')\n",
    "\n",
    "    # If the exposure type does not match, fail out \n",
    "    # to ensure TA images don't get processed by accident.\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    # Find all files matching the input configuration and split into regular/imprint.\n",
    "    use_sci, _ = get_matching(allscifiles, detector, filt, grating, fxd_slit, exp_type)\n",
    "\n",
    "    # Assign selfcal exposures.\n",
    "    for file in use_sci:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})\n",
    "\n",
    "    # Write the association to a json file.\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f780a6d-97f7-4ebc-a280-73d2c7bf4dcd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6.3 Run `Spec2Pipeline`\n",
    "\n",
    "Run the science files through the `calwebb_spec2` pipeline using the `.call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429747af-2bff-46f7-90fb-8bf6df108719",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run Stage 2 pipeline using the custom spec2dict dictionary.\n",
    "start = time.time()\n",
    "\n",
    "if dospec2:\n",
    "\n",
    "    # --------------------------Science ASN files--------------------------\n",
    "    for file in rate_sci:\n",
    "        try:  # Create ASN files.\n",
    "            asnfile = os.path.join(asn_dir, os.path.basename(file).replace('rateints.fits', 'l2asn.json'))\n",
    "            if writel2asn(file, rate_sci, asnfile, 'Level2', 'NRS_SLIT'):\n",
    "                print(f\"Applying Stage 2 Corrections & Calibrations to: {file}\")\n",
    "                spec2sci_result = Spec2Pipeline.call(asnfile,\n",
    "                                                     save_results=True,\n",
    "                                                     steps=spec2dict,\n",
    "                                                     output_dir=spec2_dir)\n",
    "        except Exception as e:\n",
    "            # A handle for when no slits fall on NRS1/2.\n",
    "            print(f\"Skipped processing {os.path.basename(asnfile)}: {e}\")\n",
    "    print(\"Stage 2 has been completed!\\n\")\n",
    "else:\n",
    "    print('Skipping Spec2 processing for SCI data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5b124",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "For “cal” or “calints” products that have not been resampled, the extraction region will be curved to follow the calculated trace.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark.\n",
    "time3 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time3 - time0) / 60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a3156-a553-4b0e-865b-fbf7a0172df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Stage 2 products.\n",
    "\n",
    "# ------------------------1/f noise cleaned files-------------------------\n",
    "masks = sorted(glob.glob(spec2_dir + '*mask.fits'))\n",
    "rate_sci_cl = sorted(glob.glob(spec2_dir + '*nsclean.fits'))\n",
    "flicker_noise = sorted(glob.glob(spec2_dir + '*flicker_noise.fits'))\n",
    "\n",
    "print(f\"NSCLEAN | Masks :\\n{'-' * 20}\\n\" + \"\\n\".join(masks))\n",
    "print(f\"NSCLEAN | 1/F NOISE:\\n{'-' * 20}\\n\" + \"\\n\".join(flicker_noise))\n",
    "print(f\"NSCLEAN | RATE:\\n{'-' * 20}\\n\" + \"\\n\".join(rate_sci_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52bb54-7b8a-4f5e-9f01-6fb126b68c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Stage 2 products.\n",
    "\n",
    "# -----------------------------Science files-----------------------------\n",
    "sci_cal = sorted(glob.glob(spec2_dir + '*_calints.fits'))\n",
    "sci_x1d = sorted(glob.glob(spec2_dir + '*_x1dints.fits'))\n",
    "\n",
    "print(f\"SCIENCE | Stage 2 CAL Products:\\n{'-' * 20}\\n\" + \"\\n\".join(sci_cal))\n",
    "print(f\"SCIENCE | Stage 2 X1D Products:\\n{'-' * 20}\\n\" + \"\\n\".join(sci_x1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c66e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Reorganized TSO Data Products**: Previously, each extension contained the 1D spectrum of a single integration. Now, each extension provides a table of all 1D spectra from a single segment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd801b5-38bc-4da4-b44b-47c7b476c0aa",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Stage 3: `Tso3Pipeline` (`calwebb_tso3`)\n",
    "\n",
    "In this section, we process our calibrated spectra from Stage 2 (`calwebb_spec2`) through the Tso3 (`calwebb_tso3`) pipeline to create Stage 3 [data products](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html).\n",
    "\n",
    "* **Input**: An ASN file that lists multiple exposures or exposure segments of a science target (`_calints.fits`).\n",
    "* **Output**: Calibrated time-series spectra and white-light curve.\n",
    "\t* `_x1dints.fits`: Extracted 1D spectroscopic data for all integrations contained in the input exposures.\n",
    "    * `_whtlt.ecsv`: An ASCII catalog in `ecsv` format containing the wavelength-integrated white-light photometry of the source.   \n",
    "\n",
    "The `TSO3Pipeline` performs additional corrections (e.g., outlier detection) and produces calibrated time-series spectra and white-light curve of the source object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95efe55-4f4a-43fd-85cb-62121856ceac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.1 Configure `Tso3Pipeline`\n",
    "\n",
    "The `Tso3Pipeline` has the following steps available for NIRSpec BOTS:\n",
    "\n",
    "> * `outlier_detection` : Identification of bad pixels or cosmic-rays that remain in each of the input images.\n",
    "> * `extract_1d`: Extracts a 1D signal from 2D or 3D datasets.\n",
    "> * `white_light`: Sums the spectroscopic flux over all wavelengths in each integration of a multi-integration extracted spectrum product to produce an integrated (“white”) flux as a function of time for the target. \n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_tso3) •\n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_tso3.html)\n",
    "\n",
    "Below, we set up a dictionary that defines how the `TSO3Pipeline` should be configured for BOTS data. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "To override specific steps and reference files, use the examples below. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c295161-668f-4387-b3d9-c812c4b4583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Tso3 pipeline should be configured.\n",
    "\n",
    "# -------------------------Boilerplate dictionary setup-------------------------\n",
    "tso3dict = {}\n",
    "tso3dict['outlier_detection'], tso3dict['pixel_replace'] = {}, {}\n",
    "tso3dict['white_light'], tso3dict['extract_1d'] = {}, {}\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "# Overrides for various reference files.\n",
    "# Files should be in the base local directory or provide full path.\n",
    "#tso3dict['extract_1d']['override_extract1d'] = 'myfile.json'\n",
    "\n",
    "# Run pixel replacement code to extrapolate values for otherwise bad pixels.\n",
    "# This can help mitigate 5-10% negative dips in spectra of bright sources.\n",
    "# Use the 'fit_profile' algorithm.\n",
    "tso3dict['pixel_replace']['skip'] = False\n",
    "tso3dict['pixel_replace']['n_adjacent_cols'] = 5\n",
    "tso3dict['pixel_replace']['algorithm'] = 'fit_profile'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5eea3-6474-4a96-9f8a-6f43c596b4f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.2 Create `Tso3Pipeline` ASN Files\n",
    "\n",
    "[Stage 3 ASN files](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/level3_asn_technical.html) for BOTS data includes `science` exposure types. A Stage 3 ASN file requires at least one `science` file, although there is usually more than one. **Note that the science exposures should be in the `_calints.fits` format.**\n",
    "\n",
    "In practice, Stage 3 ASN files can be downloaded directly from MAST, however, here we provide an example of manually creating Stage 3 ASN files. Below we create an ASN files for each GRATING/FILTER combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04edd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel3asn(scifiles, detector=None):\n",
    "    \"\"\"\n",
    "    Create a Level 3 association file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scifiles : list of str\n",
    "        List of all science exposure files.\n",
    "    detector : str, optional\n",
    "        Detector names to include. If None, include all.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    # Filter based on GRATING/FILTER.\n",
    "    from collections import defaultdict\n",
    "    grouped = defaultdict(lambda: {'sci': [], 'bg': []})\n",
    "\n",
    "    for f in scifiles:\n",
    "        if detector:\n",
    "            det = fits.getval(f, 'DETECTOR')\n",
    "            if det != detector:\n",
    "                continue  # Skip if detector does not match.\n",
    "\n",
    "        filt = fits.getval(f, 'FILTER')\n",
    "        grat = fits.getval(f, 'GRATING')\n",
    "        grouped[(filt, grat)]['sci'].append(f)\n",
    "\n",
    "    # Make ASN for each FILTER/GRATING.\n",
    "    for (filt, grat), files in grouped.items():\n",
    "        name = f\"{filt}_{grat}\".lower()\n",
    "        asnfile = os.path.join(asn_dir, f\"{name}_l3asn.json\")\n",
    "        asn = afl.asn_from_list(files['sci'], rule=DMS_Level3_Base, product_name=name)\n",
    "        asn.data['asn_type'] = 'tso3'\n",
    "        asn.data['program'] = program if 'program' in globals() else \"9999\"\n",
    "\n",
    "        with open(asnfile, 'w') as f:\n",
    "            f.write(asn.dump()[1])\n",
    "    print(\"Level 3 ASN creation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5531738",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dotso3:\n",
    "    # Restrict to one detector for the demo.\n",
    "    writel3asn(sci_cal, detector=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ddd33",
   "metadata": {},
   "source": [
    "Check that the association files for Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc96c6-d9b1-4925-b45b-1d163bda96b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open an ASN file as an example.\n",
    "# Check that file paths have been correctly updated.\n",
    "if dotso3:\n",
    "    spec3_asn = glob.glob(asn_dir + '*l3asn.json')[0]\n",
    "    with open(spec3_asn, 'r') as f_obj:\n",
    "        asnfile_data = json.load(f_obj)\n",
    "    display(JSON(asnfile_data, expanded=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54795e19-aed2-4529-b7a2-f89e97642d78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.3 Run `Tso3Pipeline`\n",
    "\n",
    "Run the science files through the `calwebb_tso3` pipeline using the `.call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d0f98-c87f-4e99-a01a-f083fc91565f",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Run Stage 3 pipeline using the custom tso33dict dictionary.\n",
    "if dotso3:\n",
    "    for tso3_asn in glob.glob(asn_dir + '*l3asn.json'):\n",
    "        print(f\"Applying Stage 3 to: {os.path.basename(tso3_asn)}\")\n",
    "        tso3_result = Tso3Pipeline.call(tso3_asn,\n",
    "                                        save_results=True,\n",
    "                                        steps=tso3dict,\n",
    "                                        output_dir=tso3_dir)\n",
    "    print(\"Stage 3 has been completed!\\n\")\n",
    "else:\n",
    "    print(\"Skipping Stage 3. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fda6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark.\n",
    "time4 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time4 - time0) / 60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0a03c-765e-4798-a613-a2cb2bd594a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Stage 3 products.\n",
    "\n",
    "stage3_whtlt = sorted(glob.glob(tso3_dir + '*_whtlt.ecsv'))\n",
    "stage3_x1d = sorted(glob.glob(tso3_dir + '*_x1dints.fits'))\n",
    "\n",
    "print(f\"Stage 3 White Light Products:\\n{'-' * 20}\\n\" + \"\\n\".join(stage3_whtlt))\n",
    "print(f\"Stage 3 X1D Products:\\n{'-' * 20}\\n\" + \"\\n\".join(stage3_x1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42987c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Visualizing the Data\n",
    "\n",
    "Define convenience funcitons for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1462f",
   "metadata": {},
   "source": [
    "Function to consolidate all extracted spectra (from each segment) and their corresponding timestamps into single large arrays using the helper function `compile_segments`. This structure simplifies the analysis and plotting.\n",
    "\n",
    "With the consolidated arrays, we plot three one-dimensional spectra from the spectral time series using the `display_spectra` helper function. We also offset two spectra by a constant amount to make them easier to distinguish since nearly all TSO spectra have the same flux (except reduced flux during transit or secondary eclipse). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398eff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_segments(data_products):\n",
    "    \"\"\"\n",
    "    Compiles extracted 1D spectra, corresponding timestamps,\n",
    "    and wavelengths from a list of X1D data products.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_products : list of str\n",
    "        A list of data products (X1DINT files).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_spec_1D : numpy.ndarray\n",
    "        A 2D array where each row corresponds to a spectrum from a single\n",
    "        integration, and columns represent flux values at each wavelength.\n",
    "    all_times : numpy.ndarray\n",
    "        A 1D array containing the mid-integration times (e.g., BJD_TDB) for\n",
    "        each spectrum in `all_spec_1D`.\n",
    "    \"\"\"\n",
    "\n",
    "    data_products = [data_products] if isinstance(data_products, str) else data_products\n",
    "\n",
    "    # Return empty arrays if the input list is empty.\n",
    "    if not data_products:\n",
    "        return None, None\n",
    "\n",
    "    for i, product in enumerate(data_products):\n",
    "\n",
    "        x1d = datamodels.open(product)\n",
    "        \n",
    "        n_spec, n_pix = x1d.spec[0].spec_table.WAVELENGTH.shape\n",
    "        seg_spec_1D = np.zeros([n_spec, n_pix])\n",
    "        wave_um = x1d.spec[0].spec_table.WAVELENGTH[0, :]\n",
    "\n",
    "        for j in range(n_spec):\n",
    "            seg_spec_1D[j, :] = x1d.spec[0].spec_table.FLUX[j, :]\n",
    "\n",
    "        if i == 0:\n",
    "            all_spec_1D = seg_spec_1D\n",
    "            all_times = x1d.int_times.int_mid_BJD_TDB\n",
    "        if i > 0:\n",
    "            all_spec_1D = np.concatenate((all_spec_1D, seg_spec_1D), axis=0)\n",
    "            all_times = np.concatenate((all_times,\n",
    "                                        x1d.int_times.int_mid_BJD_TDB),\n",
    "                                       axis=0)\n",
    "\n",
    "    # We also trim several columns at the start and end of the spectra.\n",
    "    # These belong to the reference pixels and are marked 'nan'.\n",
    "    print(\"Trimming first/last 5 reference pixels with nan-values ...\")\n",
    "    all_spec_1D = all_spec_1D[:, 5:-5]\n",
    "    wave_um = wave_um[5:-5]\n",
    "\n",
    "    return all_spec_1D, all_times, wave_um"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94106cb4",
   "metadata": {},
   "source": [
    "Function to display Stage 1 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rate(rates,\n",
    "                 slits_models=[],\n",
    "                 integration=0,\n",
    "                 extname='data',\n",
    "                 cmap='viridis',\n",
    "                 bad_color=(1, 0.7, 0.7),\n",
    "                 vmin=None,\n",
    "                 vmax=None,\n",
    "                 scale='asinh',\n",
    "                 aspect='auto',\n",
    "                 title_prefix=None,\n",
    "                 title_path=False,\n",
    "                 save_plot=False):\n",
    "    \"\"\"\n",
    "    Display countrate images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rates : list of str\n",
    "        A list of RATE[INTS] files to be displayed.\n",
    "    slits_models : list of str, optional\n",
    "        A list of CAL[INTS] or S2D files containing the slit models.\n",
    "        If provided, slit cutouts will be overlaid on the countrate images.\n",
    "    integration : {None, 'min', int}, optional\n",
    "        Specifies the integration to use for multi-integration data.\n",
    "        If 'min', the minimum value across all integrations is used.\n",
    "        If an integer, the specific integration index is used (default 0).\n",
    "    extname : str, optional\n",
    "        The name of the data extension to extract from ('data', 'dq', etc.).\n",
    "    cmap : str, optional\n",
    "        Colormap to use for displaying the image. Default is 'viridis'.\n",
    "    bad_color : tuple of float, optional\n",
    "        Color to use for NaN pixels. Default is light red (1, 0.7, 0.7).\n",
    "    vmin : float, optional\n",
    "        Minimum value for color scaling. If None, determined from the data.\n",
    "    vmax : float, optional\n",
    "        Maximum value for color scaling. If None, determined from the data.\n",
    "    scale : {'linear', 'log', 'asinh'}, optional\n",
    "        Scale to use for the image normalization. Default is 'asinh'.\n",
    "    aspect : str, optional\n",
    "        Aspect ratio of the plot. Default is 'auto'.\n",
    "    title_prefix : str, optional\n",
    "        Optional prefix for the plot title.\n",
    "    title_path : bool, optional\n",
    "        If True, uses the full file path for the title;\n",
    "        otherwise, uses the basename. Default is False.\n",
    "    save_plot : bool, optional\n",
    "        If True, saves the plot as a PNG file. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------Check Inputs-------------------------------\n",
    "    rates = [rates] if isinstance(rates, str) else rates\n",
    "    slits_models = [slits_models] if isinstance(slits_models, str) else slits_models\n",
    "    nrates = len(rates)\n",
    "\n",
    "    # ------------------------------Set up figures------------------------------\n",
    "    fig, axes = plt.subplots(nrates, 1, figsize=(12, 12 * nrates),\n",
    "                             sharex=True, height_ratios=[1] * nrates)\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    axes = [axes] if nrates == 1 else axes\n",
    "\n",
    "    cmap = plt.get_cmap(cmap)  # Set up colormap and bad pixel color.\n",
    "    cmap.set_bad(bad_color, 1.0)\n",
    "\n",
    "    # ---------------------------Plot countrate image---------------------------\n",
    "    for i, (rate, cal) in enumerate(itertools.zip_longest(rates,\n",
    "                                                          slits_models,\n",
    "                                                          fillvalue=None)):\n",
    "\n",
    "        # -------------------Open files as JWST datamodels-------------------\n",
    "        model = datamodels.open(rate)\n",
    "        slits_model = datamodels.open(cal) if cal else None\n",
    "\n",
    "        # -----------------------Extract the 2D/3D data----------------------\n",
    "        data_2d = getattr(model, extname)\n",
    "        if data_2d.ndim == 3:  # Handle multi-integration data.\n",
    "            if integration == 'min':\n",
    "                data_2d = np.nanmin(data_2d, axis=0)\n",
    "            elif isinstance(integration, int) and 0 <= integration < data_2d.shape[0]:\n",
    "                data_2d = data_2d[integration]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid integration '{integration}' for 3D data.\")\n",
    "\n",
    "        # ---------------------------Scale the data-------------------------\n",
    "        sigma_clipped_data = sigma_clip(data_2d, sigma=5, maxiters=3)\n",
    "        vmin = np.nanmin(sigma_clipped_data) if vmin is None else vmin\n",
    "        vmax = np.nanmax(sigma_clipped_data) if vmax is None else vmax\n",
    "        stretch_map = {'log': LogStretch(), 'linear': LinearStretch(),\n",
    "                       'asinh': AsinhStretch()}\n",
    "        if scale in stretch_map:\n",
    "            norm = ImageNormalize(sigma_clipped_data,\n",
    "                                  interval=ManualInterval(vmin=vmin, vmax=vmax),\n",
    "                                  stretch=stretch_map[scale])\n",
    "        else:\n",
    "            norm = simple_norm(sigma_clipped_data, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        # ----------------Plot the countrate image & colorbar---------------\n",
    "        plt.subplots_adjust(left=0.05, right=0.85)\n",
    "        im = axes[i].imshow(data_2d, origin='lower', cmap=cmap,\n",
    "                            norm=norm, aspect=aspect, interpolation='nearest')\n",
    "        units = model.meta.bunit_data\n",
    "        cbar_ax = fig.add_axes([axes[i].get_position().x1 + 0.02,\n",
    "                                axes[i].get_position().y0, 0.02,\n",
    "                                axes[i].get_position().height])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label(units, fontsize=12)\n",
    "\n",
    "        # -----------------Draw slits and label source ids------------------\n",
    "        # slits_model can be s2d/cal from spec2 - contains slit models for all sources.\n",
    "        if slits_model:\n",
    "            slit_patches = []\n",
    "            for slit in slits_model.slits:\n",
    "                slit_patch = Rectangle((slit.xstart, slit.ystart),\n",
    "                                       slit.xsize, slit.ysize)\n",
    "                slit_patches.append(slit_patch)\n",
    "                y = slit.ystart + slit.ysize / 2\n",
    "                x = slit.xstart if 'nrs1' in rate else slit.xstart + slit.xsize\n",
    "                ha = 'right' if 'nrs1' in rate else 'left'\n",
    "                plt.text(x, y, slit.source_id, color='w', ha=ha, va='center',\n",
    "                         fontsize=7, path_effects=[], weight='bold')\n",
    "            axes[i].add_collection(PatchCollection(slit_patches, ec='r', fc='None'))\n",
    "\n",
    "        # -----------------Construct title and axis labels------------------\n",
    "        filename = model.meta.filename\n",
    "        title = (f\"{title_prefix + ' ' if title_prefix else ''}\"\n",
    "                 f\"{filename if title_path else os.path.basename(filename)}\")\n",
    "        if integration is not None:\n",
    "            title = title.replace('rateints', f'rateints[{integration}]')\n",
    "        axes[i].set_title(title, fontsize=14)\n",
    "        axes[i].set_xlabel(\"Pixel Column\", fontsize=12)\n",
    "        axes[i].set_ylabel(\"Pixel Row\", fontsize=12)\n",
    "\n",
    "        # -------------------------Save the figure?-------------------------\n",
    "        if save_plot:\n",
    "            save_plot = rate.replace('fits', 'png')\n",
    "            if integration:\n",
    "                save_plot = save_plot.replace('.png', '%s.png' % integration)\n",
    "            fig.savefig(save_plot, dpi=200)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df073b",
   "metadata": {},
   "source": [
    "Function to display the calibrated BOTS spectra from Stage 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_spectra(data_products,\n",
    "                    integrations=0,\n",
    "                    offsets=0):\n",
    "    \"\"\"\n",
    "    Displays the calibrated BOTS spectra from Stage 2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_products : str or list of str\n",
    "        File path or list of file paths to X1D data products.\n",
    "    integrations : int or list of int, optional\n",
    "        Indices of integrations to plot (default is 0).\n",
    "    offset : int or list of int, optional\n",
    "        Offsets to apply between spectra (default is 0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------Check and sort input lists-----------------------\n",
    "    data_products = [data_products] if isinstance(data_products, str) else data_products\n",
    "    integrations = [integrations] if isinstance(integrations, int) else integrations\n",
    "    offsets = [offsets] * len(integrations) if isinstance(offsets, (int)) else offsets\n",
    "\n",
    "    # Sort NRS1 and NRS2 products.\n",
    "    products = {\n",
    "        \"NRS1\": [f for f in sorted(data_products) if 'nrs1' in f],\n",
    "        \"NRS2\": [f for f in sorted(data_products) if 'nrs2' in f]\n",
    "    }\n",
    "\n",
    "    # ----------Load extracted spectra and time stamps into one array---------\n",
    "    for key, product_list in products.items():\n",
    "        if not product_list:\n",
    "            continue\n",
    "\n",
    "        # Load all spectra from list of segments.\n",
    "        # This makes plotting and analysis easier.\n",
    "        all_spec1D, all_times, wave_um = compile_segments(product_list)\n",
    "\n",
    "        # Print summary.\n",
    "        print(f\"\\n{key} Summary:\")\n",
    "        print(f\"  Total number of time stamps: {len(all_spec1D)}\")\n",
    "        print(f\"  Total number of 1D spectra:  {all_spec1D.shape[0]}\")\n",
    "        print(f\"  Total number of columns:     {all_spec1D.shape[1]}\")\n",
    "        print(f\"  Total length of wavemap:     {len(wave_um)}\\n\")\n",
    "\n",
    "        # --------------------------Set up figures--------------------------\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[1, 2])\n",
    "        fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "        ax2d, ax1d = axes\n",
    "\n",
    "        for idx, i in enumerate(integrations):\n",
    "            ax1d.plot(wave_um, all_spec1D[i, :] - offsets[idx],\n",
    "                      label=f'Spectrum {i}')\n",
    "\n",
    "        ax1d.set_xlabel(\"Wavelength (microns)\")\n",
    "        ax1d.set_ylabel(\"Flux (Jy) + Constant offset\")\n",
    "        ax1d.grid(True)\n",
    "        ax1d.legend()\n",
    "\n",
    "        # ------------------------Plot CAL FITS file------------------------\n",
    "        x1d = datamodels.open(product_list[0]).spec[0]\n",
    "        # Handle both cases dynamically\n",
    "        if 'x1dints_mod' in product_list[0]:\n",
    "            cal = datamodels.open(product_list[0].replace('x1dints_mod', 'calints'))\n",
    "        else:\n",
    "            cal = datamodels.open(product_list[0].replace('x1dints', 'calints'))\n",
    "\n",
    "        ax2d.imshow(cal.data[0], aspect='auto', vmin=np.nanpercentile(cal.data[0], 10),\n",
    "                    vmax=np.nanpercentile(cal.data[0], 90), origin='lower')\n",
    "        ystart, ystop = (x1d.extraction_ystart - 1,\n",
    "                         x1d.extraction_ystop - 1\n",
    "                         )\n",
    "        extract_width = ystop - ystart + 1\n",
    "        slit = cal\n",
    "        _, _, _, trace = location_from_wcs(cal, slit)\n",
    "        ax2d.plot(np.arange(len(trace)), trace + extract_width, color='r')\n",
    "        ax2d.plot(np.arange(len(trace)), trace - extract_width, color='r')\n",
    "        ax2d.set_title(cal.meta.filename)\n",
    "        ax2d.set_xlabel(\"Pixel Column\")\n",
    "        ax2d.set_ylabel(\"Pixel Row\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # If no products were found, display a message\n",
    "    if not any(products.values()):\n",
    "        print(\"No NRS1 or NRS2 products found. Exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3c999",
   "metadata": {},
   "source": [
    "Function to display the white light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_light_curve(all_spec_1D,\n",
    "                        all_times,\n",
    "                        wave_um,\n",
    "                        total_flux_cols=(120, -20),\n",
    "                        correct_tilt_event=False,\n",
    "                        before_transit=(0, 170),\n",
    "                        tilt_event=270,\n",
    "                        after_transit=(330, 460)):\n",
    "    \"\"\"\n",
    "    Create and display white light curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_spec_1D : ndarray\n",
    "        2D array of spectra (integrations x columns).\n",
    "    all_times : ndarray\n",
    "        Array of time stamps for integrations.\n",
    "    wave_um : ndarray\n",
    "        Array of wavelengths corresponding to columns.\n",
    "    total_flux_cols : tuple of int\n",
    "        Tuple specifying the start and end column indices\n",
    "        for summing flux to calculate the white light curve.\n",
    "    correct_tilt_event : bool, optional\n",
    "        If True, applies a correction to address tilt events\n",
    "        by normalizing affected regions.\n",
    "    before_transit : tuple of int, optional\n",
    "        Tuple specifying the range of indices (start, end)\n",
    "        defining the data before the transit.\n",
    "    tilt_event : int, optional\n",
    "        Index specifying the start of the tilt event for applying corrections.\n",
    "    after_transit : tuple of int, optional\n",
    "        Tuple specifying the range of indices (start, end)\n",
    "        defining the data after the transit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------Set up figures--------------------------\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[1, 2])\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "    axlc, axslc = axes\n",
    "\n",
    "    # ---------------------Obtain white light curve---------------------\n",
    "    n_spec = len(all_times)  # Number of spectra (integrations).\n",
    "    wlc_flux = np.zeros(n_spec)\n",
    "\n",
    "    # Sum all flux (total flux) in a given range.\n",
    "    for i in range(n_spec):\n",
    "        col_start, col_end = total_flux_cols[0], total_flux_cols[1]\n",
    "        wlc_flux[i] = np.nansum(all_spec_1D[i, col_start:col_end])\n",
    "\n",
    "    # Normalize by the median flux of the first twenty points.\n",
    "    wlc_flux /= np.nanmedian(wlc_flux[0:20])\n",
    "    if correct_tilt_event:\n",
    "        # Normalize the post-tilt region using the after_transit median.\n",
    "        wlc_flux[tilt_event:] /= np.nanmedian(wlc_flux[after_transit[0]:after_transit[1]])\n",
    "\n",
    "    # Calculate light curve scatter from first ~100 points.\n",
    "    wlc_flux_s = sigma_clip(wlc_flux, sigma=2, maxiters=2, masked=False)\n",
    "    sigma_wlc = np.sqrt(np.nanvar(wlc_flux_s[2:100]))\n",
    "    sigma_wlc_ppm = round(sigma_wlc * 1e6, 0)\n",
    "    print(f\"White Light Curve scatter (ppm):  {sigma_wlc_ppm}\")\n",
    "\n",
    "    # Plot white light curve\n",
    "    time_axis = (all_times - np.nanmean(all_times)) * 24.0\n",
    "    axlc.plot(time_axis,\n",
    "              wlc_flux, color='r', marker='o', markersize=2,\n",
    "              label=f\"White light curve, (r.m.s.={round(sigma_wlc * 1e6, 0)} ppm)\")\n",
    "    wavestart = round(wave_um[col_start], 4)\n",
    "    waveend = round(wave_um[col_end], 4)\n",
    "    axlc.set_title(f\"White Light Curve (λ = {wavestart} - {waveend} μm)\", fontsize=15)\n",
    "    axlc.legend(loc=\"lower right\")\n",
    "    axlc.set_xlabel(\"Time since mid-exposure, hr\", fontsize=15)\n",
    "    axlc.set_ylabel(\"Normalized flux\", fontsize=15)\n",
    "\n",
    "    # Add secondary x-axis for integration indices\n",
    "    integration_indices = np.arange(n_spec)\n",
    "    tick_positions = np.linspace(time_axis.min(),\n",
    "                                 time_axis.max(),\n",
    "                                 len(integration_indices))\n",
    "    axlc_secondary = axlc.secondary_xaxis('top')\n",
    "    axlc_secondary.set_xlabel(\"Integration Index\", fontsize=12)\n",
    "    axlc_secondary.set_xticks(tick_positions[::len(tick_positions) // 10])\n",
    "    axlc_secondary.set_xticklabels([f\"{int(idx)}\" for idx in\n",
    "                                    integration_indices[::len(integration_indices) // 10]])\n",
    "\n",
    "    # -----------------Obtain spectroscopic light curve-----------------\n",
    "    lc_map = np.copy(all_spec_1D)\n",
    "    spec_xlen = len(lc_map[0, :])\n",
    "    for j in range(spec_xlen):\n",
    "        # Normalize each spectrum by the mean.\n",
    "        lc_map[:, j] /= np.nanmean(lc_map[before_transit[0]:before_transit[1], j])\n",
    "        if correct_tilt_event:\n",
    "            # Correct for tilt event.\n",
    "            lc_map[tilt_event:, j] /= np.nanmean(lc_map[after_transit[0]:after_transit[1], j])\n",
    "    axslc.set_title(\"Spectroscopic Light Curves\", fontsize=15)\n",
    "    slc = axslc.imshow(lc_map,\n",
    "                       interpolation=\"bilinear\",\n",
    "                       aspect=\"auto\",\n",
    "                       cmap=\"inferno_r\",\n",
    "                       origin=\"lower\",\n",
    "                       clim=(0.977, 1.005),\n",
    "                       )\n",
    "\n",
    "    axslc.set_xlabel(r\"x-column, pixel\", fontsize=15)\n",
    "    axslc.set_ylabel(\"Integration \", fontsize=15)\n",
    "    plt.colorbar(slc, ax=axslc, label=r\"Normalized Flux\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d086c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.1 Display `Detector1Pipeline` Products\n",
    "\n",
    "Inspect the Stage 1 slope products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8753df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_file = rate_sci[-1]  # Show a rate file, as an example.\n",
    "display_rate(rate_file, integration=100, vmin=-0.1, vmax=1, scale='asinh',\n",
    "             aspect=10, title_prefix='REPROCESSED')  # , extname='dq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe2c54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.2 Display `Spec2Pipeline` Products\n",
    "\n",
    "First, let's visually inspect that the 1/f noise was removed from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976db111",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_file_cl = rate_sci_cl[0]  # Show a rate file, as an example.\n",
    "noise = flicker_noise[0]\n",
    "mask = masks[0]\n",
    "\n",
    "display_rate(mask, integration=100, vmin=0, vmax=1,\n",
    "             aspect=10, title_prefix='MASK ')\n",
    "display_rate(rate_file_cl, integration=100, vmin=-0.1, vmax=1,\n",
    "             aspect=10, scale='asinh', title_prefix='CLEANED ')\n",
    "display_rate(noise, integration=100, vmin=-0.1, vmax=1,\n",
    "             aspect=10, title_prefix='1/F NOISE ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b55be",
   "metadata": {},
   "source": [
    "Now inspect the Stage 2 calibrated spectra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59906732",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_spectra(sci_x1d, integrations=[0, 10, 100], offsets=[0, 70, 140])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159c43c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, we derive white light curves and a spectroscopic light curves from the large arrays we made and plot them.\n",
    "\n",
    "* To produce a white light curve, we sum the flux from the full wavelength range of each extracted one-dimensional spectrum. Then we normalize the light curve by dividing the light curve flux to the median flux of the first twenty data points out-of-the-transit. We calculate and report the scatter using the first ~100 data points.\n",
    "\n",
    "* While the white light curve provides information regarding the overall quality of the data, the light curves from each pixel (wavelength) contain information about the atmosphere of a transiting exoplanet. The second figure in the plot shows chromatic light curves (also known as wavelength maps). To produce them, we obtain a copy of all spectra and normalize each spectrum by its mean value.\n",
    "\n",
    "* You may find that a light curve also exhibits the morphology of a transit event along with a step-function flux jump near the mid-transit. This flux jump (a decrease of flux in this case) is attributed to a 'tilt' event associated with one of the segments of the JWST mirror. A 'tilt' event is considered any uncommanded change in the tip-tilt orientation of a mirror segment and can be caused by a micrometeorite impact. For further details, please consult the following page: [JWST TSO noise sources](https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-noise-sources).\n",
    "\n",
    "* In our plots below, we also correct for the tilt event by renormalizing the post-tilt event light curves. To do that, we divide these light curves by the mean out-of-transit post-tilt event flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f49ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort NRS1 from NRS2 files.\n",
    "sci_x1d_nrs1 = [f for f in sci_x1d if \"nrs1\" in f]\n",
    "sci_x1d_nrs2 = [f for f in sci_x1d if \"nrs2\" in f]\n",
    "\n",
    "# Compile the NRS1 and NRS2 spectra into one array.\n",
    "all_spec1D_nrs1, all_times_nrs1, wave_um_nrs1 = compile_segments(sci_x1d_nrs1)\n",
    "all_spec1D_nrs2, all_times_nrs2, wave_um_nrs2 = compile_segments(sci_x1d_nrs2)\n",
    "\n",
    "# Plot the light curves for each detector.\n",
    "display_light_curve(all_spec1D_nrs1, all_times_nrs1, wave_um_nrs1,\n",
    "                    total_flux_cols=(20, -20),  # Sum flux over integration range.\n",
    "                    # Tilt event correction parameters:\n",
    "                    correct_tilt_event=True,\n",
    "                    before_transit=(0, 170),  # Before transit integration range.\n",
    "                    tilt_event=270,  # Tilt event integration.\n",
    "                    after_transit=(330, 460))  # After transit integration range.\n",
    "\n",
    "display_light_curve(all_spec1D_nrs2, all_times_nrs2, wave_um_nrs2,\n",
    "                    total_flux_cols=(5, -5),  # Sum flux over integration range.\n",
    "                    # Tilt event correction parameters:\n",
    "                    correct_tilt_event=True,\n",
    "                    before_transit=(0, 170),  # Before transit integration range.\n",
    "                    tilt_event=270,  # Tilt event integration.\n",
    "                    after_transit=(330, 460))  # After transit integration range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52815098",
   "metadata": {},
   "source": [
    "* The white light curve shows a relatively low scatter (230 ppm) for NRS1 and a slightly higher scatter (800 ppm) for NRS2.\n",
    "\n",
    "* The 2D plots above show all extracted 1D spectra covering the transit event. The horizontal axis is the spectral direction (wavelength), and the vertical - each integration (time). The dark pixels correspond to the out-of-transit data (pre/post transit), the orange horizontal stripes indicate the ingress and egress portions, and the yellow stripe shows the in-transit part of the light curve. We have corrected the tilt event data by normalizing all spectra after integration 330 using the post-transit flux. The cells above can be rerun without correction for the tilt event. In this case, all light curves after the tilt event will look offset by a constant amount owing to the redistributed wavefront."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4693df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.3 Display `Tso3Pipeline` Products\n",
    "\n",
    "Inspect the Stage 3 combined calibrated spectra. The white light curve produced from Stage 3 is not corrected like above for any tilt events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the white light curve.\n",
    "data = Table.read(stage3_whtlt[0], format=\"ascii.ecsv\")\n",
    "\n",
    "\n",
    "# Check if there is NRS2 data.\n",
    "has_nrs2 = \"MJD_UTC_NRS2\" in data.colnames and \"whitelight_flux_NRS2\" in data.colnames\n",
    "\n",
    "# Extract and normalize NRS1 white light fluxes.\n",
    "mjd_nrs1 = data[\"MJD_UTC_NRS1\"]\n",
    "flux_nrs1 = data[\"whitelight_flux_NRS1\"] / np.nanmedian(data[\"whitelight_flux_NRS1\"][:100])\n",
    "\n",
    "# Extract and normalize NRS2 white light fluxes if present.\n",
    "if has_nrs2:\n",
    "    mjd_nrs2 = data[\"MJD_UTC_NRS2\"]\n",
    "    flux_nrs2 = data[\"whitelight_flux_NRS2\"] / np.nanmedian(data[\"whitelight_flux_NRS2\"][:100])\n",
    "else:\n",
    "    mjd_nrs2 = None\n",
    "    flux_nrs2 = None\n",
    "\n",
    "# Calculate scatter.\n",
    "sigma_wlc_nrs1 = np.sqrt(np.nanvar(flux_nrs1[2:100]))\n",
    "print(f\"NRS1 White Light Curve scatter (ppm): {round(float(sigma_wlc_nrs1) * 1e6, 0)}\")\n",
    "if has_nrs2:\n",
    "    sigma_wlc_nrs2 = np.sqrt(np.nanvar(flux_nrs2[2:100]))\n",
    "    print(f\"NRS2 White Light Curve scatter (ppm): {round(float(sigma_wlc_nrs2) * 1e6, 0)}\")\n",
    "\n",
    "# Plot white light curve.\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot NRS1.\n",
    "plt.plot(\n",
    "    (mjd_nrs1 - np.nanmean(mjd_nrs1)) * 24.0,\n",
    "    flux_nrs1,\n",
    "    color=\"blue\",\n",
    "    marker=\"o\",\n",
    "    markersize=2,\n",
    "    label=f\"NRS1 (r.m.s.={round(sigma_wlc_nrs1 * 1e6, 0)} ppm)\"\n",
    ")\n",
    "# Plot NRS2.\n",
    "plt.plot(\n",
    "    (mjd_nrs2 - np.nanmean(mjd_nrs2)) * 24.0,\n",
    "    flux_nrs2,\n",
    "    color=\"orange\",\n",
    "    marker=\"o\",\n",
    "    markersize=2,\n",
    "    label=f\"NRS2 (r.m.s.={round(sigma_wlc_nrs2 * 1e6, 0)} ppm)\"\n",
    ")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"White Light Curve\", fontsize=15)\n",
    "plt.xlabel(\"Time since mid-exposure, hr\", fontsize=15)\n",
    "plt.ylabel(\"Normalized flux\", fontsize=15)\n",
    "\n",
    "# Set plot limits\n",
    "#plt.ylim([0.965, 1.015])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a29d2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "We now have white and spectroscopic light curves ready for fitting (not covered in this notebook).\n",
    "\n",
    "It should be pointed out that the workaround solutions lead to a lower white light curve scatter for this particular data set (approximately 70 ppm lower, or 160 ppm for NRS1). Two factors determine the difference:\n",
    "\n",
    "1. The pixel replacement step in the workaround uses a nominal PSF profile constructed from adjacent columns (of a column that needs to be corrected) to identify high and low pixels in addition to the data quality flags.\n",
    "2. At the time of writing, the spectral resampling step is unavailable in the STScI pipeline. Instead, in the workaround notebook, we fit for the star's centroid to locate and trace the spectra (without resampling) and perform aperture extraction using the trace.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442af61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Modifying the EXTRACT1D Reference File (as needed)\n",
    "\n",
    "[extract_1d](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d) •\n",
    "[Editing JSON reference file](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/reference_files.html#editing-json-reference-file-format-for-non-ifu-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25720a9f",
   "metadata": {},
   "source": [
    "As of Build 11.3, the `extract_1d` step now uses a curved trace to extract the 1D spectra by default for un-resampled 2D spectra (resampling is skipped for BOTS data). As an example, we still provide a way to modify the extraction region if needed.\n",
    "\n",
    "The EXTRACT1D reference file, along with several other parameter files, can be found in the `CRDS_PATH` directory. While some files, like `.json` files, can be manually edited, we modify them using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the EXTRACT1D reference file.\n",
    "\n",
    "# If you don't know the reference file name this should work.\n",
    "# extract_1d_ref = Spec2Pipeline().get_reference_file(sci_cal, 'extract1d')\n",
    "\n",
    "refs = api.dump_references(crds_client.get_context_used('jwst'),\n",
    "                           ['jwst_nirspec_extract1d_0006.json'])\n",
    "extract_1d_ref = refs['jwst_nirspec_extract1d_0006.json']\n",
    "\n",
    "# Open EXTRACT1D reference file in read-mode.\n",
    "with open(extract_1d_ref, \"r\") as ref_file:\n",
    "    params = json.load(ref_file)\n",
    "\n",
    "    # S1600A1 full slit\n",
    "    params[\"apertures\"][0][\"extract_width\"] = 27\n",
    "    params[\"apertures\"][0].pop(\"nod2_offset\")  # remove\n",
    "    params[\"apertures\"][0].pop(\"nod3_offset\")  # remove\n",
    "    params[\"apertures\"][0].pop(\"nod5_offset\")  # remove\n",
    "    # params[\"apertures\"][0][\"xstart\"] = 100  # lower x-index\n",
    "\n",
    "# Write changes to a new file.\n",
    "newData = json.dumps(params, indent=4)\n",
    "\n",
    "# Add the suffix '_bots' to distinguish the file from the default version.\n",
    "basename = os.path.basename(extract_1d_ref)[:-5]\n",
    "extract_1d_ref_mod = os.path.join(basedir, f'{basename}_bots.json')\n",
    "with open(extract_1d_ref_mod, \"w\") as file:\n",
    "    file.write(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d34a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the EXTRACT1D reference file.\n",
    "with open(extract_1d_ref_mod, 'r') as f_obj:\n",
    "    extract_1d_ref_mod_data = json.load(f_obj)\n",
    "\n",
    "JSON(extract_1d_ref_mod_data, expanded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6346f65",
   "metadata": {},
   "source": [
    "Now, we re-extract the 1D spectrum by running the `Extract1dStep` and overriding the reference file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b64097",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Re-extract the NRS2 1D spectra.\n",
    "sci_cal_nrs2 = [f for f in sci_cal if \"nrs2\" in f]\n",
    "\n",
    "for cal in sci_cal_nrs2:\n",
    "    Extract1dStep.call(cal,\n",
    "                       save_results=True,\n",
    "                       output_dir=spec2_dir,\n",
    "                       output_use_model=True,\n",
    "                       suffix='x1dints_mod',  # Default suffix is `_extract1dstep.fits`\n",
    "                       use_source_posn=False,\n",
    "                       override_extract1d=extract_1d_ref_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_x1d_mod = sorted(glob.glob(spec2_dir + '*nrs2_x1dints_mod.fits'))\n",
    "display_spectra(sci_x1d_mod, integrations=[0, 10, 100], offsets=[0, 70, 140])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685dff52-1b00-47e6-96c5-c34a5679d12a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concluding Remarks\n",
    "\n",
    "In this notebook, we demonstrated how to obtain white and spectroscopic light curves by (re-) running the three stages of the JWST pipeline. The saved data producs can now be provided to light curve fitting codes for measurements of the physical properties of the exoplanet (or other source with temporal variability) and obtaining a transmission spectrum. It should be pointed out that the analyses performed here are only a subset of the possible analyses one can perform, and are in no way the final word on _how_ JWST data _should_ be analyzed. This will be solidified more and more as data comes and best practices are established in the current and future cycles.\n",
    "\n",
    "In conclusion, I would like to express my gratitude to the entire JWST team that has supported the creation of this notebook through discussions and testing, which have improved the notebook. In particular, special thanks to the Time-Series Observations Working Group at STScI, including Néstor Espinoza, Leonardo Ubeda, Sarah Kendrew, Elena Manjavacas, Brian Brooks, Mike Reagan, Loïc Albert, Everett Schlawin, Stephan Birkmann among others. To the NIRCam IDT team for multiple fruitful discussions, including Everett Schlawin, Thomas Beatty, Tom Greene and Jarron Leisenring. To the ERS Transiting Exoplanet team who have provided several venues for discussion and community input. To the several JWST team members, including behind the pipeline and the mission itself, including and in no particular order Bryan Hilbert, Armin Rest, Anton Koekemoer, Alicia Canipe, Melanie Clarke, James Muzerolle, Kayli Glidic, Jeff Valenti and Karl Gordon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ebe2c8-e15a-4969-99b4-8dcdcc245bbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Related Notebooks\n",
    "* [JWebbinar in December 2023](https://github.com/exonik/JWebbinar2023-TSO/blob/main/Part2-Spec2.ipynb)\n",
    "* [NIRSpec Workaround Notebooks](https://github.com/spacetelescope/jwst-caveat-examples/tree/main/NIRSPEC_General)\n",
    "* [JDAT: JWST Data Analysis Example Notebooks](https://github.com/spacetelescope/jdat_notebooks/tree/main/notebooks)\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "       <img src=\"https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_footer.png\" alt=\"Space Telescope Logo\\\" align=\"right\" style=\"width: 200px\"/>\n",
    "</figure>\n",
    "\n",
    "[Top of Page](#NIRSpec-BOTS-Pipeline-Notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
