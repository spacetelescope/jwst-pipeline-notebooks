{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86593a18-2c3e-49a3-80c2-df1c27d98f76",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img style=\"float: center;\" src='https://github.com/spacetelescope/jwst-pipeline-notebooks/raw/main/_static/stsci_header.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d38475-0078-4abd-a46f-1f073d6ec2e4",
   "metadata": {},
   "source": [
    "# NIRSpec IFU Pipeline Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579bc0b7-a989-4e7a-937d-1251aa5e0aec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Authors**: Kayli Glidic (kglidic@stsci.edu), Maria Pena-Guerrero (pena@stsci.edu), Leonardo Ubeda (lubeda@stsci.edu); NIRSpec branch<br>\n",
    "**Last Updated**: February 7, 2025 </br>\n",
    "**Pipeline Version**: 1.17.1 (Build 11.2, Context jwst_1322.pmap)\n",
    "\n",
    "**Purpose**:<br>\n",
    "End-to-end calibration with the James Webb Space Telescope (JWST) pipeline is divided into three main processing stages. This notebook provides a framework for processing generic Near-Infrared Spectrograph (NIRSpec) fixed slit (FS) data through [stages 1-3 of the JWST pipeline](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/stages-of-jwst-data-processing#gsc.tab=0), including how to use associations for multi-exposure observations and how to interact and work with JWST datamodels. Data is assumed to be organized into three folders: science, background, and associations, as specified in the paths set up below. In most cases, editing cells outside the [Configuration](#1.-Configuration) section is unnecessary unless the standard pipeline processing options or plot parameters need to be modified.\n",
    "\n",
    "**[Data](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))**:<br>\n",
    "This notebook is set up to use observations of Tarantula Nebula with the G140H, G235H, and G395H grisms obtained by Proposal ID (PID) 2729, Observation 5. This observation has a CYCLING dither pattern with 8 points. These observations do not include a nod or background. The demo data will automatically download unless disabled (i.e., to use local files instead).\n",
    "\n",
    "**[JWST pipeline version and CRDS context](#Set-CRDS-Context-and-Server)**:<br>\n",
    "This notebook was written for the above-specified pipeline version and associated build context for this version of the JWST Calibration Pipeline. Information about this and other contexts can be found in the JWST Calibration Reference Data System (CRDS [server](https://jwst-crds.stsci.edu/)). If you use different pipeline versions, please refer to the table [here](https://jwst-crds.stsci.edu/display_build_contexts/) to determine what context to use. To learn more about the differences for the pipeline, read the relevant [documentation](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/jwst-operations-pipeline-build-information#references).<br>\n",
    "\n",
    "Please note that pipeline software development is a continuous process, so results in some cases may be slightly different if a subsequent version is used. **For optimal results, users are strongly encouraged to reprocess their data using the most recent pipeline version and [associated CRDS context](https://jwst-crds.stsci.edu/display_build_contexts/), taking advantage of bug fixes and algorithm improvements.**\n",
    "Any [known issues](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/nirspec-known-issues/nirspec-ifu-known-issues#gsc.tab=0) for this build are noted in the notebook.\n",
    "\n",
    "**Updates**:<br>\n",
    "This notebook is regularly updated to incorporate the latest pipeline improvements. Find the most up-to-date version of this notebook [here](https://github.com/spacetelescope/jwst-pipeline-notebooks/).\n",
    "\n",
    "**Recent Changes**:</br>\n",
    "* October 15, 2024: Converted notebook to follow standard template. </br>\n",
    "* November 4, 2024: Notebook updated to JWST pipeline version 1.16.0 (Build 11.1).\n",
    "* January 7, 2025: Add handling for background and CRDS.\n",
    "* February 7, 2025: Always construct associations within this notebook, generalize plotting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f8278-0dd2-4b8b-b4f7-8c3982364536",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [1. Configuration](#1.-Configuration)\n",
    "* [2. Package Imports](#2.-Package-Imports)\n",
    "* [3. Demo Mode Setup](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data))\n",
    "* [4. Directory Setup](#4.-Directory-Setup)\n",
    "* [5. Stage 1: `Detector1Pipeline` (`calwebb_detector1`)](#5.-Stage-1:-Detector1Pipeline-(calwebb_detector1))\n",
    "    * [5.1 Configure `Detector1Pipeline`](#5.1-Configure-Detector1Pipeline)\n",
    "    * [5.2 Run `Detector1Pipeline`](#5.2-Run-Detector1Pipeline)\n",
    "        * [5.2.1 Calibrating Science Files](#5.2.1-Calibrating-Science-Files)\n",
    "        * [5.2.2 Calibrating Background Files](#5.2.2-Calibrating-Background-Files)\n",
    "* [6. Stage 2: `Spec2Pipeline` (`calwebb_spec2`)](#5.-Stage-2:-Spec2Pipeline-(calwebb_spec2))\n",
    "    * [6.1 Configure `Spec2Pipeline`](#6.1-Configure-Spec2Pipeline)\n",
    "    * [6.2 Create `Spec2Pipeline` Association Files](#6.2-Create-Spec2Pipeline-Association-Files)\n",
    "    * [6.3 Run `Spec2Pipeline`](#6.3-Run-Spec2Pipeline)\n",
    "        * [6.3.1 Calibrating Science Files](#6.3.1-Calibrating-Science-Files)\n",
    "        * [6.3.2 Calibrating Background Files](#6.3.2-Calibrating-Background-Files)\n",
    "* [7. Stage 3: `Spec3Pipeline` (`calwebb_spec3`)](#5.-Stage-3:-Spec3Pipeline-(calwebb_spec3))\n",
    "    * [7.1 Configure `Spec3Pipeline`](#7.1-Configure-Spec3Pipeline)\n",
    "    * [7.2 Create `Spec3Pipeline` Association Files](#7.2-Create-Spec3Pipeline-Association-Files)\n",
    "    * [7.3 Run `Spec3Pipeline`](#7.3-Run-Spec3Pipeline)\n",
    "* [8. Visualize the Data](#8.-Visualize-the-Data)\n",
    "    * [8.1 Display `Detector1Pipeline` Products](#8.1-Display-Detector1Pipeline-Products)\n",
    "    * [8.2 Display `Spec3Pipeline` Products](#8.2-Display-Spec3Pipeline-Products)\n",
    "* [9. Modifying the EXTRACT1D Reference File (as needed)](#9.-Modifying-the-EXTRACT1D-Reference-File-(as-needed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0485328-3765-423f-be14-ead5125d1e30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Configuration\n",
    "#### Install dependencies and parameters\n",
    "To make sure that the pipeline version is compatible with the steps discussed below and that the required dependencies and packages get installed, you can create a fresh Conda environment and install the provided requirements.txt file before starting this notebook:\n",
    "\n",
    "    conda create -n nirspec_ifu_pipeline python=3.12\n",
    "    conda activate nirspec_ifu_pipeline\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "Set the basic parameters to configure the notebook. These parameters determine what data gets used, where data is located (if already on disk), and the type of background subtraction (if any). The list of parameters includes:<br>\n",
    "\n",
    "* `demo_mode`:\n",
    "    * `True`: Downloads example data from the [Barbara A. Mikulski Archive for Space Telescopes (MAST)](https://archive.stsci.edu/) and processes it through the pipeline. All processing will occur in a local directory unless modified in [Section 3](#3.-Demo-Mode-Setup-(ignore-if-not-using-demo-data)) below.\n",
    "    * `False`: Process your own downloaded data; provide its location. <br><br>\n",
    "* **Directories with data**:\n",
    "    * `sci_dir`: Directory where science observation data is stored.\n",
    "    * `bg_dir`: Directory where background observation data is stored.\n",
    "    * `asn_dir`: Directory where Stage 2/3 associations are stored.<br><br>\n",
    "* **[Backgroud subtraction methods](https://jwst-docs.stsci.edu/jwst-near-infrared-spectrograph/nirspec-observing-strategies/nirspec-background-recommended-strategies#gsc.tab=0)** (`True` = run, `False` = skip):\n",
    "    * `master_bg`: Apply master-background subtraction in `calwebb_spec3`? Uses dedicated background observations.\n",
    "    * `pixel_bg`: Apply pixel-to-pixel background subtraction in `calwebb_spec2`? This is the default pipeline setting. Uses noded observations.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15465922-ba78-42c0-b816-5114e86f6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import necessary for configuration.\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Control logging level: INFO, WARNING, ERROR\n",
    "# Run command loging.disable if want to hide logging\n",
    "# ERROR messages.\n",
    "# logging.disable(logging.ERROR)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86048200-34c7-4391-a2bf-8304c347dc2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Note that `demo_mode` must be set appropriately below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc84e97-2e63-4bd2-8fea-28e8836ea65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for demo_mode, data mode directories, and processing steps.\n",
    "\n",
    "# -------------------------------DEMO MODE-----------------------------------\n",
    "demo_mode = True\n",
    "\n",
    "if demo_mode:\n",
    "    print('Running in demonstration mode using online example data!')\n",
    "\n",
    "# ----------------------------User Mode Directories--------------------------\n",
    "else:  # If demo_mode = False, look for user data in these paths.\n",
    "\n",
    "    # Set directory paths for processing specific data; adjust to your local\n",
    "    # directory setup (examples provided below).\n",
    "    basedir = os.path.join(os.getcwd(), '')\n",
    "\n",
    "    # Directory to science observation data; expects uncalibrated data in\n",
    "    # sci_dir/uncal/ and results in stage1, stage2, and stage3 directories.\n",
    "    sci_dir = os.path.join(basedir, 'ifu_data_02729/Obs005', '')\n",
    "\n",
    "    # Directory to dedicated background observation data; expects uncalibrated data in\n",
    "    # bg_dir/uncal/ and results in stage1, stage2, and stage3 directories.\n",
    "    # bg_dir = os.path.join(basedir, 'ifu_data_02729/Obs002', '')\n",
    "    bg_dir = ''  # If no dedicated background observation, use an empty string.\n",
    "\n",
    "# ---------------------------Set Processing Steps----------------------------\n",
    "# Individual pipeline stages can be turned on/off here.  Note that a later\n",
    "# stage won't be able to run unless data products have already been\n",
    "# produced from the prior stage.\n",
    "\n",
    "# Science processing.\n",
    "dodet1 = True  # calwebb_detector1\n",
    "dospec2 = True  # calwebb_spec2\n",
    "dospec3 = True  # calwebb_spec3\n",
    "doviz = True  # Visualize calwebb outputs\n",
    "\n",
    "# Dedicated Background Processing\n",
    "dodet1bg = False  # calwebb_detector1\n",
    "dospec2bg = False # calwebb_spec2 (needed for Master Background subtraction)\n",
    "\n",
    "# How should background subtraction be done?\n",
    "# Dedicated backgrounds can either use master or pixel-based subtraction\n",
    "# Nodded backgrounds can use only pixel-based subtraction\n",
    "# If none are selected, data will not be background subtracted.\n",
    "master_bg = False  # Master-background subtraction in spec3.\n",
    "pixel_bg = False  # Pixel-based background subtraction in spec2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306fdb62-9040-4721-a23e-8715dea89b9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Set CRDS Context and Server\n",
    "Before importing `CRDS` and `JWST` modules, we need to configure our environment. This includes defining a CRDS cache directory in which to keep the reference files that will be used by the calibration pipeline. If the local CRDS cache directory has not been set, it will automatically be created in the home directory.<br><br>\n",
    "[Build Context Table](https://jwst-crds.stsci.edu/display_build_contexts/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5467b-54c7-46b7-8d99-6b8df7228233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------Set CRDS context and paths------------------------\n",
    "# Each version of the calibration pipeline is associated with a specific CRDS\n",
    "# context file. The pipeline will select the appropriate context file behind\n",
    "# the scenes while running. However, if you wish to override the default context\n",
    "# file and run the pipeline with a different context, you can set that using\n",
    "# the CRDS_CONTEXT environment variable. Here we show how this is done,\n",
    "# although we leave the line commented out in order to use the default context.\n",
    "# If you wish to specify a different context, uncomment the line below.\n",
    "# os.environ['CRDS_CONTEXT'] = 'jwst_1298.pmap'  # CRDS context for 1.16.0\n",
    "\n",
    "# Set CRDS cache directory to user home if not already set.\n",
    "if os.getenv('CRDS_PATH') is None:\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.path.expanduser('~'), 'crds_cache')\n",
    "\n",
    "# Check whether the CRDS server URL has been set. If not, set it.\n",
    "if os.getenv('CRDS_SERVER_URL') is None:\n",
    "    os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "# Output the current CRDS path and server URL in use.\n",
    "print('CRDS local filepath:', os.environ['CRDS_PATH'])\n",
    "print('CRDS file server:', os.environ['CRDS_SERVER_URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029fff4-5d06-4a82-9970-59cce1581822",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Package Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b707504-8009-433b-8619-2a2e7df4ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the entire available screen width for this notebook.\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f758419-689e-418e-bb25-4116adaea1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------General Imports----------------------\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "import asdf\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# ----------------------Astropy Imports----------------------\n",
    "# Astropy utilities for opening FITS files, downloading demo files, etc.\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.visualization import ImageNormalize, ManualInterval, LogStretch\n",
    "from astropy.visualization import LinearStretch, AsinhStretch, simple_norm\n",
    "\n",
    "# -------------------- Astroquery Imports ----------------------\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# ----------------------Plotting Imports---------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec as grd\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa430fc-4f7e-498d-8192-b14a3a0fdb50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Installation instructions for the JWST pipeline found here: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview) •\n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io) •\n",
    "[Github](https://github.com/spacetelescope/jwst)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c8b54-be86-4e32-8218-717461fd1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------JWST Calibration Pipeline Imports----------------------\n",
    "import jwst  # Import the base JWST and CRDS packages.\n",
    "import crds\n",
    "from crds.client import api\n",
    "from stpipe import crds_client\n",
    "\n",
    "# JWST pipelines (each encompassing many steps).\n",
    "from jwst.pipeline import Detector1Pipeline  # calwebb_detector1\n",
    "from jwst.pipeline import Spec2Pipeline  # calwebb_spec2\n",
    "from jwst.pipeline import Spec3Pipeline  # calwebb_spec3\n",
    "from jwst.extract_1d import Extract1dStep  # Extract1D Step\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels  # JWST pipeline utilities: datamodels.\n",
    "from jwst.associations import asn_from_list as afl  # Tools for creating association files.\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase  # Define Lvl2 ASN.\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base  # Define Lvl3 ASN.\n",
    "\n",
    "# Check the default context for the Pipeline version\n",
    "default_context = crds.get_default_context('jwst', state='build')\n",
    "print(\"JWST Calibration Pipeline Version = {}\".format(jwst.__version__))\n",
    "print(f\"Default CRDS Context for JWST Version {jwst.__version__}: {default_context}\")\n",
    "print(f\"Using CRDS Context: {os.environ.get('CRDS_CONTEXT', default_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef1327-a825-44a0-aa98-14e7eb5d72c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Define Convience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an input file list, return all matching a particular detector (nrs1 vs nrs2) and grating/filter.\n",
    "# Returns two lists: one for non-imprint exposures, one for imprint exposures\n",
    "def get_matching(files, detector, filt, grating):\n",
    "    # Pre-select only NRS_IFU type data since other data (e.g., TA images) don't have the necessary keywords to match\n",
    "    files_regular = []\n",
    "    files_imprint = []\n",
    "    for file in files:\n",
    "        hdr = fits.getheader(file)\n",
    "        if (hdr['EXP_TYPE'] != 'NRS_IFU'):\n",
    "            continue\n",
    "        if ((hdr['DETECTOR'] == detector) & (hdr['FILTER'] == filt) & (hdr['GRATING'] == grating)):\n",
    "            if hdr['IS_IMPRT']:\n",
    "                files_imprint.append(file)\n",
    "            else:\n",
    "                files_regular.append(file)\n",
    "\n",
    "    return files_regular, files_imprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f30a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given two input exposures, determine whether or not the \n",
    "# grating wheel tilt matches closely enough to be associated\n",
    "def match_gwa(file1, file2):\n",
    "    hdr1, hdr2 = fits.getheader(file1), fits.getheader(file2)\n",
    "    gwa_match = False\n",
    "    if np.allclose((hdr1['GWA_XTIL'], hdr1['GWA_YTIL']), (hdr2['GWA_XTIL'], hdr2['GWA_YTIL']), \n",
    "                   atol=1.0e-8, rtol=0):\n",
    "        gwa_match = True\n",
    "    \n",
    "    return gwa_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime.\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081de5e-cd82-4f67-bc07-09ee15839000",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Demo Mode Setup (ignore if not using demo data)\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The data in this notebook is public and does not require a token. For other data sets, you may need to provide a token. For more infomation visit the\n",
    "[astroquery](https://astroquery.readthedocs.io/en/latest/index.html) documentation.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "If running in demonstration mode, set up the program information to retrieve the uncalibrated data (`_uncal.fits`) automatically from MAST using `astroquery`. MAST provides flexibility by allowing searches based on proposal ID and observation ID, rather than relying solely on filenames. More information about the JWST file naming conventions can be found [here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html).\n",
    "\n",
    "The IFU demo data in this notebook is from the [Early Release Science (ERS) Proposal ID 2729](https://www.stsci.edu/jwst/science-execution/program-information?id=2729) and features observations of the Tarantula Nebula (extended source) using multiple grisms. The program setup is briefly summarized in the table below.\n",
    "\n",
    "| Demo Target: Tarantula Nebula | | |\n",
    "|:-----------:|:-------:|:---:|\n",
    "| Proposal ID | 02729 | Program number |\n",
    "| OBSERVTN | 005 | Observation number |\n",
    "| [GRATING/FILTER](https://jwst-docs.stsci.edu/jwst-near-infrared-spectrograph/nirspec-observing-modes/nirspec-ifu-spectroscopy) | G140H/F100LP | λ: 0.97–1.89 μm (a medium resolution, R ~ 1000) |\n",
    "|                | G235H/F170LP | λ: 1.66–3.17 μm (a high resolution, R ~ 2700) |\n",
    "|                | G395H/F290LP | λ: 2.87–5.27 μm (a high resolution, R ~ 2700) |\n",
    "| SUBARRAY | SUBS200A1 | Subarray used |\n",
    "| NINTS | 2 | Number of integrations in exposure |\n",
    "| NGROUPS | 30 | Number of groups in integration |\n",
    "|   DURATION  | 87.533 [s] | Total duration of one exposure |\n",
    "|   READPATT  | NRSIRS2RAPID | Readout Pattern |\n",
    "|   PATTTYPE  | CYCLING | Primary dither pattern type |\n",
    "|   PATTSIZE  | LARGE | Primary dither pattern size (1.0\" extent) |\n",
    "|   NUMDTHPT  | 8 | Total number of points in pattern |\n",
    "|   SRCTYAPT  | UNKNOWN | Source Type selected in APT |\n",
    "\n",
    "> **Note:** The presence of a physical gap between detectors affects high-resolution IFU observations because the spectra are long enough to span both NIRSpec detectors. When using the grating-filter combination G140H/F070LP (or PRISM/CLEAR) the resulting spectra do not have any gaps because the spectra do not extend beyond NRS1. [More Info ...](https://jwst-docs.stsci.edu/jwst-near-infrared-spectrograph/nirspec-operations/nirspec-ifu-operations/nirspec-ifu-wavelength-ranges-and-gaps#NIRSpecIFUWavelengthRangesandGaps-Wavelengthgaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87964d-a1d0-4421-810a-d121e5ba7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the program information and directories to collect\n",
    "# the data in demo_mode.\n",
    "\n",
    "if demo_mode:\n",
    "\n",
    "    print('Running in demonstration mode. '\n",
    "          'Example data will be downloaded from MAST!')\n",
    "\n",
    "    # NOTE:\n",
    "    # For non public data sets, you may need to provide a token.\n",
    "    # However, for security it is not recommended to enter tokens into\n",
    "    # a terminal or Jupyter notebook.\n",
    "    #Observations.login(token=\"\")\n",
    "\n",
    "    # --------------Program and observation information--------------\n",
    "    program = \"02729\"\n",
    "    sci_observtn = \"005\"\n",
    "    bg_observtn = None\n",
    "    # Possible filter options [\"F100LP;G140H\",\"F170LP;G235H\",\"F290LP;G395H\"].\n",
    "    # Limiting selection to one.\n",
    "    filters = [\"F290LP;G395H\"]\n",
    "\n",
    "    # ----------Define the base and observation directories----------\n",
    "    basedir = os.path.join(os.getcwd(), '')\n",
    "    sci_dir = os.path.join(basedir, f'ifu_data_{program}')\n",
    "    sci_dir = os.path.join(sci_dir, f'Obs{sci_observtn}')\n",
    "    \n",
    "    uncal_dir = os.path.join(sci_dir, 'uncal/')\n",
    "\n",
    "    # If no background observation, leave blank.\n",
    "    bg_dir = os.path.join(basedir, f'ifu_data_{program}')\n",
    "    bg_dir = os.path.join(bg_dir, f'Obs{bg_observtn}') if bg_observtn else ''\n",
    "    uncal_bgdir = os.path.join(bg_dir, 'uncal/') if bg_observtn else ''\n",
    "\n",
    "    # ------Ensure directories for downloading MAST data exists------\n",
    "    os.makedirs(uncal_dir, exist_ok=True)\n",
    "    # Makes directory only when a background observation is provided.\n",
    "    if bg_observtn:\n",
    "        os.makedirs(uncal_bgdir, exist_ok=True)\n",
    "\n",
    "else:\n",
    "    print('Running with user provided data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8aea8-6e07-4d93-9db5-c0a8e1950617",
   "metadata": {},
   "source": [
    "<br>Click on the following links to learn more about querying and downloading data:<br>\n",
    "• [Downloading data](https://astroquery.readthedocs.io/en/latest/mast/mast_obsquery.html#downloading-data)<br>\n",
    "• [Observations Class](https://astroquery.readthedocs.io/en/latest/api/astroquery.mast.ObservationsClass.html)<br>\n",
    "• [Products Field Descriptions](https://mast.stsci.edu/api/v0/_productsfields.html)<br><br>\n",
    "\n",
    "Compile tables of files from MAST associated with the science (SCI) and, if applicable, background (BKG) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d030abc-85d6-4313-84dd-b750c0bfcae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list of observation IDs for the specified demo program.\n",
    "\n",
    "if demo_mode:\n",
    "    # --------------------SCIENCE Observation--------------------\n",
    "    sci_obs_id_table = Observations.query_criteria(instrument_name=['NIRSPEC/IFU'],\n",
    "                                                   provenance_name=[\"CALJWST\"],\n",
    "                                                   obs_id=[f'*{program}*{sci_observtn}*'])\n",
    "\n",
    "    # ------------------BACKGROUND Observation-------------------\n",
    "    if bg_observtn:\n",
    "        bg_obs_id_table = Observations.query_criteria(instrument_name=['NIRSPEC/IFU'],\n",
    "                                                      provenance_name=[\"CALJWST\"],\n",
    "                                                      obs_id=[f'*{program}*{bg_observtn}*'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e5b0e-04b6-4536-ad82-7836582b8bbd",
   "metadata": {},
   "source": [
    "Filter these tables to identify uncalibrated data and their association files for download from MAST.\n",
    "\n",
    "The demo dataset consists of eight `_uncal.fits` files (per detector), each approximately 63 MB in size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6daca-94e7-41c1-9326-0747b276c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert visits into a list of uncalibrated data and ASN files.\n",
    "\n",
    "if demo_mode:\n",
    "    file_criteria = {'filters': filters, 'calib_level': [1],\n",
    "                     'productSubGroupDescription': 'UNCAL'}\n",
    "\n",
    "    # Initialize lists for science, background, and ASN files.\n",
    "    sci_downloads, bg_downloads = [], []\n",
    "\n",
    "    pfilter = Observations.filter_products  # Alias for filter_products method.\n",
    "\n",
    "    # ----------Identify uncalibrated SCIENCE files associated with each visit----------\n",
    "    for exposure in sci_obs_id_table:\n",
    "        sci_products = Observations.get_product_list(exposure)\n",
    "\n",
    "        # Filter for full-size science files (exclude smaller confirmation images).\n",
    "        avg_sci_size = np.nanmean(sci_products['size'])\n",
    "        sci_products = sci_products[sci_products['size'] > avg_sci_size]\n",
    "        sci_downloads.extend(pfilter(sci_products, **file_criteria)['dataURI'])\n",
    "\n",
    "    # Filter for full-size background files (exclude smaller confirmation images).\n",
    "    if bg_observtn:\n",
    "        for exposure in bg_obs_id_table:\n",
    "            bg_products = Observations.get_product_list(exposure)\n",
    "\n",
    "            avg_bg_size = np.nanmean(bg_products['size'])\n",
    "            bg_products = bg_products[bg_products['size'] > avg_bg_size]\n",
    "            bg_downloads.extend(pfilter(bg_products, **file_criteria)['dataURI'])\n",
    "\n",
    "    # Filter out other observations and remove duplicates.\n",
    "    sci_downloads = {f for f in sci_downloads if f\"jw{program}{sci_observtn}\" in f}\n",
    "\n",
    "    if bg_observtn:\n",
    "        bg_downloads = {f for f in bg_downloads if f\"jw{program}{bg_observtn}\" in f}\n",
    "        print(f\"Background files selected for downloading: {len(bg_downloads)}\")\n",
    "    else:\n",
    "        print(\"Background files selected for downloading: 0\")\n",
    "\n",
    "    print(f\"Science files selected for downloading: {len(sci_downloads)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43bfc6-4993-43ed-a114-f146c5e15032",
   "metadata": {},
   "source": [
    "Download the data. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Warning**: If this notebook is halted during this step, the downloaded file may be incomplete, and cause crashes later on!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ad497-7d84-4c36-819e-857181a40cf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download data and place them into the appropriate directories.\n",
    "\n",
    "if demo_mode:\n",
    "    for file in sci_downloads:\n",
    "        sci_manifest = Observations.download_file(file, local_path=uncal_dir)\n",
    "    for file in bg_downloads:\n",
    "        bg_manifest = Observations.download_file(file, local_path=uncal_bgdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705405fd-71a1-4167-ad73-dd10b8e58873",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Directory Setup\n",
    "Set up detailed paths to input/output stages here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8ef3c-9bcf-467d-846f-896be31f909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define/create output subdirectories to keep data products organized.\n",
    "\n",
    "# -----------------------------Science Directories------------------------------\n",
    "uncal_dir = os.path.join(sci_dir, 'uncal/')  # Uncalibrated pipeline inputs.\n",
    "asn_dir = os.path.join(sci_dir, 'asn/')  # Association files\n",
    "det1_dir = os.path.join(sci_dir, 'stage1/')  # calwebb_detector1 pipeline outputs.\n",
    "spec2_dir = os.path.join(sci_dir, 'stage2/')  # calwebb_spec2 pipeline outputs.\n",
    "spec3_dir = os.path.join(sci_dir, 'stage3/')  # calwebb_spec3 pipeline outputs.\n",
    "\n",
    "os.makedirs(asn_dir, exist_ok=True)\n",
    "os.makedirs(det1_dir, exist_ok=True)\n",
    "os.makedirs(spec2_dir, exist_ok=True)\n",
    "os.makedirs(spec3_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------Background Directories-----------------------------\n",
    "uncal_bgdir = os.path.join(bg_dir, 'uncal/')  # Uncalibrated pipeline inputs.\n",
    "asn_bgdir = os.path.join(bg_dir, 'asn/')  # Association files\n",
    "det1_bgdir = os.path.join(bg_dir, 'stage1/')  # calwebb_detector1 pipeline outputs.\n",
    "spec2_bgdir = os.path.join(bg_dir, 'stage2/')  # calwebb_spec2 pipeline outputs.\n",
    "\n",
    "if bg_dir:\n",
    "    os.makedirs(asn_bgdir, exist_ok=True)\n",
    "    os.makedirs(det1_bgdir, exist_ok=True)\n",
    "    os.makedirs(spec2_bgdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1838a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark.\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time1-time0)/60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72221863-9f9e-4f90-bd94-76b849e55381",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Stage 1: `Detector1Pipeline` (`calwebb_detector1`)\n",
    "\n",
    "In this section, we process the data through the `calwebb_detector1` pipeline to create Stage 1 [data products](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html).\n",
    "\n",
    "* **Input**: Raw exposure (`_uncal.fits`) containing original data from all detector readouts (ncols x nrows x ngroups x nintegrations).\n",
    "* **Output**: Uncalibrated countrate (slope) image in units of DN/s:\n",
    "    * `_rate.fits`: A single countrate image averaged over multiple integrations (if available).\n",
    "    * `_rateints.fits`: Countrate images for each integration, saved in multiple extensions.\n",
    "\n",
    "The `Detector1Pipeline` applies basic detector-level corrections on a group-by-group basis, followed by ramp fitting for all exposure types, commonly referred to as \"ramps-to-slopes\" processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ddf895-e00e-49d1-b8eb-373ffe78d8e0",
   "metadata": {},
   "source": [
    "### 5.1 Configure `Detector1Pipeline`\n",
    "\n",
    "The `Detector1Pipeline` has the following steps available for NIRSpec FS:\n",
    "\n",
    "* `group_scale` : Rescales pixel values to correct for improper onboard frame averaging.\n",
    "* `dq_init` : Initializes the data quality (DQ) flags for the input data.\n",
    "* `saturation` : Flags pixels at or below the A/D floor or above the saturation threshold.\n",
    "* `superbias` : Subtracts the superbias reference file from the input data.\n",
    "* `refpix` : Use reference pixels to correct bias drifts.\n",
    "* `linearity` : Applies a correction for non-linear detector response.\n",
    "* `dark_current` : Subtracts the dark current reference file from the input data.\n",
    "* `jump` : Performs CR/jump detection on each ramp integration within an exposure.\n",
    "* `clean_flicker_noise`: Removes flicker (1/f) noise from calibrated ramp images (similar to `nsclean` in spec2).\n",
    "* `ramp_fit` : Determines the mean count rate (counts per second) for each pixel by performing a linear fit to the input data.\n",
    "* `gain_scale` : Corrects pixel values for non-standard gain settings, primarily in NIRSpec subarray data.\n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_detector1) •\n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/stable/jwst/pipeline/calwebb_detector1.html)\n",
    "\n",
    "Below, we set up a dictionary that defines how the `Detector1Pipeline` should be configured for IFU data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8c4ff-2e94-4a45-87f2-774a8d341c28",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  To override specific steps and reference files, use the examples below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26281a0-e2a5-475f-9197-37aba48b5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Detector1 pipeline should be configured.\n",
    "\n",
    "# -------------------------Boilerplate dictionary setup-------------------------\n",
    "det1dict = {}\n",
    "det1dict['group_scale'], det1dict['dq_init'], det1dict['saturation'] = {}, {}, {}\n",
    "det1dict['superbias'], det1dict['refpix'] = {}, {}\n",
    "det1dict['linearity'], det1dict['dark_current'], det1dict['jump'] = {}, {}, {}\n",
    "det1dict['clean_flicker_noise'], det1dict['ramp_fit'] = {}, {}\n",
    "det1dict['gain_scale'] = {}\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "\n",
    "# Overrides for various reference files (example).\n",
    "# Files should be in the base local directory or provide full path.\n",
    "#det1dict['dq_init']['override_mask'] = 'myfile.fits' # Bad pixel mask\n",
    "#det1dict['superbias']['override_superbias'] = 'myfile.fits' # Bias subtraction\n",
    "#det1dict['dark_current']['override_dark'] = 'myfile.fits' # Dark current subtraction\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example).\n",
    "det1dict['linearity']['skip'] = False  # This is the default.\n",
    "\n",
    "# Turn on multi-core processing for jump step (off by default).\n",
    "# Choose what fraction of cores to use (quarter, half, or all).\n",
    "det1dict['jump']['maximum_cores'] = 'half'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91161939-27d3-4749-aef2-28bf926496de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on detection of cosmic ray snowballs (on by default).\n",
    "det1dict['jump']['expand_large_events'] = True\n",
    "det1dict['jump']['expand_factor'] = 3  # (default 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f044d7-37ba-4b48-8f59-08b0730ceaa4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Many exposures are affected by artifacts known as [snowballs](https://jwst-docs.stsci.edu/known-issues-with-jwst-data/shower-and-snowball-artifacts#gsc.tab=0) caused by large cosmic ray events. These artifacts are particularly significant in deep exposures with long integration times, with an estimated rate of one snowball per detector (FULL FRAME) per 20 seconds. To expand the number of pixels flagged as jumps around large cosmic ray events, set `expand_large_events` to True. An `expand_factor` of 3 works well for NIRSpec observations to cover most snowballs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634c000-8f6e-4cea-8945-04a466cb1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on 1/f noise correction in Stage 1? (off by default).\n",
    "#det1dict['clean_flicker_noise']['skip'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79ba3a-9a70-4c75-b3a5-254472233a3b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "JWST detector readout electronics (a.k.a. SIDECAR ASICs) generate significant 1/f noise during detector operations and signal digitization. This noise manifests as faint banding along the detector's slow axis and varies from column to column. For NIRSpec data, the primary pipeline algorithm to address 1/f noise is `nsclean` in the `Spec2Pipeline` (Rauscher 2023) but is off by default.\n",
    "\n",
    "An additional 1/f noise-cleaning algorithm, `clean_flicker_noise`, has been implemented at the group stage in the `Detector1Pipeline`. This step is also off by default.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570918c-3950-418e-a317-d3d3b67c080e",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2 Run `Detector1Pipeline`\n",
    "\n",
    "Run the science files and, if available, any background files through the `calwebb_detector1` pipeline using the `.call()` method.\n",
    "\n",
    "We use `.call()` instead of `.run()` to ensure that the latest default parameters from CRDS are applied ([ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/stpipe/call_via_run.html)).\n",
    "\n",
    "This stage takes approximately 14 minutes to process sixteen `_uncal.fits` files (~1 minutes per file) and generate `_rate.fits` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00c73d-a305-40e5-a49b-5eac31ad8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final list of UNCAL files ready for Stage 1 processing.\n",
    "uncal_sci = sorted(glob.glob(uncal_dir + '*uncal.fits'))\n",
    "print(f\"Science UNCAL Files:\\n{'-'*20}\\n\" + \"\\n\".join(uncal_sci))\n",
    "\n",
    "if bg_dir:\n",
    "    uncal_bg = sorted(glob.glob(uncal_bgdir + '*uncal.fits'))\n",
    "    print(f\"Background UNCAL Files:\\n{'-'*20}\\n\" + \"\\n\".join(uncal_bg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d4951-9b86-4ec9-a48f-3baddd0e2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_det1 = time.perf_counter()  # Tracks runtime for Stage 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffb1b3-06ff-4103-8d8b-c596e704b688",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 5.2.1 Calibrating Science Files\n",
    "Identify the input science files and execute the `calwebb_detector1` pipeline using the `call` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d412cc-ca0a-4f5e-ba10-b05924addaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Stage 1 pipeline on the science using the custom det1dict dictionary.\n",
    "\n",
    "if dodet1:\n",
    "    # --------------------------Science UNCAL files--------------------------\n",
    "    for uncal_file in sorted(glob.glob(uncal_dir + '*uncal.fits')):\n",
    "\n",
    "        print(f\"Applying Stage 1 Corrections & Calibrations to: \"\n",
    "              f\"{os.path.basename(uncal_file)}\")\n",
    "\n",
    "        det1_result = Detector1Pipeline.call(uncal_file,\n",
    "                                             save_results=True,\n",
    "                                             steps=det1dict,\n",
    "                                             output_dir=det1_dir)\n",
    "    print(\"Stage 1 has been completed for SCI data! \\n\")\n",
    "else:\n",
    "    print('Skipping Detector1 processing for SCI data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76170d97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 5.2.2 Calibrating Background Files\n",
    "\n",
    "Identify the input background files and execute the `calwebb_detector1` pipeline using the `call` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade759a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 1 pipeline on any background using the custom det1dict dictionary.\n",
    "\n",
    "if dodet1bg:\n",
    "    # ------------------------Background UNCAL files-------------------------\n",
    "    for uncal_file in sorted(glob.glob(uncal_bgdir + '*uncal.fits')):\n",
    "\n",
    "        print(f\"Applying Stage 1 Corrections & Calibrations to: \"\n",
    "              f\"{os.path.basename(uncal_file)}\")\n",
    "\n",
    "        det1bg_result = Detector1Pipeline.call(uncal_file,\n",
    "                                               save_results=True,\n",
    "                                               steps=det1dict,\n",
    "                                               output_dir=det1_bgdir)\n",
    "    print(\"Stage 1 has been completed for BKG data! \\n\")\n",
    "else:\n",
    "    print('Skipping Detector1 processing for BKG data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710231c9-3958-4d9f-87bd-dca96434654b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b39d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark.\n",
    "time2 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time2-time0)/60.0, 1):0.4f} min\")\n",
    "print(f\"Runtime for Stage 1: {round((time2-time_det1)/60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final list of RATE[INTS] files ready for Stage 2 processing.\n",
    "rate_sci = sorted(glob.glob(det1_dir + '*_rate.fits'))\n",
    "rateints_sci = sorted(glob.glob(det1_dir + '*_rateints.fits'))\n",
    "print(f\"SCIENCE | RATE[INTS] Files:\\n{'-'*20}\\n\" + \"\\n\".join(rate_sci + rateints_sci))\n",
    "\n",
    "rate_bg = sorted(glob.glob(det1_bgdir + '*_rate.fits'))\n",
    "rateints_bg = sorted(glob.glob(det1_bgdir + '*_rateints.fits'))\n",
    "print(f\"BACKGROUND | RATE[INTS] Files:\\n{'-' * 20}\\n\" + \"\\n\".join(rate_bg + rateints_bg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ac929-fe39-431c-b43c-f86f8e7c779e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Stage 2: `Spec2Pipeline` (`calwebb_spec2`)\n",
    "\n",
    "In this section, we process our countrate (slope) image products from Stage 1 (`calwebb_detector1`) through the Spec2 (`calwebb_spec2`) pipeline to create Stage 2 [data products](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html).\n",
    "\n",
    "* **Input**: A single countrate (slope) image (`_rate[ints].fits`) or an association file listing multiple inputs.\n",
    "* **Output**: Calibrated products (rectified and unrectified) and 1D spectra.\n",
    "    * `_cal[ints].fits`: Calibrated 2D (unrectified) spectra (ncols x nrows).\n",
    "\t* `_s3d.fits`: Resampled 3D IFU cube (ncols x nrows x nwaves).\n",
    "\t* `_x1d[ints].fits`: Extracted 1D spectroscopic data (wavelength vs. flux).\n",
    "\n",
    "In Stage 2, each exposure (or association) and detector produces a single file, with multiple extensions corresponding to each source.\n",
    "\n",
    "The `Spec2Pipeline` applies additional instrumental corrections and calibrations (e.g., slit loss, path loss, etc.,) to countrate products that result in a fully calibrated individual exposure (per nod/dither position). The `Spec2Pipeline` also converts countrate products from units of DN/s to flux (Jy) for point sources and surface brightness (MJy/sr) for extended sources.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note there has been a bug in the `cube_build` step that caused the point source flux to not be conserved when using different spatial sampling. A fix has been implemented as of release DMS build 9.3/CAL_VER 1.10.2. In order to enable the correct functionality, the units of the `_cal.fits` files and cubes will now be in surface brightness, and only the 1D extracted spectra will be in units of Jy.\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2787867-968b-4795-a0f4-f46dd3c517c7",
   "metadata": {},
   "source": [
    "### 6.1 Configure `Spec2Pipeline`\n",
    "The `Spec2Pipeline` has the following steps available for NIRSpec IFU:\n",
    "\n",
    "* `assign_wcs`: Assigns wavelength solution for spectra.\n",
    "* `msaflagopen`: Flags pixels in NIRSpec exposures affected by MSA shutters stuck in the open position.\n",
    "* `nsclean`: Cleans 1/f noise.\n",
    "* `imprint`: Removes patterns caused by the MSA structure in NIRSpec MOS and IFU exposures.\n",
    "* `bkg_subtract`: Performs image subtraction for background removal.\n",
    "* `srctype`: Determines whether a spectroscopic source should be classified as a point or extended object.\n",
    "* `flat_field`: Applies flat-field corrections to the input science dataset.\n",
    "* `pathloss`: Calculates and applies corrections for signal loss in spectroscopic data.\n",
    "* `photom`: Applies photometric calibrations to convert data from countrate to surface brightness or flux density.\n",
    "* `pixel_replace`: Interpolates and estimates flux values for pixels flagged as DO_NOT_USE in 2D extracted spectra.\n",
    "* `cube_build`: Produces 3D spectral cubes.\n",
    "* `extract_1d`: Extracts a 1D signal from 2D or 3D datasets.\n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_spec2) •\n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html)\n",
    "\n",
    "Below, we set up a dictionary that defines how the `Spec2Pipeline` should be configured for IFU data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602ae90-8adf-4091-9039-1dbb1cb5fcc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "If pixel-to-pixel background subtraction was chosen above, it will be applied during this stage.</br>\n",
    "To override specific steps and reference files, use the examples below. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a8e4e-fbc1-427c-b94e-f47393647edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec2 pipeline should be configured.\n",
    "\n",
    "# -------------------------Boilerplate dictionary setup-------------------------\n",
    "spec2dict = {}\n",
    "spec2dict['assign_wcs'], spec2dict['msa_flagging'] = {}, {}\n",
    "spec2dict['nsclean'], spec2dict['imprint_subtract'] = {}, {}\n",
    "spec2dict['bkg_subtract'] = {}\n",
    "spec2dict['srctype'], spec2dict['wavecorr'] = {}, {}\n",
    "spec2dict['flat_field'], spec2dict['pathloss'] = {}, {}\n",
    "spec2dict['photom'], spec2dict['pixel_replace'] = {}, {}\n",
    "spec2dict['cube_build'], spec2dict['extract_1d'] = {}, {}\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "\n",
    "# Overrides for various reference files (example).\n",
    "# Files should be in the base local directory or provide full path.\n",
    "#spec2dict['extract_1d']['override_extract1d'] = 'myfile.json'\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example).\n",
    "spec2dict['bkg_subtract']['skip'] = not pixel_bg\n",
    "spec2dict['imprint_subtract']['skip'] = False\n",
    "\n",
    "# Run pixel replacement code to extrapolate values for otherwise bad pixels\n",
    "# This can help mitigate 5-10% negative dips in spectra of bright sources.\n",
    "# Use the 'fit_profile' algorithm.\n",
    "#spec2dict['pixel_replace']['skip'] = False\n",
    "#spec2dict['pixel_replace']['n_adjacent_cols'] = 5\n",
    "#spec2dict['pixel_replace']['algorithm'] = 'fit_profile'\n",
    "\n",
    "# Run nsclean for 1/f noise.\n",
    "#spec2dict['nsclean']['skip'] = False\n",
    "#spec2dict['nsclean']['n_sigma'] = 2\n",
    "\n",
    "# Turn on bad pixel self-calibration, where all exposures on a given detector are used to find and\n",
    "# flag bad pixels that may have been missed by the bad pixel mask.\n",
    "# This step is experimental, and works best when dedicated background observations are included\n",
    "#spec2dict['badpix_selfcal']['skip'] = False\n",
    "#spec2dict['badpix_selfcal']['flagfrac_upper']=0.005 # Fraction of pixels to flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7dff6-a5be-457a-9c65-c160d64e2347",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "To correct for 1/f noise with `nsclean` in Stage 2, see the **IFU_NSClean_example** demo notebook for IFU data [here](https://github.com/spacetelescope/jdat_notebooks/tree/main/notebooks/NIRSpec/NIRSpec_NSClean).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ee91f-4afd-45e9-acd7-8b876c45a97b",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.2 Create `Spec2Pipeline` Association Files\n",
    "\n",
    "[Association (ASN) files](https://jwst-pipeline.readthedocs.io/en/stable/jwst/associations/overview.html) define the relationships between multiple exposures, allowing them to get processed as a set rather than individually. Processing an ASN file enables the exposures to be calibrated, archived, retrieved, and reprocessed as a set rather than as individual objects.\n",
    "\n",
    "[Stage 2 ASN files](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/level2_asn_technical.html) for IFU data can include `science`, `background`, and `imprint` exposure types. A Stage 2 ASN file requires at least one `science` file but can contain multiple `background` and `imprint` (leakcals) files that enable pixel-to-pixel background subtraction and imprint subtraction in `calwebb_spec2`.\n",
    "\n",
    "Here we construct the necessary association files based on the observing sequence.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Background subtraction may not be correctly applied if more than *one* `science` file is included in the association. Additionally, pixel-to-pixel background subtraction will only be performed if the grating wheel has not moved between the target and off-scene associated background exposures. If the grating wheel moved between the target and background exposures (as would be the case if they were in different visits), pipeline processing will follow a more involved \"master background\" subtraction done in Stage 3.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association background and imprint setup for nodded exposures\n",
    "def asn_nod(asn, onescifile, sci, sci_imprint, pattnum):\n",
    "    # Assign background exposures\n",
    "    for file in sci:\n",
    "        with fits.open(file) as hdu:\n",
    "            hdu.verify()\n",
    "            # If dither position is different from the input position, use it as a background\n",
    "            this_dither = hdu[0].header['PATT_NUM']\n",
    "            if (this_dither != pattnum):\n",
    "                asn['products'][0]['members'].append({'expname': file, 'exptype': 'background'})\n",
    "\n",
    "    # Assign imprint exposures (pipeline handles figuring out which one is best)\n",
    "    for file in sci_imprint:\n",
    "        with fits.open(file) as hdu:\n",
    "            hdu.verify()\n",
    "            if (match_gwa(onescifile, file)):\n",
    "                asn['products'][0]['members'].append({'expname': file, 'exptype': 'imprint'})      \n",
    "            \n",
    "    # Assign selfcal exposures\n",
    "    for file in sci:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})\n",
    "    for file in sci_imprint:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})\n",
    "        \n",
    "    return asn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association background and imprint setup for dithered exposures\n",
    "def asn_dither(asn, onescifile, sci, sci_imprint, bg, bg_imprint):\n",
    "    # Assign background exposures\n",
    "    for file in bg:\n",
    "        with fits.open(file) as hdu:\n",
    "            hdu.verify()\n",
    "            asn['products'][0]['members'].append({'expname': file, 'exptype': 'background'})    \n",
    "            \n",
    "    # Assign imprint exposures (pipeline handles figuring out which one is best)\n",
    "    for file in sci_imprint:\n",
    "        with fits.open(file) as hdu:\n",
    "            hdu.verify()\n",
    "            if (match_gwa(onescifile, file)):\n",
    "                asn['products'][0]['members'].append({'expname': file, 'exptype': 'imprint'})  \n",
    "    for file in bg_imprint:\n",
    "        with fits.open(file) as hdu:\n",
    "            hdu.verify()\n",
    "            if (match_gwa(bg[0], file)):\n",
    "                asn['products'][0]['members'].append({'expname': file, 'exptype': 'imprint'})\n",
    "            \n",
    "    # Assign selfcal exposures\n",
    "    for file in sci:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})\n",
    "    for file in sci_imprint:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})\n",
    "    for file in bg:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})\n",
    "    for file in bg_imprint:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'selfcal'})\n",
    "    \n",
    "    return asn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an association file for each science exposure\n",
    "# 'allscifiles' will be examined for potential imprint and nodded background exposures\n",
    "# 'bgfiles' will be examined for relevant dedicated background exposures\n",
    "# Returns True if the association was written successfully, and False otherwise\n",
    "# (e.g., if the input 'onescifile' was an imprint exposure)\n",
    "def writel2asn(onescifile, allscifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list([onescifile], rule=DMSLevel2bBase, product_name=prodname)  # Wrap in array since input was single exposure\n",
    "\n",
    "    # Configuration for this sci file\n",
    "    with fits.open(onescifile) as hdu:\n",
    "        hdu.verify()\n",
    "        hdr = hdu[0].header\n",
    "        exptype = hdr['EXP_TYPE'] # Exposure type\n",
    "        if (exptype == 'NRS_IFU'):\n",
    "            detector, grating, filt = hdr['DETECTOR'], hdr['GRATING'], hdr['FILTER']\n",
    "            imprint = hdr['IS_IMPRT']\n",
    "            patttype = hdr['PATTTYPE'] # Dither pattern type\n",
    "            pattnum = hdr['PATT_NUM'] # Dither pattern number\n",
    "        \n",
    "    # If this is any exposure type other than NRS_IFU fail out (to ensure TA images don't get processed by accident)\n",
    "    if (exptype != 'NRS_IFU'):\n",
    "        return False\n",
    "        \n",
    "    # If this is an imprint exposure, fail out since those shouldn't be processed alone\n",
    "    if imprint:\n",
    "        return False\n",
    "    \n",
    "    # Find all files matching the input configuration and split into regular/imprint\n",
    "    use_sci, use_sci_imprint = get_matching(allscifiles, detector, filt, grating)\n",
    "    if (len(bgfiles) > 0):\n",
    "        use_bg, use_bg_imprint = get_matching(bgfiles, detector, filt, grating)\n",
    "    else:\n",
    "        use_bg, use_bg_imprint = '', ''\n",
    "\n",
    "    # If this uses nodded exposures set up pixel-based background subtraction accordingly\n",
    "    if ((patttype == '2-POINT-NOD') | (patttype == '4-POINT-NOD')):\n",
    "        asn = asn_nod(asn, onescifile, use_sci, use_sci_imprint, pattnum)\n",
    "    # Otherwise handle as dithered exposures\n",
    "    else:\n",
    "        asn = asn_dither(asn, onescifile, use_sci, use_sci_imprint, use_bg, use_bg_imprint)\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000fc0c-3271-4b73-992f-c2f1dbc8ac79",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.3 Run `Spec2Pipeline`\n",
    "\n",
    "Run the science files and, if available, any background files through the `calwebb_spec2` pipeline using the `.call()` method.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Perform pixel-to-pixel background subtraction (if desired) here in Stage 2. Otherwise, reduce the backgrounds individually for master background subtraction in Stage 3 (if desired).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4287317-5b5a-4c76-bc9b-14ab5d125bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec2 = time.perf_counter()  # Tracks runtime for Stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ce104-4dd5-4ec7-9d0b-544d6746fa2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 6.3.1 Calibrating Science Files\n",
    "\n",
    "Identify the Stage 2 science files and execute the `calwebb_spec2` pipeline using the `call` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac78ac0-0a85-45eb-8474-300b5e311699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save on runtime, make a new version of our spec2 parameter dictionary\n",
    "# that turns off creation of quicklook 3d cubes/1d spectra for science data.\n",
    "# Any master background subtraction in spec3 will require the 1d spectra from spec2.\n",
    "spec2dict_sci = copy.deepcopy(spec2dict)\n",
    "spec2dict_sci['cube_build']['skip'] = True  # S2D products.\n",
    "spec2dict_sci['extract_1d']['skip'] = True  # X1D products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 2 pipeline using the custom spec2dict_sci dictionary.\n",
    "\n",
    "if dospec2:\n",
    "    # --------------------------Science files--------------------------\n",
    "    for file in rate_sci:\n",
    "        try:\n",
    "            asnfile = os.path.join(asn_dir, os.path.basename(file).replace('rate.fits', 'l2asn.json'))\n",
    "            if writel2asn(file, rate_sci, rate_bg, asnfile, 'Level2'):\n",
    "                print(f\"Applying Stage 2 Corrections & Calibrations to: {file}\")\n",
    "                spec2sci_result = Spec2Pipeline.call(asnfile,\n",
    "                                                     save_results=True,\n",
    "                                                     steps=spec2dict_sci,\n",
    "                                                     output_dir=spec2_dir)\n",
    "        except Exception as e:\n",
    "            # A handle for when no slices fall on NRS2.\n",
    "            print(f\"Skipped processing {os.path.basename(asnfile)}: {e}\")\n",
    "    print(\"Stage 2 has been completed for SCI data! \\n\")\n",
    "else:\n",
    "    print('Skipping Spec2 processing for SCI data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbdb688-6f4b-403d-864e-27d4e7b725ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 6.3.2 Calibrating Background Files\n",
    "Prepare background files for master background subtraction in Stage 3. Will skip if no background files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Stage 2 pipeline using the custom spec2dict dictionary.\n",
    "\n",
    "if dospec2bg and master_bg:\n",
    "    # ---------------------Background ASN or RATE files---------------------\n",
    "    for file in rate_bg:\n",
    "        try:\n",
    "            asnfile = os.path.join(asn_bgdir, os.path.basename(file).replace('rate.fits', 'l2asn.json'))\n",
    "            if writel2asn(file, rate_bg, '', asnfile, 'Level2'):\n",
    "                print(f\"Applying Stage 2 Corrections & Calibrations to: {os.path.basename(file)}\")\n",
    "                spec2bg_result = Spec2Pipeline.call(asnfile,\n",
    "                                                    save_results=True,\n",
    "                                                    steps=spec2dict,\n",
    "                                                    output_dir=spec2_bgdir)\n",
    "        except Exception as e:\n",
    "            # A handle for when no slices fall on NRS2.\n",
    "            print(f\"Skipped processing {os.path.basename(file)}: {e}\")\n",
    "    print(\"Stage 2 has been completed for BKG data! \\n\")\n",
    "else:\n",
    "    print(\"Skipping Stage 2 for BKG data. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0cbd8-540e-4124-a298-39b097804c63",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcaf6c-2a1f-4cd4-842f-2ef5a4372a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmarks.\n",
    "time3 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time3-time0)/60.0, 1):0.4f} min\")\n",
    "print(f\"Runtime for Spec2: {round((time3-time_spec2)/60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e309f-bd7d-40ab-b3f2-3e7dabaed17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Stage 2 products.\n",
    "\n",
    "# -----------------------------Science files-----------------------------\n",
    "sci_cal = sorted(glob.glob(spec2_dir + '*_cal.fits'))\n",
    "sci_s3d = sorted(glob.glob(spec2_dir + '*_s3d.fits'))\n",
    "sci_x1d = sorted(glob.glob(spec2_dir + '*_x1d.fits'))\n",
    "\n",
    "print(f\"SCIENCE | Stage 2 CAL Products:\\n{'-'*20}\\n\" + \"\\n\".join(sci_cal))\n",
    "print(f\"SCIENCE | Stage 2 S3D Products:\\n{'-'*20}\\n\" + \"\\n\".join(sci_s3d))\n",
    "print(f\"SCIENCE | Stage 2 X1D Products:\\n{'-'*20}\\n\" + \"\\n\".join(sci_x1d))\n",
    "\n",
    "# ----------------------------Background files---------------------------\n",
    "bg_cal = sorted(glob.glob(spec2_bgdir + '*_cal.fits'))\n",
    "bg_s3d = sorted(glob.glob(spec2_bgdir + '*_s3d.fits'))\n",
    "bg_x1d = sorted(glob.glob(spec2_bgdir + '*_x1d.fits'))\n",
    "\n",
    "print(f\"BACKGROUND | Stage 2 CAL Products:\\n{'-'*20}\\n\" + \"\\n\".join(bg_cal))\n",
    "print(f\"BACKGROUND | Stage 2 S3D Products:\\n{'-'*20}\\n\" + \"\\n\".join(bg_s3d))\n",
    "print(f\"BACKGROUND | Stage 2 X1D Products:\\n{'-'*20}\\n\" + \"\\n\".join(bg_x1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df814c-fba1-4d99-a7f9-9686b442f30e",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Stage 3: `Spec3Pipeline` (`calwebb_spec3`)\n",
    "\n",
    "In this section, we process our calibrated spectra from Stage 2 (`calwebb_spec2`) through the Spec3 (`calwebb_spec3`) pipeline to create Stage 3 [data products](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html).\n",
    "\n",
    "* **Input**: An ASN file that lists multiple calibrated exposures (`_cal.fits`) in addition to any background exposures (`_x1d.fits`).\n",
    "* **Output**: A single calibrated product (rectified and unrectified) and 1D spectrum. These data products have units of MJy/sr (or Jy for extracted point-source spectra).\n",
    "\t* `_cal.fits`: Calibrated 2D (unrectified) spectra (ncols x nrows).\n",
    "    * `_crf.fits`: Calibrated 2D (unrectified) spectra whose DQ array has been updated to flag pixels detected as outliers (ncols x nrows).\n",
    "    * `_s3d.fits`: Resampled 3D IFU cube (ncols x nrows x nwaves).\n",
    "\t* `_x1d.fits`: Extracted 1D spectroscopic data.\n",
    "\n",
    "In Stage 3, single files are created for each source, one extension in the file.\n",
    "\n",
    "The `Spec3Pipeline` performs additional corrections (e.g., outlier detection, background subtraction) and combines calibrated data from multiple exposures (e.g. a dither/nod pattern) into a single 3D spectral product, as well as a combined 1D spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644289da-cd18-4237-add6-3571cc31c4e6",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.1 Configure `Spec3Pipeline`\n",
    "\n",
    "The `Spec3Pipeline` has the following steps available for NIRSpec IFU:\n",
    "\n",
    "* `assign_mtwcs`: Modifies the WCS output frame in each exposure of a Moving Target (MT) observation association.\n",
    "* `master_background`: Master background subtraction.\n",
    "* `outlier_detection` : Identification of bad pixels or cosmic-rays that remain in each of the input images.\n",
    "* `pixel_replace`: Interpolates and estimates flux values for pixels flagged as DO_NOT_USE in 2D extracted spectra.\n",
    "* `cube_build`: Produces 3D spectral cubes from 2D images.\n",
    "* `extract_1d`: Extracts a 1D signal from 2D or 3D datasets.\n",
    "\n",
    "For more information about each step and a full list of step arguments, please refer to the official documentation: [JDox](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/stages-of-jwst-data-processing/calwebb_spec3) •\n",
    "[ReadtheDocs](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html)\n",
    "\n",
    "Below, we set up a dictionary that defines how the `Spec3Pipeline` should be configured for IFU data.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "If master background subtraction was chosen above, it will be applied during this stage.</br>\n",
    "To override specific steps and reference files, use the examples below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0164f8-2fd6-4344-be02-a0c64d540a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary to define how the Spec3 pipeline should be configured.\n",
    "\n",
    "# -------------------------Boilerplate dictionary setup-------------------------\n",
    "spec3dict = {}\n",
    "spec3dict['assign_mtwcs'], spec3dict['master_background'] = {}, {}\n",
    "spec3dict['outlier_detection'], spec3dict['pixel_replace'] = {}, {}\n",
    "spec3dict['cube_build'], spec3dict['extract_1d'] = {}, {}\n",
    "\n",
    "# ---------------------------Override reference files---------------------------\n",
    "\n",
    "# Overrides for various reference files.\n",
    "# Files should be in the base local directory or provide full path.\n",
    "#spec3dict['extract_1d']['override_extract1d'] = 'myfile.json'\n",
    "\n",
    "# -----------------------------Set step parameters------------------------------\n",
    "\n",
    "# Overrides for whether or not certain steps should be skipped (example).\n",
    "spec3dict['outlier_detection']['skip'] = False\n",
    "\n",
    "# Master background usage was set up above, propagate that here.\n",
    "spec3dict['master_background']['skip'] = not master_bg\n",
    "\n",
    "# Run pixel replacement code to extrapolate values for otherwise bad pixels.\n",
    "# This can help mitigate 5-10% negative dips in spectra of bright sources.\n",
    "# Use the 'fit_profile' algorithm.\n",
    "#spec3dict['pixel_replace']['skip'] = False\n",
    "#spec3dict['pixel_replace']['n_adjacent_cols'] = 5\n",
    "#spec3dict['pixel_replace']['algorithm'] = 'fit_profile'\n",
    "\n",
    "# Testing found this to be a better kernel size.\n",
    "# The kernel size must only contain odd values.\n",
    "spec3dict['outlier_detection']['kernel_size'] = '3 3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65566c6f-27ca-4ada-a830-a1477ca13a74",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "As of DMS build B9.3rc1/CAL_VER 1.11.0, a new outlier detection algorithm for IFU data has been implemented. If using a pipeline version before this build, we recommend that outlier detection be skipped/turned off. To learn more about how the algorithm operates, refer to the documentation [here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/outlier_detection/outlier_detection_ifu.html#outlier-detection-ifu).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d31036-4946-4c4e-8305-d1861ee88399",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.2 Create `Spec3Pipeline` Association Files\n",
    "\n",
    "[Stage 3 ASN files](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/level3_asn_technical.html) for IFU data can include `science` and `background` exposure types. A Stage 3 ASN file requires at least one `science` file (there is usually more than one) but can contain multiple `background` files that enable master background subtraction in `calwebb_spec3`. **Note that the science exposures should be in the `_cal.fits` format, while the background exposures must be in the `_x1d.fits` format.**\n",
    "\n",
    "Exposures from multiple filter/grating settings can be included in the same association file, but the pipeline will treat them independently and create output products for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "\n",
    "    # Add background files to the association\n",
    "    for file in bgfiles:\n",
    "        asn['products'][0]['members'].append({'expname': file, 'exptype': 'background'})\n",
    "\n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec3_asn = os.path.join(asn_dir, 'l3asn.json')\n",
    "if dospec3:\n",
    "    writel3asn(sci_cal, bg_x1d, spec3_asn, 'Level3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d14c2e-2ef0-41e7-8305-486717b18a93",
   "metadata": {},
   "source": [
    "---\n",
    "### 7.3 Run `Spec3Pipeline`\n",
    "\n",
    "Run the science files and, if available, any background files through the `calwebb_spec3` pipeline using the `.call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f441c-2b37-44be-8d7a-e81fcf8e87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec3 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83cd70-12b5-4fa9-8f84-1e2aa22342bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Stage 3 pipeline using the custom spec3dict dictionary.\n",
    "\n",
    "if dospec3:\n",
    "    print(f\"Applying Stage 3 Corrections & Calibrations to: \"f\"{os.path.basename(spec3_asn)}\")\n",
    "    spec3_result = Spec3Pipeline.call(spec3_asn, save_results=True, steps=spec3dict, output_dir=spec3_dir)\n",
    "    print(\"Stage 3 has been completed! \\n\")\n",
    "else:\n",
    "    print(\"Skipping Stage 3. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4db487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmarks.\n",
    "time4 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time4-time0)/60.0, 1):0.4f} min\")\n",
    "print(f\"Runtime for Spec3: {round((time4-time_spec3)/60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d78925-70f4-4871-bc69-e3072400ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Stage 3 products.\n",
    "\n",
    "stage3_cal = sorted(glob.glob(spec3_dir + '*_crf.fits'))\n",
    "stage3_s3d = sorted(glob.glob(spec3_dir + '*_s3d.fits'))\n",
    "stage3_x1d = sorted(glob.glob(spec3_dir + '*_x1d.fits'))\n",
    "\n",
    "print(f\"Stage 3 CAL Products:\\n{'-'*20}\\n\" + \"\\n\".join(stage3_cal))\n",
    "print(f\"Stage 3 S3D Products:\\n{'-'*20}\\n\" + \"\\n\".join(stage3_s3d))\n",
    "print(f\"Stage 3 X1D Products:\\n{'-'*20}\\n\" + \"\\n\".join(stage3_x1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea912bc0",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visualize the Data\n",
    "Define convenience funcitons for visualization.\n",
    "\n",
    "Function to display Stage 1 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rate(rates,\n",
    "                 slits_models=[],\n",
    "                 integration=0,\n",
    "                 extname='data',\n",
    "                 cmap='viridis',\n",
    "                 bad_color=(1, 0.7, 0.7),\n",
    "                 vmin=None,\n",
    "                 vmax=None,\n",
    "                 scale='asinh',\n",
    "                 aspect='auto',\n",
    "                 title_prefix=None,\n",
    "                 title_path=False,\n",
    "                 save_plot=False):\n",
    "    \"\"\"\n",
    "    Display countrate images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rates : list of str\n",
    "        A list of RATE[INTS] files to be displayed.\n",
    "    slits_models : list of str, optional\n",
    "        A list of CAL[INTS] or S2D files containing the slit models.\n",
    "        If provided, slit cutouts will be overlaid on the countrate images.\n",
    "    integration : {None, 'min', int}, optional\n",
    "        Specifies the integration to use for multi-integration data.\n",
    "        If 'min', the minimum value across all integrations is used.\n",
    "        If an integer, the specific integration index is used (default 0).\n",
    "    extname : str, optional\n",
    "        The name of the data extension to extract from ('data', 'dq', etc.).\n",
    "    cmap : str, optional\n",
    "        Colormap to use for displaying the image. Default is 'viridis'.\n",
    "    bad_color : tuple of float, optional\n",
    "        Color to use for NaN pixels. Default is light red (1, 0.7, 0.7).\n",
    "    vmin : float, optional\n",
    "        Minimum value for color scaling. If None, determined from the data.\n",
    "    vmax : float, optional\n",
    "        Maximum value for color scaling. If None, determined from the data.\n",
    "    scale : {'linear', 'log', 'asinh'}, optional\n",
    "        Scale to use for the image normalization. Default is 'asinh'.\n",
    "    aspect : str, optional\n",
    "        Aspect ratio of the plot. Default is 'auto'.\n",
    "    title_prefix : str, optional\n",
    "        Optional prefix for the plot title.\n",
    "    title_path : bool, optional\n",
    "        If True, uses the full file path for the title;\n",
    "        otherwise, uses the basename. Default is False.\n",
    "    save_plot : bool, optional\n",
    "        If True, saves the plot as a PNG file. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------Check Inputs-------------------------------\n",
    "    rates = [rates] if isinstance(rates, str) else rates\n",
    "    slits_models = [slits_models] if isinstance(slits_models, str) else slits_models\n",
    "    nrates = len(rates)\n",
    "\n",
    "    # ------------------------------Set up figures------------------------------\n",
    "    fig, axes = plt.subplots(nrates, 1, figsize=(12, 12 * nrates),\n",
    "                             sharex=True, height_ratios=[1] * nrates)\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    axes = [axes] if nrates == 1 else axes\n",
    "\n",
    "    cmap = plt.get_cmap(cmap)  # Set up colormap and bad pixel color.\n",
    "    cmap.set_bad(bad_color, 1.0)\n",
    "\n",
    "    # ---------------------------Plot countrate image---------------------------\n",
    "    for i, (rate, cal) in enumerate(itertools.zip_longest(rates,\n",
    "                                                          slits_models,\n",
    "                                                          fillvalue=None)):\n",
    "\n",
    "        # -------------------Open files as JWST datamodels-------------------\n",
    "        model = datamodels.open(rate)\n",
    "        slits_model = datamodels.open(cal) if cal else None\n",
    "\n",
    "        # -----------------------Extract the 2D/3D data----------------------\n",
    "        data_2d = getattr(model, extname)\n",
    "        if data_2d.ndim == 3:  # Handle multi-integration data.\n",
    "            if integration == 'min':\n",
    "                data_2d = np.nanmin(data_2d, axis=0)\n",
    "            elif isinstance(integration, int) and 0 <= integration < data_2d.shape[0]:\n",
    "                data_2d = data_2d[integration]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid integration '{integration}' for 3D data.\")\n",
    "\n",
    "        # ---------------------------Scale the data-------------------------\n",
    "        sigma_clipped_data = sigma_clip(data_2d, sigma=5, maxiters=3)\n",
    "        vmin = np.nanmin(sigma_clipped_data) if vmin is None else vmin\n",
    "        vmax = np.nanmax(sigma_clipped_data) if vmax is None else vmax\n",
    "        stretch_map = {'log': LogStretch(), 'linear': LinearStretch(),\n",
    "                       'asinh': AsinhStretch()}\n",
    "        if scale in stretch_map:\n",
    "            norm = ImageNormalize(sigma_clipped_data,\n",
    "                                  interval=ManualInterval(vmin=vmin, vmax=vmax),\n",
    "                                  stretch=stretch_map[scale])\n",
    "        else:\n",
    "            norm = simple_norm(sigma_clipped_data, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        # ----------------Plot the countrate image & colorbar---------------\n",
    "        plt.subplots_adjust(left=0.05, right=0.85)\n",
    "        im = axes[i].imshow(data_2d, origin='lower', cmap=cmap,\n",
    "                            norm=norm, aspect=aspect, interpolation='nearest')\n",
    "        units = model.meta.bunit_data\n",
    "        cbar_ax = fig.add_axes([axes[i].get_position().x1 + 0.02,\n",
    "                                axes[i].get_position().y0, 0.02,\n",
    "                                axes[i].get_position().height])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label(units, fontsize=12)\n",
    "\n",
    "        # -----------------Draw slits and label source ids------------------\n",
    "        # slits_model can be s2d/cal from spec2 - contains slit models for all sources.\n",
    "        if slits_model:\n",
    "            slit_patches = []\n",
    "            for slit in slits_model.slits:\n",
    "                slit_patch = Rectangle((slit.xstart, slit.ystart),\n",
    "                                       slit.xsize, slit.ysize)\n",
    "                slit_patches.append(slit_patch)\n",
    "                y = slit.ystart + slit.ysize / 2\n",
    "                x = slit.xstart if 'nrs1' in rate else slit.xstart + slit.xsize\n",
    "                ha = 'right' if 'nrs1' in rate else 'left'\n",
    "                plt.text(x, y, slit.source_id, color='w', ha=ha, va='center',\n",
    "                         fontsize=7, path_effects=[], weight='bold')\n",
    "            axes[i].add_collection(PatchCollection(slit_patches, ec='r', fc='None'))\n",
    "\n",
    "        # -----------------Construct title and axis labels------------------\n",
    "        filename = model.meta.filename\n",
    "        title = (f\"{title_prefix + ' ' if title_prefix else ''}\"\n",
    "                 f\"{filename if title_path else os.path.basename(filename)}\")\n",
    "        if integration is not None:\n",
    "            title = title.replace('rateints', f'rateints[{integration}]')\n",
    "        axes[i].set_title(title, fontsize=14)\n",
    "        axes[i].set_xlabel(\"Pixel Column\", fontsize=12)\n",
    "        axes[i].set_ylabel(\"Pixel Row\", fontsize=12)\n",
    "\n",
    "        # -------------------------Save the figure?-------------------------\n",
    "        if save_plot:\n",
    "            save_plot = rate.replace('fits', 'png')\n",
    "            if integration:\n",
    "                save_plot = save_plot.replace('.png', '%s.png' % integration)\n",
    "            fig.savefig(save_plot, dpi=200)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba9267",
   "metadata": {},
   "source": [
    "Function to display the IFU cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ifu_cubeslices(s3d_product,\n",
    "                        wavelengths,\n",
    "                        spaxel_loc=[0, 0],\n",
    "                        vmin=None,\n",
    "                        vmax=None,\n",
    "                        cmap='viridis',\n",
    "                        title=None,\n",
    "                        save_figure=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to that takes a 3D IFU data cube and generates:\n",
    "\n",
    "    > 2D cube slices based on wavelength (microns).\n",
    "    > Associated 1D spectrum for a designated spaxel (spatial pixel) in the data cube.\n",
    "    > Corresponding 3D weight image giving the relative weights of the output spaxels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s3d_product : list of str\n",
    "        3D IFU data cube FITS file.\n",
    "    wavelengths : list of float\n",
    "        List of wavelength values (microns) at which to create 2D slices.\n",
    "    spaxel_loc: tuple\n",
    "        List of spaxel location in which to plot the associated 1D spectrum.\n",
    "    cmap: str\n",
    "        Color Map.\n",
    "    vmin, vmax : float, float\n",
    "        Minimum & Maximum signal value to use for scaling.\n",
    "    title : str, optional\n",
    "        Figure Title. Default is None.\n",
    "    save_figure : bool, optional\n",
    "        Save figure?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------ Set up figure ------------------------------\n",
    "\n",
    "    num_wavelengths = len(wavelengths) if wavelengths else 1\n",
    "    fig = plt.figure(figsize=(8 * (num_wavelengths), 18))\n",
    "    gs = grd.GridSpec(3, num_wavelengths, hspace=0.4, wspace=0.7,\n",
    "                      height_ratios=[1]*3, width_ratios=[1]*(num_wavelengths))\n",
    "\n",
    "    cmap_custom = cm.colors.LinearSegmentedColormap.from_list(\"\", [\"darkred\",\n",
    "                                                                   \"darkturquoise\",\n",
    "                                                                   \"blue\"])\n",
    "    colors = cmap_custom(np.linspace(0, 1, num_wavelengths))\n",
    "\n",
    "    plot_count = 0\n",
    "\n",
    "    # ---------------------------- Extract Cube Data ---------------------------\n",
    "\n",
    "    root = s3d_product[:-9]  # Root file name.\n",
    "    s3d = datamodels.open(s3d_product)  # 3D IFU data cube.\n",
    "    x1d3 = datamodels.open(root + '_x1d.fits')  # 1D extracted spectrum.\n",
    "    x1d3wave = x1d3.spec[0].spec_table.WAVELENGTH\n",
    "    cube = s3d.data\n",
    "\n",
    "    # World Coordinate System (WCS) Transformation keywords.\n",
    "    wcs = WCS(fits.open(s3d_product)[1].header)\n",
    "    # wcs = s3d.meta.wcs  # WCS transformation.\n",
    "    wmap = s3d.weightmap  # 3D weight image.\n",
    "    cdelt3 = s3d.meta.wcsinfo.cdelt3  # Axis 3 coordinate increment at reference point.\n",
    "    crval3 = s3d.meta.wcsinfo.crval3  # Third axis value at the reference pixel.\n",
    "    # detector = s3d.meta.instrument.detector\n",
    "    grating = s3d.meta.instrument.grating\n",
    "    ffilter = s3d.meta.instrument.filter\n",
    "    # bunit = s3d.meta.bunit_data\n",
    "\n",
    "    # Wavelength range of the grating/filter combination.\n",
    "    wavstart = s3d.meta.wcsinfo.waverange_start\n",
    "    wavend = s3d.meta.wcsinfo.waverange_end\n",
    "\n",
    "    # Loop through each wavelength slices.\n",
    "    for i, wslice in enumerate(wavelengths):\n",
    "\n",
    "        if float(wavstart) <= wslice*10**-6 <= float(wavend):\n",
    "\n",
    "            # ------------------------- Plot wavelength slice -------------------------\n",
    "            nslice = int((wslice - crval3)/cdelt3)  # The slice of the cube.\n",
    "            slice_mean = np.nanmean(cube[(nslice-2):(nslice+2), :, :], axis=0)\n",
    "            if not vmin and not vmax:\n",
    "                vmin = np.nanpercentile(slice_mean, 2)\n",
    "                vmax = np.nanpercentile(slice_mean, 98)\n",
    "                if vmin < -vmax:\n",
    "                    vmin = -vmax\n",
    "            slice_norm = ImageNormalize(slice_mean, vmin=vmin,\n",
    "                                        vmax=vmax, stretch=AsinhStretch())\n",
    "            ax1 = plt.subplot(gs[plot_count], projection=wcs, slices=('x', 'y', nslice))\n",
    "            slice_image = ax1.imshow(slice_mean, norm=slice_norm,\n",
    "                                     origin='lower', aspect='auto', cmap=cmap)\n",
    "            cb_image = fig.colorbar(slice_image, fraction=0.046, pad=0.04)\n",
    "            cb_image.set_label('MJy/sr', labelpad=-1, fontsize=22)\n",
    "            cb_image.ax.tick_params(labelsize=20)\n",
    "            cb_image.ax.yaxis.get_offset_text().set_fontsize(20)\n",
    "\n",
    "            ax1.set_xlabel('RA', fontsize=22)\n",
    "            ax1.set_ylabel('DEC', labelpad=-1, fontsize=22)\n",
    "            ax1.set_title(\n",
    "                f\"{os.path.basename(s3d_product)}\\n\"\n",
    "                f\"Grating/Filter: {grating}/{ffilter}\\n\"\n",
    "                f\"{wslice} microns\",\n",
    "                fontsize=20)\n",
    "            ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "            ax1.coords[0].set_ticklabel(rotation=13, ha='right', pad=24)\n",
    "\n",
    "            # ------------------------- Plot spaxel spectrum -------------------------\n",
    "            # Zoom in on a Spaxel: Spectrum\n",
    "            x1d3flux_loc = cube[:, spaxel_loc[1], spaxel_loc[0]]\n",
    "            ax2 = plt.subplot(gs[num_wavelengths + plot_count])\n",
    "\n",
    "            ax2.plot(x1d3wave, x1d3flux_loc, linewidth=1, color=colors[i])\n",
    "            spaxel_rect = plt.Rectangle((spaxel_loc[0]-.5, spaxel_loc[1]-.5), 1, 1,\n",
    "                                        fill=False, color='black', linewidth=2)\n",
    "            ax1.add_patch(spaxel_rect)\n",
    "\n",
    "            ax2.grid(linewidth=2)\n",
    "            ax2.set_xlabel('$\\u03BB [\\u03BC$m]', fontsize=22)\n",
    "            ax2.set_ylabel(\"Surface Brightness \\n (MJy/sr)\", fontsize=22)\n",
    "            ax2.set_title('Spaxel at (x, y)=' + repr(spaxel_loc), fontsize=25)\n",
    "            ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "            ax2.yaxis.get_offset_text().set_fontsize(15)\n",
    "\n",
    "            # ------------------------- Plot weight slice -------------------------\n",
    "            # Corresponding Weight Map (wmap) for cube slice.\n",
    "            ax3 = plt.subplot(gs[2*num_wavelengths + plot_count],\n",
    "                              projection=wcs, slices=('x', 'y', nslice))\n",
    "            slice_mean_wmap = np.nanmean(wmap[(nslice-2):(nslice+2), :, :], axis=0)\n",
    "            slice_norm_wmap = ImageNormalize(slice_mean_wmap, stretch=AsinhStretch())\n",
    "            slice_wmap = ax3.imshow(slice_mean_wmap, norm=slice_norm_wmap,\n",
    "                                    origin='lower', aspect='auto', cmap=cmap)\n",
    "            cb_wmap = fig.colorbar(slice_wmap, fraction=0.046, pad=0.04)\n",
    "            cb_wmap.set_label('Weight', labelpad=-1, fontsize=22)\n",
    "            cb_wmap.ax.tick_params(labelsize=20)\n",
    "            cb_wmap.ax.yaxis.get_offset_text().set_fontsize(20)\n",
    "\n",
    "            ax3.set_xlabel('RA', fontsize=22)\n",
    "            ax3.set_ylabel('DEC', labelpad=-1, fontsize=22)\n",
    "            ax3.set_title(str(wslice)+' microns: Weight Map', fontsize=25)\n",
    "            ax3.tick_params(axis='both', which='major', labelsize=15)\n",
    "            ax3.coords[0].set_ticklabel(rotation=13, ha='right', pad=24)\n",
    "            # Scale information.\n",
    "            ax2.set_ylim(np.nanpercentile(x1d3flux_loc, 2),\n",
    "                         np.nanpercentile(x1d3flux_loc, 98))\n",
    "            ax2.xaxis.set_tick_params(labelsize=20)\n",
    "            ax2.yaxis.set_tick_params(labelsize=20)\n",
    "            ax2.set_aspect(0.5/ax2.get_data_ratio())\n",
    "            plot_count += 1\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=25)\n",
    "        plt.subplots_adjust(top=0.8)\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 0.98, 0.98])\n",
    "\n",
    "    if save_figure:\n",
    "        fig.savefig(root+\".png\", dpi=24, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e839a",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 8.1 Display `Detector1Pipeline` Products\n",
    "\n",
    "Inspect the Stage 1 slope products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d37e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_file = rate_sci[-1]  # Show the last rate file, as an example.\n",
    "display_rate(rate_file, vmin=0, vmax=2, scale='asinh',\n",
    "             aspect=1, title_prefix='REPROCESSED')  # , extname='dq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac19d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.2 Display `Spec3Pipeline` Products\n",
    "Inspect the Stage 3 combined calibrated spectra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_s3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b03f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = stage3_s3d[0]  # Show the last cube file, as an example.\n",
    "x1d_file = stage3_x1d[0]\n",
    "hdu = fits.open(x1d_file)\n",
    "wavetemp = hdu[1].data['wavelength']\n",
    "wavemin, wavemax = np.nanmin(wavetemp), np.nanmax(wavetemp)\n",
    "hdu.close()\n",
    "\n",
    "# Define parameters for the plot.\n",
    "title = 'Level 3 IFU Product: 3D Cube Slices vs. Corresponding 3D Weighted Map'\n",
    "\n",
    "# Slices may be blank if the spectrum falls on the other detector at that wavelength.\n",
    "wavelengths = [3.0, 3.4, 4.0]  # Wavelength slices (um) to take from the 3D data cube.\n",
    "# If all wavelengths are outside cube range, use a default wavelength 1/8 through the range for display\n",
    "if ((np.nanmin(wavelengths) > wavemax) | (np.nanmax(wavelengths) < wavemin)):\n",
    "    wavelengths = [wavemin + 0.125 * (wavemax - wavemin)]\n",
    "spaxel_loc = [30, 29]  # Spaxel locations for associated 1D spectrum [x, y].\n",
    "\n",
    "show_ifu_cubeslices(cube, wavelengths=wavelengths,\n",
    "                    spaxel_loc=spaxel_loc, title='REPROCESSED '+title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 9))\n",
    "\n",
    "x1d = datamodels.open(x1d_file)\n",
    "x1d_wave = x1d.spec[0].spec_table.WAVELENGTH\n",
    "x1d_flux = x1d.spec[0].spec_table.FLUX\n",
    "grating = x1d.meta.instrument.grating\n",
    "ffilter = x1d.meta.instrument.filter\n",
    "\n",
    "plt.plot(x1d_wave, x1d_flux, linewidth=2, label=f'REPROCESSED ({grating}/{ffilter})')\n",
    "\n",
    "for wave in wavelengths:\n",
    "    plt.vlines(wave, np.nanpercentile(x1d_flux, 2), np.nanpercentile(x1d_flux, 98),\n",
    "               'black', 'dotted', label=f'{wave} microns', linewidth=5)\n",
    "plt.xlabel('Wavelength (μm)', fontsize=15)\n",
    "plt.ylabel('Flux (Jy)', fontsize=15)\n",
    "plt.title(\"Level 3 IFU Product: Extracted 1D Spectrum\",\n",
    "          fontsize=20)\n",
    "plt.ylim(np.nanpercentile(x1d_flux, 2), np.nanpercentile(x1d_flux, 98))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4553d-7897-49fa-b00b-41eb728b691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time5 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time5-time0)/60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f6362-e267-427a-9985-56d29975455c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Modifying the EXTRACT1D Reference File (as needed)\n",
    "\n",
    "The `extract_1d` step is controlled by a different set of parameters in the EXTRACT1D reference file for extended vs. point source IFU data. \n",
    "\n",
    "[Extraction for 3D IFU Data:](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html)\n",
    "\n",
    "> * For extended sources, rectangular aperture photometry is used, with the entire image extracted and no background subtraction, regardless of what is specified in the reference file or step arguments.\n",
    "> * For point source data, the extraction aperture is centered at the RA/DEC target location indicated by the header. If the target location is undefined in the header, then the extraction region is the center of the IFU cube.\n",
    "> * For point sources, a circular extraction aperture is used, along with an optional circular annulus for background extraction and subtraction. The size of the extraction region and the background annulus size varies with wavelength. The extraction-related vectors are found in the ASDF EXTRACT1D reference file. For each element in the wavelength vector, there are three size components: `radius`, `inner_bkg`, and `outer_bkg`. The radius vector sets the extraction size while `inner_bkg` and `outer_bkg` specify the limits of an annular background aperture. \n",
    "\n",
    "Below is an example of how to modify the EXTRACT1D reference file for point sources. More information about this file and how to modify it in [extract_1d](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d) and\n",
    "[Editing JSON reference file](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/reference_files.html#editing-json-reference-file-format-for-non-ifu-data).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The `ifu_autocen` parameter provides a new method to center on the point sources even if the header information is imperfect due to inaccuracies caused by, e.g., FGS.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "**Warning**: Currently, there is no aperture correction in place for NIRSpec, so the `radius` parameter **MUST** remain unchanged for point source to ensure proper flux calibration!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d43e8-fc8c-42dc-801b-860306e900be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know the reference file name this should work.\n",
    "#extract_1d_ref = Spec3Pipeline().get_reference_file(stage3_s3d, 'extract1d')\n",
    "\n",
    "refs = api.dump_references(crds_client.get_context_used('jwst'),\n",
    "                           ['jwst_nirspec_extract1d_0002.asdf'])\n",
    "extract_1d_ref = refs['jwst_nirspec_extract1d_0002.asdf']\n",
    "\n",
    "# Construct the modified file name\n",
    "basename = os.path.basename(extract_1d_ref)[:-5]  # Remove \".asdf\"\n",
    "extract_1d_ref_mod = os.path.join(spec3_dir, f\"{basename}_demo.asdf\")\n",
    "\n",
    "print('Original x1d reference file', extract_1d_ref)\n",
    "print('Modified x1d reference file', extract_1d_ref_mod)\n",
    "\n",
    "# Open the original ASDF file, modify it, and save the modified version\n",
    "# in your current directory.\n",
    "with asdf.open(extract_1d_ref, mode='r') as ref_file:\n",
    "    # Create a copy of the original tree\n",
    "    tree = ref_file.tree.copy()\n",
    "\n",
    "    # Modify the tree.\n",
    "    tree['data']['radius'] = np.full((2048,), 0.45, dtype='float32')\n",
    "    tree['data']['inner_bkg'] = np.full((2048,), 1.0, dtype='float32')\n",
    "    tree['data']['outer_bkg'] = np.full((2048,), 1.2, dtype='float32')\n",
    "\n",
    "    # Save the modified tree to a new file.\n",
    "    with asdf.AsdfFile(tree) as new_file:\n",
    "        new_file.write_to(extract_1d_ref_mod, all_array_storage='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56a2a4-469b-4462-a213-bb3a86c63ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check modified file contents.\n",
    "with asdf.open(extract_1d_ref_mod) as ref_file:\n",
    "    # Pretty-print the ASDF tree structure.\n",
    "    pprint(ref_file.tree, depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c5fe4-70d8-4db2-b00e-ffa662242909",
   "metadata": {},
   "source": [
    "Now, we re-extract the 1D spectrum by running the `Extract1dStep` and overriding the reference file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f6211-0930-4d5f-a1d1-bb731f3ee327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s3d in stage3_s3d:\n",
    "    Extract1dStep.call(s3d,\n",
    "                       save_results=True,\n",
    "                       output_dir=spec3_dir,\n",
    "                       output_use_model=True,\n",
    "                       suffix='x1d_mod',  # Default suffix is `_extract1dstep.fits`\n",
    "                       use_source_posn=False,\n",
    "                       ifu_autocen=False, # Set this to True for isolated point sources\n",
    "                       override_extract1d=extract_1d_ref_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dcc8e-094a-4790-ae5c-0cb6873c559b",
   "metadata": {},
   "source": [
    "We now plot again the 3D/1D final cube/spectra and showing the original extraction box in red and the new extraction box in black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ee82e-ce11-4292-8d5f-f24431bcdba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary files\n",
    "stage3_x1d_mod = sorted(glob.glob(spec3_dir + '*_x1d_mod.fits'))\n",
    "x1d_file = stage3_x1d_mod[0]\n",
    "cube_data = datamodels.open(cube).data\n",
    "x1d_mod = datamodels.open(x1d_file)\n",
    "x1d_wave_mod = x1d_mod.spec[0].spec_table.WAVELENGTH\n",
    "x1d_flux_mod = x1d_mod.spec[0].spec_table.FLUX\n",
    "\n",
    "# Setup the figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 9),\n",
    "                               gridspec_kw={'width_ratios': [5, 3],\n",
    "                                            'wspace': 0.1})\n",
    "\n",
    "# Plot the original and modified spectra\n",
    "ax1.plot(x1d_wave, x1d_flux, linewidth=2, label=\"Original Extraction\")\n",
    "ax1.plot(x1d_wave_mod, x1d_flux_mod, linewidth=2, label=\"Modified Extraction\")\n",
    "ax1.set_xlabel('Wavelength (μm)', fontsize=15)\n",
    "ax1.set_ylabel('Flux (Jy)', fontsize=15)\n",
    "ax1.set_title(\"Level 3 IFU Product: Extracted 1D Spectrum\", fontsize=20)\n",
    "ax1.set_ylim(np.nanpercentile(x1d_flux, 2), np.nanpercentile(x1d_flux, 98))\n",
    "ax1.ticklabel_format(axis='y', style='sci', scilimits=(0, -2))\n",
    "ax1.legend(fontsize=15)\n",
    "\n",
    "# Plot the IFU cube slice\n",
    "slice_mean = np.nanmean(cube_data[400:500, :, :], axis=0)\n",
    "vmin = np.nanpercentile(slice_mean, 2)\n",
    "vmax = np.nanpercentile(slice_mean, 98)\n",
    "if vmin < -vmax:\n",
    "    vmin = -vmax\n",
    "slice_full = ax2.imshow(slice_mean,\n",
    "                        norm=ImageNormalize(vmin=vmin, vmax=vmax, stretch=LinearStretch()),\n",
    "                        origin='lower', cmap='viridis')\n",
    "plt.colorbar(slice_full, ax=ax2,\n",
    "             fraction=0.046, pad=0.04).set_label('MJy/sr', fontsize=15)\n",
    "\n",
    "# Annotate the extraction regions\n",
    "with asdf.open(extract_1d_ref_mod, mode='r') as ref_file:\n",
    "    radii_data = ref_file.tree['data']\n",
    "    print(\"Radius [arcsec]:\", radii_data['radius'][0])\n",
    "    print(\"Inner background [arcsec]:\", radii_data['inner_bkg'][0])\n",
    "    print(\"Outer background [arcsec]:\", radii_data['outer_bkg'][0])\n",
    "\n",
    "    if x1d_mod.spec[0].source_type == 'POINT':\n",
    "        x_cen, y_cen = x1d_mod.spec[0].extraction_x, x1d_mod.spec[0].extraction_y\n",
    "        for label, radius, color in zip(['Radius', 'Inner Background Radius',\n",
    "                                         'Outer Background Radius'],\n",
    "                                        ['radius', 'inner_bkg', 'outer_bkg'],\n",
    "                                        ['black', 'blue', 'red']):\n",
    "            ax2.add_patch(Circle((x_cen, y_cen), radii_data[radius][0] * 10,\n",
    "                                 fill=False, color=color, label=label))\n",
    "\n",
    "    ax2.set_xlabel('X (pixels)', fontsize=15)\n",
    "    ax2.set_ylabel('Y (pixels)', fontsize=15)\n",
    "    ax2.set_title(\"Full IFU Cube: \\n Extraction Region Preview\", fontsize=20)\n",
    "    ax2.legend(fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a4186-78dd-44c5-a7dc-b8696407b236",
   "metadata": {},
   "source": [
    "**The spectra will look identical if the source is extended.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3aa32-d540-4ffd-b772-aa27c0622ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time5 = time.perf_counter()\n",
    "print(f\"Runtime so far: {round((time5-time0)/60.0, 1):0.4f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c013b62-d4e2-4aad-a9f0-c0b684c347bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Related Notebooks\n",
    "\n",
    "\n",
    "* [NIRSpec Workaround Notebooks](https://github.com/spacetelescope/jwst-caveat-examples/tree/main/NIRSPEC_General)\n",
    "* [JDAT: JWST Data Analysis Example Notebooks](https://github.com/spacetelescope/jdat_notebooks/tree/main/notebooks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2475a-cb07-4ceb-9640-b045b82c9f96",
   "metadata": {},
   "source": [
    "<figure>\n",
    "       <img src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\\\" align=\"right\" style=\"width: 200px\"/>\n",
    "</figure>\n",
    "   \n",
    "[Top of Page](#NIRSpec-IFU-Pipeline-Notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
